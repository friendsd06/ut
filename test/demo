# Atomic INSERT Operations with Databricks & MyBatis

## üéØ Overview

**Challenge**: Need to insert single records into three different tables atomically using Spring Boot, MyBatis, and Databricks JDBC driver.

**Goal**: Ensure all three inserts succeed or all fail (ACID compliance) to maintain data consistency.

---

## üö® The Problem

### Why Standard @Transactional Doesn't Work Well

- **Databricks JDBC limitations**: Limited support for traditional JDBC transactions
- **Auto-commit issues**: SQL Warehouse often has auto-commit enabled by default
- **Connection pooling**: State not properly maintained across operations
- **Transaction isolation**: Different from traditional RDBMS behavior

### Example Scenario
```java
@Transactional  // ‚ùå May not work reliably with Databricks
public void insertOrderData(Customer customer, Order order, OrderItem item) {
    customerMapper.insert(customer);     // ‚úÖ Succeeds
    orderMapper.insert(order);           // ‚ùå Fails
    orderItemMapper.insert(item);        // ‚ùå Never executes
    // Result: Inconsistent data - customer exists but no order!
}
```

---

## ‚úÖ Solution 1: Manual SqlSession Transaction (Recommended)

### Implementation
```java
public void insertThreeRecordsAtomically(
        CustomerRecord customer,
        OrderRecord order,
        OrderItemRecord orderItem) throws Exception {

    SqlSession sqlSession = sqlSessionFactory.openSession(false); // Manual commit

    try {
        // Get mappers from session
        CustomerMapper customerMapper = sqlSession.getMapper(CustomerMapper.class);
        OrderMapper orderMapper = sqlSession.getMapper(OrderMapper.class);
        OrderItemMapper orderItemMapper = sqlSession.getMapper(OrderItemMapper.class);

        // Execute all inserts
        customerMapper.insertCustomer(customer);
        orderMapper.insertOrder(order);
        orderItemMapper.insertOrderItem(orderItem);

        // Commit all changes atomically
        sqlSession.commit();

    } catch (Exception e) {
        sqlSession.rollback(); // Rollback on any failure
        throw e;
    } finally {
        sqlSession.close();
    }
}
```

### ‚úÖ Advantages
- **True atomicity**: All operations use same SqlSession
- **Explicit control**: Manual commit/rollback
- **Databricks compatible**: Works regardless of JDBC driver limitations
- **Clear error handling**: Explicit rollback on failures

---

## ‚úÖ Solution 2: MyBatis Batch Operations

### Implementation
```java
public void insertThreeRecordsAsBatch(
        CustomerRecord customer,
        OrderRecord order,
        OrderItemRecord orderItem) throws Exception {

    SqlSession sqlSession = sqlSessionFactory.openSession(ExecutorType.BATCH, false);

    try {
        CustomerMapper customerMapper = sqlSession.getMapper(CustomerMapper.class);
        OrderMapper orderMapper = sqlSession.getMapper(OrderMapper.class);
        OrderItemMapper orderItemMapper = sqlSession.getMapper(OrderItemMapper.class);

        // Add to batch
        customerMapper.insertCustomer(customer);
        orderMapper.insertOrder(order);
        orderItemMapper.insertOrderItem(orderItem);

        // Execute batch atomically
        sqlSession.flushStatements();
        sqlSession.commit();

    } catch (Exception e) {
        sqlSession.rollback();
        throw e;
    } finally {
        sqlSession.close();
    }
}
```

### ‚úÖ Advantages
- **Better performance**: Single round-trip to database
- **Atomic execution**: Batch succeeds or fails as unit
- **Scalable**: Works well for multiple record sets

---

## ‚úÖ Solution 3: Delta Lake MERGE Operations

### Why MERGE is Better for Databricks
- **Native Delta Lake support**: Each MERGE is inherently atomic
- **Optimistic concurrency**: Handles concurrent operations automatically
- **No JDBC transaction dependency**: Works regardless of driver limitations

### Implementation
```java
private void mergeInsertCustomer(CustomerRecord customer) {
    String sql = """
        MERGE INTO customers AS target
        USING (SELECT ? as customer_id, ? as name, ? as email) AS source
        ON target.customer_id = source.customer_id
        WHEN NOT MATCHED THEN
            INSERT (customer_id, name, email, created_at)
            VALUES (source.customer_id, source.name, source.email, current_timestamp())
        """;

    jdbcTemplate.update(sql, customer.getCustomerId(), customer.getName(), customer.getEmail());
}

public void insertThreeRecordsWithMerge(CustomerRecord customer, OrderRecord order, OrderItemRecord orderItem) {
    mergeInsertCustomer(customer);    // Atomic operation 1
    mergeInsertOrder(order);          // Atomic operation 2
    mergeInsertOrderItem(orderItem);  // Atomic operation 3
}
```

---

## üîß Required Configuration

### DataSource Configuration
```java
@Bean
public DataSource dataSource() {
    HikariConfig config = new HikariConfig();
    config.setJdbcUrl("jdbc:databricks://workspace:443/default");
    config.setUsername("token");
    config.setPassword("access-token");
    config.setAutoCommit(false); // üîë Critical for transactions
    config.setMaximumPoolSize(10);
    return new HikariDataSource(config);
}
```

### MyBatis Configuration
```java
@Bean
public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception {
    SqlSessionFactoryBean sessionFactory = new SqlSessionFactoryBean();
    sessionFactory.setDataSource(dataSource);

    Configuration configuration = new Configuration();
    configuration.setDefaultExecutorType(ExecutorType.SIMPLE);
    configuration.setUseGeneratedKeys(false); // Databricks may not support

    sessionFactory.setConfiguration(configuration);
    return sessionFactory.getObject();
}
```

---

## üéØ Recommended Approach

### For Single Record Inserts (Our Use Case)
**Use Manual SqlSession Transaction (Solution 1)**

**Reasons:**
- ‚úÖ Most reliable with Databricks
- ‚úÖ Explicit transaction control
- ‚úÖ Clear error handling
- ‚úÖ Works regardless of Spring transaction issues

### Code Template
```java
@Service
public class AtomicInsertService {

    @Autowired
    private SqlSessionFactory sqlSessionFactory;

    public void insertOrderDataAtomically(CustomerRecord customer, OrderRecord order, OrderItemRecord orderItem) {
        SqlSession sqlSession = sqlSessionFactory.openSession(false);
        try {
            // Get mappers
            CustomerMapper customerMapper = sqlSession.getMapper(CustomerMapper.class);
            OrderMapper orderMapper = sqlSession.getMapper(OrderMapper.class);
            OrderItemMapper orderItemMapper = sqlSession.getMapper(OrderItemMapper.class);

            // Execute inserts
            validateResult(customerMapper.insertCustomer(customer), "Customer insert failed");
            validateResult(orderMapper.insertOrder(order), "Order insert failed");
            validateResult(orderItemMapper.insertOrderItem(orderItem), "Order item insert failed");

            sqlSession.commit();
        } catch (Exception e) {
            sqlSession.rollback();
            throw new RuntimeException("Atomic insert failed", e);
        } finally {
            sqlSession.close();
        }
    }

    private void validateResult(int result, String errorMessage) {
        if (result != 1) throw new RuntimeException(errorMessage);
    }
}
```

---

## üîç Error Handling & Monitoring

### Validation Strategy
```java
private void validateInsertResult(int result, String operation) {
    if (result != 1) {
        throw new RuntimeException(operation + " failed - affected rows: " + result);
    }
    System.out.println("‚úÖ " + operation + " completed successfully");
}
```

### Retry Mechanism
```java
public void insertWithRetry(CustomerRecord customer, OrderRecord order, OrderItemRecord orderItem, int maxRetries) {
    for (int attempt = 1; attempt <= maxRetries; attempt++) {
        try {
            insertThreeRecordsAtomically(customer, order, orderItem);
            return; // Success
        } catch (Exception e) {
            if (attempt == maxRetries) throw e;
            // Wait before retry (exponential backoff)
            Thread.sleep(1000 * attempt);
        }
    }
}
```

### Data Consistency Verification
```sql
-- Check for orphaned records
SELECT 'Orphaned Orders' as issue, COUNT(*) as count
FROM orders o LEFT JOIN customers c ON o.customer_id = c.customer_id
WHERE c.customer_id IS NULL
UNION ALL
SELECT 'Orphaned Items' as issue, COUNT(*) as count
FROM order_items oi LEFT JOIN orders o ON oi.order_id = o.order_id
WHERE o.order_id IS NULL;
```

---

## üìä Performance Comparison

| Approach | Atomicity | Performance | Databricks Compatibility | Complexity |
|----------|-----------|-------------|-------------------------|------------|
| @Transactional | ‚ùå Unreliable | ‚≠ê‚≠ê‚≠ê | ‚ùå Poor | ‚≠ê Low |
| Manual SqlSession | ‚úÖ Reliable | ‚≠ê‚≠ê‚≠ê | ‚úÖ Excellent | ‚≠ê‚≠ê Medium |
| Batch Operations | ‚úÖ Reliable | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Excellent | ‚≠ê‚≠ê Medium |
| MERGE Operations | ‚úÖ Reliable | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Native | ‚≠ê‚≠ê‚≠ê High |

---

## üöÄ Implementation Plan

### Phase 1: Immediate (Week 1)
1. **Update DataSource configuration** - Set `autoCommit=false`
2. **Implement Manual SqlSession approach** for critical insert operations
3. **Add validation and error handling**

### Phase 2: Enhancement (Week 2-3)
1. **Add retry mechanisms** for transient failures
2. **Implement monitoring** and consistency checks
3. **Performance testing** with actual data volumes

### Phase 3: Optimization (Week 4)
1. **Consider MERGE operations** for high-performance scenarios
2. **Batch processing** for bulk operations
3. **Documentation and team training**

---

## üîß Required Dependencies

### Maven Dependencies
```xml
<dependency>
    <groupId>org.mybatis.spring.boot</groupId>
    <artifactId>mybatis-spring-boot-starter</artifactId>
    <version>3.0.3</version>
</dependency>
<dependency>
    <groupId>com.databricks</groupId>
    <artifactId>databricks-jdbc</artifactId>
    <version>2.6.36</version>
</dependency>
<dependency>
    <groupId>com.zaxxer</groupId>
    <artifactId>HikariCP</artifactId>
</dependency>
```

---

## üìã Action Items

### For Development Team
- [ ] **Review current transaction usage** in existing code
- [ ] **Update DataSource configuration** with correct settings
- [ ] **Implement manual SqlSession approach** for critical operations
- [ ] **Add comprehensive error handling** and validation
- [ ] **Create unit tests** for atomic insert scenarios

### For DevOps Team
- [ ] **Monitor transaction logs** in Databricks
- [ ] **Set up alerts** for failed atomic operations
- [ ] **Performance monitoring** for database operations

### For QA Team
- [ ] **Test failure scenarios** to ensure proper rollback
- [ ] **Verify data consistency** after operations
- [ ] **Load testing** with concurrent operations

---

## ‚ùì Q&A

### Common Questions

**Q: Why not use Spring's @Transactional?**
A: Databricks JDBC driver has limited transaction support, making @Transactional unreliable.

**Q: What if one insert fails midway?**
A: With manual SqlSession, all previous inserts in the same session are rolled back automatically.

**Q: Is this approach scalable?**
A: Yes, MyBatis batch operations can handle multiple record sets efficiently.

**Q: How do we handle concurrent operations?**
A: Delta Lake provides optimistic concurrency control automatically.

---

## üìû Support & Resources

- **Technical Lead**: [Your Name] - For implementation questions
- **Databricks Documentation**: [Delta Lake ACID guarantees](https://docs.databricks.com/delta/acid-guarantees.html)
- **MyBatis Documentation**: [Transaction Management](https://mybatis.org/spring/transactions.html)
- **Team Slack**: #databricks-development

---

*Last Updated: [Current Date] | Version: 1.0*