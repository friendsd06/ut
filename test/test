# Databricks setJobGroup Example - Fire Data Analysis
#sdfds
## Use Case: Organizing Fire Department Data Analysis with Job Groups

This example shows how `setJobGroup` helps organize and monitor different phases of your fire data analysis in Databricks.

## Complete Working Example

### Cell 1: Setup and Data Loading

```python
# Cell 1: Import and initialize
import pyspark.sql.functions as f
from pyspark.sql.functions import col, count, avg, sum as spark_sum, max as spark_max, desc
import time

# Get Spark context
spark = SparkSession.getActiveSession()
sc = spark.sparkContext

print("🔥 Fire Data Analysis with Job Groups")
print(f"Application ID: {sc.applicationId}")
print("="*50)
```

### Cell 2: Data Ingestion Phase

```python
# Cell 2: Data Ingestion with Job Group
print("\n📊 PHASE 1: DATA INGESTION")
print("-" * 30)

# Set job group for data loading operations
sc.setJobGroup("01_Data_Ingestion", "Loading and validating fire incident data")

# Load fire data (using your original code structure)
fire_df = spark.read \
    .format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv")

# Cache the data for multiple operations
fire_df.cache()

# Get initial count (this will show up under "01_Data_Ingestion" job group)
print("🔄 Loading fire incident data...")
total_records = fire_df.count()
print(f"✅ Loaded {total_records:,} fire incident records")

# Show schema
print("\n📋 Data Schema:")
fire_df.printSchema()

# Clear job group
sc.setJobGroup(None, None)
```

### Cell 3: Data Quality Analysis Phase

```python
# Cell 3: Data Quality Analysis
print("\n🔍 PHASE 2: DATA QUALITY ANALYSIS")
print("-" * 35)

# Set job group for data quality operations
sc.setJobGroup("02_Data_Quality", "Analyzing data quality and completeness")

print("🔄 Checking data quality...")

# Check for null values in key columns
key_columns = ["CallType", "CallDate", "Address", "City"]
quality_report = {}

for column in key_columns:
    null_count = fire_df.filter(col(column).isNull()).count()
    total_count = fire_df.count()
    null_percentage = (null_count / total_count) * 100
    
    quality_report[column] = {
        'null_count': null_count,
        'null_percentage': null_percentage
    }
    
    print(f"  {column}: {null_count:,} nulls ({null_percentage:.1f}%)")

# Check distinct call types (your original Q1 logic)
print("\n🔄 Analyzing call types...")
distinct_call_types = fire_df.filter(col("CallType").isNotNull()) \
                           .select("CallType") \
                           .distinct() \
                           .count()

print(f"✅ Found {distinct_call_types} distinct call types")

# Clear job group
sc.setJobGroup(None, None)
```

### Cell 4: Call Type Analysis Phase

```python
# Cell 4: Call Type Analysis (Your original queries)
print("\n📈 PHASE 3: CALL TYPE ANALYSIS")
print("-" * 30)

# Set job group for call type analysis
sc.setJobGroup("03_Call_Analysis", "Detailed analysis of emergency call types")

print("🔄 Running call type analysis...")

# Your original Q1 query
print("\n📋 Q1: Distinct Call Types")
q1_df = fire_df.where(col("CallType").isNotNull()) \
             .select("CallType") \
             .distinct()

q1_count = q1_df.count()
print(f"Result: {q1_count} distinct call types")

# Your original Q2 query  
print("\n📋 Q2: Call Types with Alias")
q2_df = fire_df.where(col("CallType").isNotNull()) \
             .selectExpr("CallType as distinct_call_type") \
             .distinct()

print("Sample call types:")
sample_types = q2_df.limit(5).collect()
for i, row in enumerate(sample_types, 1):
    print(f"  {i}. {row['distinct_call_type']}")

# Show total count
q2_count = q2_df.count()
print(f"\nTotal distinct call types: {q2_count}")

# Clear job group
sc.setJobGroup(None, None)
```

### Cell 5: Advanced Analytics Phase

```python
# Cell 5: Advanced Analytics
print("\n📊 PHASE 4: ADVANCED ANALYTICS")
print("-" * 30)

# Set job group for advanced analytics
sc.setJobGroup("04_Advanced_Analytics", "Advanced fire incident analytics and insights")

print("🔄 Running advanced analytics...")

# Top call types by frequency
print("\n📈 Top 10 Call Types by Frequency:")
top_call_types = fire_df.filter(col("CallType").isNotNull()) \
                       .groupBy("CallType") \
                       .agg(count("*").alias("incident_count")) \
                       .orderBy(desc("incident_count")) \
                       .limit(10)

top_results = top_call_types.collect()
for i, row in enumerate(top_results, 1):
    print(f"  {i:2d}. {row['CallType']}: {row['incident_count']:,} incidents")

# Response time analysis (if available)
print("\n⏱️ Response Time Analysis:")
if "Delay" in fire_df.columns:
    response_stats = fire_df.filter(col("Delay").isNotNull() & (col("Delay") > 0)) \
                           .agg(
                               avg("Delay").alias("avg_delay"),
                               spark_max("Delay").alias("max_delay"),
                               count("*").alias("records_with_delay")
                           ).collect()[0]
    
    print(f"  Average response time: {response_stats['avg_delay']:.1f} minutes")
    print(f"  Maximum response time: {response_stats['max_delay']:.1f} minutes")
    print(f"  Records with delay data: {response_stats['records_with_delay']:,}")
else:
    print("  No delay/response time data available")

# Clear job group
sc.setJobGroup(None, None)
```

### Cell 6: Results Summary and Job Group Monitoring

```python
# Cell 6: Summary and Job Group Monitoring
print("\n📊 PHASE 5: RESULTS SUMMARY")
print("-" * 25)

# Set job group for final summary
sc.setJobGroup("05_Summary", "Final results compilation and reporting")

print("🔄 Compiling final results...")

# Create summary
summary_results = {
    'total_records': total_records,
    'distinct_call_types': distinct_call_types,
    'data_quality_issues': sum(1 for col, data in quality_report.items() if data['null_count'] > 0),
    'top_call_type': top_results[0]['CallType'] if top_results else 'Unknown',
    'top_call_type_count': top_results[0]['incident_count'] if top_results else 0
}

print("\n🎉 ANALYSIS COMPLETE!")
print("=" * 40)
print(f"📊 Total Records Analyzed: {summary_results['total_records']:,}")
print(f"🏷️  Distinct Call Types: {summary_results['distinct_call_types']}")
print(f"🔍 Data Quality Issues: {summary_results['data_quality_issues']} columns with nulls")
print(f"🥇 Most Common Call Type: {summary_results['top_call_type']} ({summary_results['top_call_type_count']:,} incidents)")

# Clear job group
sc.setJobGroup(None, None)

print("\n✅ All job groups completed successfully!")
```

### Cell 7: Job Group Monitoring (Optional)

```python
# Cell 7: Monitor Job Groups (Optional - for demonstration)
print("\n🔍 JOB GROUP MONITORING")
print("=" * 30)

def monitor_job_groups():
    """Function to demonstrate job group monitoring"""
    
    # Set a job group for monitoring operations
    sc.setJobGroup("06_Monitoring", "Monitoring and tracking job group execution")
    
    # Get status tracker
    status_tracker = sc.statusTracker()
    
    # Show active jobs (if any)
    active_jobs = status_tracker.getActiveJobIds()
    print(f"Active Jobs: {len(active_jobs)}")
    
    # Show executor information
    executor_info = status_tracker.getExecutorInfos()
    print(f"Active Executors: {len(executor_info)}")
    
    for executor in executor_info:
        print(f"  Executor {executor.executorId}: {executor.totalCores} cores, {executor.maxMemory} MB memory")
    
    # Clear monitoring job group
    sc.setJobGroup(None, None)
    
    print("\n💡 Job Groups Benefits:")
    print("  ✅ Organized operations by business phase")
    print("  ✅ Easy to track progress in Spark UI")
    print("  ✅ Better debugging and monitoring")
    print("  ✅ Clear separation of concerns")

# Run monitoring
monitor_job_groups()
```

## How to Use This Example

### 1. **Copy to Databricks**
- Create a new Databricks notebook
- Copy each cell above into separate notebook cells
- Run cells in order (1 → 2 → 3 → 4 → 5 → 6 → 7)

### 2. **View in Spark UI**
- Go to **Spark UI** in Databricks
- Click on **Jobs** tab
- You'll see jobs organized by groups:
  - `01_Data_Ingestion`
  - `02_Data_Quality` 
  - `03_Call_Analysis`
  - `04_Advanced_Analytics`
  - `05_Summary`

### 3. **Expected Output**
```
🔥 Fire Data Analysis with Job Groups
Application ID: application_1234567890123_4567
==================================================

📊 PHASE 1: DATA INGESTION
------------------------------
🔄 Loading fire incident data...
✅ Loaded 175,296 fire incident records

🔍 PHASE 2: DATA QUALITY ANALYSIS
-----------------------------------
🔄 Checking data quality...
  CallType: 0 nulls (0.0%)
  CallDate: 0 nulls (0.0%)
  Address: 591 nulls (0.3%)
  City: 0 nulls (0.0%)

🔄 Analyzing call types...
✅ Found 32 distinct call types

📈 PHASE 3: CALL TYPE ANALYSIS
------------------------------
🔄 Running call type analysis...

📋 Q1: Distinct Call Types
Result: 32 distinct call types

📋 Q2: Call Types with Alias
Sample call types:
  1. Medical Incident
  2. Structure Fire
  3. Alarms
  4. Traffic Collision
  5. Citizen Assist / Service Call

Total distinct call types: 32
```

## Key Benefits Demonstrated

### 🎯 **For Your Team:**

1. **Organized Execution**
   - Each phase has its own job group
   - Easy to track which phase is running
   - Clear progress monitoring

2. **Better Debugging**
   - Failed jobs are grouped by business function
   - Easy to identify which phase failed
   - Faster troubleshooting

3. **Spark UI Clarity**
   - Jobs organized by purpose in Spark UI
   - No more mystery jobs
   - Clear execution timeline

4. **Production Monitoring**
   - Can kill specific job groups if needed
   - Monitor resource usage by phase
   - Better alerting and logging

### 🚀 **Immediate Value:**
- **Before**: All operations show as generic "Spark Job"
- **After**: Clear labels like "Data_Quality", "Call_Analysis", etc.
- **Result**: Much easier to monitor and debug!

This example takes your exact fire data analysis and makes it production-ready with proper job organization! 🔥



===========================


# Airflow + Kubernetes + Spark Listener Integration

## The Challenge

When Airflow submits Spark jobs to Kubernetes, the Spark listener runs inside the pod, but Airflow needs to track the status externally. Here are proven solutions:

## Solution 1: Database-Based Status Tracking (Recommended)

### 1. Enhanced Python Listener with Database Reporting

```python
# spark_listener_with_db.py
import os
import json
import time
import logging
import psycopg2
from psycopg2.extras import RealDictCursor
import requests
from pyspark.sql import SparkSession
from pyspark import SparkContext

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AirflowSparkListener:
    """Spark Listener that reports status to external database for Airflow monitoring"""

    def __init__(self, spark_session):
        self.spark = spark_session
        self.sc = spark_session.sparkContext
        self.application_id = self.sc.applicationId
        self.application_name = self.sc.appName

        # Airflow tracking
        self.dag_id = os.getenv('AIRFLOW_DAG_ID', 'unknown')
        self.task_id = os.getenv('AIRFLOW_TASK_ID', 'unknown')
        self.run_id = os.getenv('AIRFLOW_RUN_ID', 'unknown')

        # Database connection
        self.db_connection = self._setup_database()

        logger.info(f"Listener initialized - App: {self.application_name}, DAG: {self.dag_id}")

    def _setup_database(self):
        """Setup database connection"""
        try:
            conn = psycopg2.connect(
                host=os.getenv('DB_HOST', 'postgres-service'),
                port=int(os.getenv('DB_PORT', '5432')),
                database=os.getenv('DB_NAME', 'airflow'),
                user=os.getenv('DB_USER', 'airflow'),
                password=os.getenv('DB_PASSWORD', 'airflow')
            )
            conn.autocommit = True

            # Create table if not exists
            with conn.cursor() as cursor:
                cursor.execute("""
                CREATE TABLE IF NOT EXISTS spark_job_tracking (
                    id SERIAL PRIMARY KEY,
                    dag_id VARCHAR(255),
                    task_id VARCHAR(255),
                    run_id VARCHAR(255),
                    application_id VARCHAR(255),
                    application_name VARCHAR(255),
                    status VARCHAR(100),
                    job_id INTEGER,
                    stage_id INTEGER,
                    progress_info JSONB,
                    error_message TEXT,
                    timestamp BIGINT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );

                CREATE INDEX IF NOT EXISTS idx_tracking_dag_run
                ON spark_job_tracking(dag_id, run_id, task_id);
                """)

            logger.info("Database connection established")
            return conn

        except Exception as e:
            logger.error(f"Database setup failed: {e}")
            return None

    def _report_status(self, status, job_id=None, stage_id=None,
                      progress_info=None, error_message=None):
        """Report status to database and optionally to Airflow API"""

        timestamp = int(time.time() * 1000)

        # Report to database
        if self.db_connection:
            try:
                with self.db_connection.cursor() as cursor:
                    cursor.execute("""
                    INSERT INTO spark_job_tracking
                    (dag_id, task_id, run_id, application_id, application_name,
                     status, job_id, stage_id, progress_info, error_message, timestamp)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        self.dag_id, self.task_id, self.run_id,
                        self.application_id, self.application_name,
                        status, job_id, stage_id,
                        json.dumps(progress_info) if progress_info else None,
                        error_message, timestamp
                    ))

                logger.info(f"Status reported to DB: {status}")

            except Exception as e:
                logger.error(f"Failed to report to database: {e}")

        # Report to Airflow API (optional webhook)
        airflow_webhook = os.getenv('AIRFLOW_WEBHOOK_URL')
        if airflow_webhook:
            try:
                payload = {
                    'dag_id': self.dag_id,
                    'task_id': self.task_id,
                    'run_id': self.run_id,
                    'application_id': self.application_id,
                    'status': status,
                    'timestamp': timestamp,
                    'progress_info': progress_info
                }

                response = requests.post(
                    airflow_webhook,
                    json=payload,
                    timeout=10,
                    headers={'Content-Type': 'application/json'}
                )
                response.raise_for_status()

            except Exception as e:
                logger.error(f"Failed to report to Airflow webhook: {e}")

    def on_application_start(self):
        """Application started"""
        self._report_status('APPLICATION_STARTED')

        # Get initial application info
        try:
            status_tracker = self.sc.statusTracker()
            app_info = {
                'spark_version': self.sc.version,
                'master': self.sc.master,
                'app_id': self.application_id
            }
            self._report_status('APPLICATION_INFO', progress_info=app_info)

        except Exception as e:
            logger.error(f"Error getting app info: {e}")

    def on_job_start(self, job_id):
        """Job started"""
        self._report_status('JOB_STARTED', job_id=job_id)

    def on_job_end(self, job_id, result, error=None):
        """Job ended"""
        if result == 'SUCCESS':
            self._report_status('JOB_SUCCEEDED', job_id=job_id)
        else:
            self._report_status('JOB_FAILED', job_id=job_id, error_message=error)

    def on_stage_start(self, stage_id, num_tasks):
        """Stage started"""
        progress = {'total_tasks': num_tasks, 'completed_tasks': 0}
        self._report_status('STAGE_STARTED', stage_id=stage_id, progress_info=progress)

    def on_stage_end(self, stage_id, result, num_completed=0, num_failed=0):
        """Stage completed"""
        progress = {
            'completed_tasks': num_completed,
            'failed_tasks': num_failed,
            'result': result
        }

        if result == 'SUCCESS':
            self._report_status('STAGE_COMPLETED', stage_id=stage_id, progress_info=progress)
        else:
            self._report_status('STAGE_FAILED', stage_id=stage_id, progress_info=progress)

    def on_application_end(self, result='SUCCESS', error=None):
        """Application ended"""
        if result == 'SUCCESS':
            self._report_status('APPLICATION_COMPLETED')
        else:
            self._report_status('APPLICATION_FAILED', error_message=error)

        # Close database connection
        if self.db_connection:
            self.db_connection.close()

    def report_custom_metric(self, metric_name, metric_value):
        """Report custom business metrics"""
        progress_info = {
            'metric_name': metric_name,
            'metric_value': metric_value,
            'type': 'custom_metric'
        }
        self._report_status('CUSTOM_METRIC', progress_info=progress_info)

    def report_progress_percentage(self, percentage, details=None):
        """Report job progress percentage"""
        progress_info = {
            'percentage': percentage,
            'details': details,
            'type': 'progress_update'
        }
        self._report_status('PROGRESS_UPDATE', progress_info=progress_info)

def create_spark_job_with_tracking():
    """Create Spark job with Airflow tracking"""

    # Initialize Spark
    spark = SparkSession.builder \
        .appName("Airflow Tracked Spark Job") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()

    # Initialize listener
    listener = AirflowSparkListener(spark)
    listener.on_application_start()

    try:
        # Your Spark job logic here
        logger.info("Starting data processing...")

        # Simulate job progress reporting
        listener.on_job_start(1)
        listener.report_progress_percentage(10, "Starting data ingestion")

        # Stage 1: Data loading
        listener.on_stage_start(1, 100)
        df = spark.range(1, 1000000).toDF("id")
        df = df.withColumn("value", df.id * 2)
        df.cache()

        count = df.count()
        listener.on_stage_end(1, "SUCCESS", num_completed=100)
        listener.report_progress_percentage(40, f"Data loaded: {count} rows")

        # Stage 2: Data processing
        listener.on_stage_start(2, 50)
        processed_df = df.filter(df.value > 100000)
        result_count = processed_df.count()
        listener.on_stage_end(2, "SUCCESS", num_completed=50)

        listener.report_progress_percentage(80, f"Data processed: {result_count} rows")

        # Stage 3: Save results
        listener.on_stage_start(3, 10)
        output_path = os.getenv('SPARK_OUTPUT_PATH', '/tmp/spark_output/')
        processed_df.coalesce(1).write.mode('overwrite').csv(output_path)
        listener.on_stage_end(3, "SUCCESS", num_completed=10)

        listener.on_job_end(1, "SUCCESS")
        listener.report_progress_percentage(100, "Job completed successfully")

        # Report final metrics
        listener.report_custom_metric("total_rows_processed", result_count)
        listener.report_custom_metric("job_duration_seconds", time.time())

        listener.on_application_end("SUCCESS")

        logger.info(f"Job completed successfully. Processed {result_count} rows")
        return True

    except Exception as e:
        error_msg = f"Job failed: {str(e)}"
        logger.error(error_msg)

        listener.on_job_end(1, "FAILED", str(e))
        listener.on_application_end("FAILED", str(e))

        raise Exception(error_msg)

    finally:
        spark.stop()

if __name__ == "__main__":
    create_spark_job_with_tracking()
```

### 2. Airflow DAG with Database Monitoring

```python
# airflow_dag_with_spark_tracking.py
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.spark_kubernetes import SparkKubernetesOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.operators.python import PythonOperator
from airflow.sensors.sql import SqlSensor
from airflow.providers.postgres.operators.postgres import PostgresOperator
from datetime import datetime, timedelta
import json
import logging

logger = logging.getLogger(__name__)

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'spark_job_with_tracking',
    default_args=default_args,
    description='Spark job with real-time status tracking',
    schedule_interval='@daily',
    catchup=False,
    tags=['spark', 'kubernetes', 'tracking', 'production']
)

def get_job_progress(**context):
    """Get real-time job progress from database"""
    postgres_hook = PostgresHook(postgres_conn_id='postgres_default')

    dag_id = context['dag'].dag_id
    run_id = context['run_id']
    task_id = context['task'].task_id

    # Get latest progress
    sql = """
    SELECT
        status,
        progress_info,
        timestamp,
        error_message
    FROM spark_job_tracking
    WHERE dag_id = %s AND run_id = %s AND task_id = %s
    ORDER BY timestamp DESC
    LIMIT 10
    """

    results = postgres_hook.get_records(sql, parameters=[dag_id, run_id, task_id])

    progress_info = []
    for row in results:
        status, progress_json, timestamp, error = row
        progress_data = json.loads(progress_json) if progress_json else {}

        progress_info.append({
            'status': status,
            'progress': progress_data,
            'timestamp': timestamp,
            'error': error
        })

    # Log current progress
    if progress_info:
        latest = progress_info[0]
        if latest['progress'].get('type') == 'progress_update':
            percentage = latest['progress'].get('percentage', 0)
            details = latest['progress'].get('details', '')
            logger.info(f"Job Progress: {percentage}% - {details}")
        else:
            logger.info(f"Latest Status: {latest['status']}")

    return progress_info

def validate_job_completion(**context):
    """Validate job completion and get final metrics"""
    postgres_hook = PostgresHook(postgres_conn_id='postgres_default')

    dag_id = context['dag'].dag_id
    run_id = context['run_id']
    task_id = context['task'].task_id

    # Check final status
    sql = """
    SELECT status, progress_info, error_message
    FROM spark_job_tracking
    WHERE dag_id = %s AND run_id = %s AND task_id = %s
    AND status IN ('APPLICATION_COMPLETED', 'APPLICATION_FAILED', 'JOB_FAILED')
    ORDER BY timestamp DESC LIMIT 1
    """

    result = postgres_hook.get_first(sql, parameters=[dag_id, run_id, task_id])

    if result:
        status, progress_json, error = result

        if status == 'APPLICATION_COMPLETED':
            # Get custom metrics
            metrics_sql = """
            SELECT progress_info FROM spark_job_tracking
            WHERE dag_id = %s AND run_id = %s AND task_id = %s
            AND status = 'CUSTOM_METRIC'
            ORDER BY timestamp DESC
            """

            metrics = postgres_hook.get_records(metrics_sql, parameters=[dag_id, run_id, task_id])

            custom_metrics = {}
            for metric_row in metrics:
                metric_data = json.loads(metric_row[0]) if metric_row[0] else {}
                metric_name = metric_data.get('metric_name')
                metric_value = metric_data.get('metric_value')
                if metric_name:
                    custom_metrics[metric_name] = metric_value

            logger.info(f"Job completed successfully with metrics: {custom_metrics}")
            return {'status': 'SUCCESS', 'metrics': custom_metrics}

        else:
            error_msg = f"Job failed with status: {status}, error: {error}"
            logger.error(error_msg)
            raise Exception(error_msg)

    else:
        raise Exception("No completion status found - job may still be running or failed to report")

def cleanup_old_tracking_data(**context):
    """Cleanup old tracking data"""
    postgres_hook = PostgresHook(postgres_conn_id='postgres_default')

    # Keep data for 7 days
    sql = """
    DELETE FROM spark_job_tracking
    WHERE created_at < NOW() - INTERVAL '7 days'
    """

    postgres_hook.run(sql)
    logger.info("Cleaned up old tracking data")

# Spark application configuration with tracking
spark_app_config = {
    'apiVersion': 'sparkoperator.k8s.io/v1beta2',
    'kind': 'SparkApplication',
    'metadata': {
        'name': 'tracked-spark-job-{{ ds_nodash }}-{{ ts_nodash }}',
        'namespace': 'spark-jobs'
    },
    'spec': {
        'type': 'Python',
        'pythonVersion': '3',
        'mode': 'cluster',
        'image': 'your-registry/spark-with-tracking:v3.4.0',
        'imagePullPolicy': 'Always',
        'mainApplicationFile': 's3a://your-bucket/spark-jobs/spark_listener_with_db.py',
        'sparkVersion': '3.4.0',
        'driver': {
            'cores': 1,
            'memory': '2g',
            'serviceAccount': 'spark-driver-sa',
            'env': [
                # Airflow context
                {
                    'name': 'AIRFLOW_DAG_ID',
                    'value': '{{ dag.dag_id }}'
                },
                {
                    'name': 'AIRFLOW_TASK_ID',
                    'value': '{{ task.task_id }}'
                },
                {
                    'name': 'AIRFLOW_RUN_ID',
                    'value': '{{ run_id }}'
                },
                # Database connection
                {
                    'name': 'DB_HOST',
                    'value': '{{ var.value.spark_tracking_db_host }}'
                },
                {
                    'name': 'DB_PORT',
                    'value': '{{ var.value.spark_tracking_db_port }}'
                },
                {
                    'name': 'DB_NAME',
                    'value': '{{ var.value.spark_tracking_db_name }}'
                },
                {
                    'name': 'DB_USER',
                    'value': '{{ var.value.spark_tracking_db_user }}'
                },
                {
                    'name': 'DB_PASSWORD',
                    'value': '{{ var.value.spark_tracking_db_password }}'
                },
                # Optional webhook
                {
                    'name': 'AIRFLOW_WEBHOOK_URL',
                    'value': '{{ var.value.airflow_webhook_url }}'
                },
                {
                    'name': 'SPARK_OUTPUT_PATH',
                    'value': 's3a://your-bucket/output/{{ ds }}/'
                }
            ]
        },
        'executor': {
            'cores': 2,
            'instances': 3,
            'memory': '4g',
            'serviceAccount': 'spark-executor-sa'
        },
        'deps': {
            'packages': [
                'org.postgresql:postgresql:42.3.1'
            ]
        }
    }
}

# Create tracking table
create_tracking_table = PostgresOperator(
    task_id='create_tracking_table',
    postgres_conn_id='postgres_default',
    sql="""
    CREATE TABLE IF NOT EXISTS spark_job_tracking (
        id SERIAL PRIMARY KEY,
        dag_id VARCHAR(255),
        task_id VARCHAR(255),
        run_id VARCHAR(255),
        application_id VARCHAR(255),
        application_name VARCHAR(255),
        status VARCHAR(100),
        job_id INTEGER,
        stage_id INTEGER,
        progress_info JSONB,
        error_message TEXT,
        timestamp BIGINT,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );

    CREATE INDEX IF NOT EXISTS idx_tracking_dag_run
    ON spark_job_tracking(dag_id, run_id, task_id);
    """,
    dag=dag
)

# Submit Spark job
submit_spark_job = SparkKubernetesOperator(
    task_id='submit_spark_job',
    namespace='spark-jobs',
    application_file=spark_app_config,
    kubernetes_conn_id='kubernetes_default',
    do_xcom_push=True,
    dag=dag
)

# Wait for application to start
wait_for_app_start = SqlSensor(
    task_id='wait_for_app_start',
    conn_id='postgres_default',
    sql="""
    SELECT 1 FROM spark_job_tracking
    WHERE dag_id = '{{ dag.dag_id }}'
    AND run_id = '{{ run_id }}'
    AND task_id = '{{ task.task_id }}'
    AND status = 'APPLICATION_STARTED'
    """,
    timeout=300,
    poke_interval=10,
    dag=dag
)

# Monitor progress periodically
monitor_progress_1 = PythonOperator(
    task_id='monitor_progress_1',
    python_callable=get_job_progress,
    dag=dag
)

monitor_progress_2 = PythonOperator(
    task_id='monitor_progress_2',
    python_callable=get_job_progress,
    dag=dag
)

# Wait for completion
wait_for_completion = SqlSensor(
    task_id='wait_for_completion',
    conn_id='postgres_default',
    sql="""
    SELECT 1 FROM spark_job_tracking
    WHERE dag_id = '{{ dag.dag_id }}'
    AND run_id = '{{ run_id }}'
    AND task_id = '{{ task.task_id }}'
    AND status IN ('APPLICATION_COMPLETED', 'APPLICATION_FAILED', 'JOB_FAILED')
    """,
    timeout=3600,
    poke_interval=30,
    dag=dag
)

# Validate completion
validate_completion = PythonOperator(
    task_id='validate_completion',
    python_callable=validate_job_completion,
    dag=dag
)

# Cleanup old data
cleanup_data = PythonOperator(
    task_id='cleanup_old_data',
    python_callable=cleanup_old_tracking_data,
    dag=dag
)

# Task dependencies
create_tracking_table >> submit_spark_job >> wait_for_app_start

# Parallel monitoring
wait_for_app_start >> [monitor_progress_1, monitor_progress_2]

# Add delays between monitoring
from airflow.operators.bash import BashOperator

delay_1 = BashOperator(
    task_id='delay_1',
    bash_command='sleep 30',
    dag=dag
)

delay_2 = BashOperator(
    task_id='delay_2',
    bash_command='sleep 30',
    dag=dag
)

# Monitoring sequence with delays
monitor_progress_1 >> delay_1 >> monitor_progress_2 >> delay_2 >> wait_for_completion

wait_for_completion >> validate_completion >> cleanup_data
```

### 3. Real-time Progress Dashboard Query

```sql
-- Real-time job progress query for monitoring dashboard
WITH latest_status AS (
    SELECT DISTINCT ON (dag_id, run_id, task_id)
        dag_id,
        run_id,
        task_id,
        application_id,
        status,
        progress_info,
        timestamp,
        error_message
    FROM spark_job_tracking
    WHERE created_at > NOW() - INTERVAL '24 hours'
    ORDER BY dag_id, run_id, task_id, timestamp DESC
),
progress_summary AS (
    SELECT
        dag_id,
        run_id,
        task_id,
        application_id,
        status,
        CASE
            WHEN progress_info->>'type' = 'progress_update'
            THEN (progress_info->>'percentage')::integer
            ELSE NULL
        END as progress_percentage,
        progress_info->>'details' as progress_details,
        error_message,
        timestamp
    FROM latest_status
)
SELECT
    dag_id,
    run_id,
    task_id,
    application_id,
    status,
    COALESCE(progress_percentage, 0) as progress_pct,
    progress_details,
    error_message,
    TO_TIMESTAMP(timestamp/1000) as last_update,
    CASE
        WHEN status LIKE '%COMPLETED%' THEN 'SUCCESS'
        WHEN status LIKE '%FAILED%' THEN 'FAILED'
        WHEN status LIKE '%STARTED%' OR status LIKE '%PROGRESS%' THEN 'RUNNING'
        ELSE 'UNKNOWN'
    END as overall_status
FROM progress_summary
ORDER BY timestamp DESC;
```

## Key Benefits of This Approach:

### ✅ **Real-time Tracking**
- Airflow gets live updates from Spark job
- Progress percentage and detailed status
- Custom business metrics reporting

### ✅ **Failure Detection**
- Immediate error reporting
- Detailed error messages with context
- Early failure detection before pod failure

### ✅ **Progress Monitoring**
- Stage-by-stage progress tracking
- Task completion counts
- Custom milestone reporting

### ✅ **Integration Ready**
- Works with any Kubernetes operator
- Database agnostic (PostgreSQL, MySQL, etc.)
- Webhook support for external systems

## Usage Instructions:

1. **Deploy the tracking table** in your Airflow database
2. **Build Docker image** with the enhanced listener code
3. **Configure environment variables** in your Spark application
4. **Use the Airflow DAG** template provided
5. **Monitor progress** through database queries or dashboards

This gives you comprehensive job tracking that bridges Spark execution in Kubernetes with Airflow orchestration! 🚀

==================

# Simple Databricks Spark Listener Test

## Quick Test Code for Databricks

### 1. Simple Python Listener (Copy-Paste Ready)

```python
# Cell 1: Simple Spark Listener Class
import time
import json
from pyspark.sql import SparkSession

class SimpleSparkListener:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.sc = spark_session.sparkContext
        self.application_id = self.sc.applicationId
        self.application_name = self.sc.appName
        self.events = []

        print(f"🚀 Listener initialized for: {self.application_name}")
        print(f"📱 Application ID: {self.application_id}")

    def log_event(self, event_type, details=None):
        """Log events with timestamp"""
        timestamp = int(time.time() * 1000)
        event = {
            'timestamp': timestamp,
            'application_id': self.application_id,
            'event_type': event_type,
            'details': details or {}
        }
        self.events.append(event)

        # Print to console for immediate feedback
        print(f"⏰ {time.strftime('%H:%M:%S')} - {event_type}")
        if details:
            print(f"   📋 Details: {details}")

    def on_job_start(self, job_id):
        self.log_event("JOB_STARTED", {"job_id": job_id})

    def on_job_end(self, job_id, status):
        self.log_event("JOB_ENDED", {"job_id": job_id, "status": status})

    def on_stage_start(self, stage_id, num_tasks):
        self.log_event("STAGE_STARTED", {"stage_id": stage_id, "num_tasks": num_tasks})

    def on_stage_end(self, stage_id, status):
        self.log_event("STAGE_COMPLETED", {"stage_id": stage_id, "status": status})

    def get_events(self):
        """Return all captured events"""
        return self.events

    def print_summary(self):
        """Print a summary of all events"""
        print("\n" + "="*50)
        print("🔍 LISTENER SUMMARY")
        print("="*50)

        for event in self.events:
            time_str = time.strftime('%H:%M:%S', time.localtime(event['timestamp']/1000))
            print(f"{time_str} | {event['event_type']} | {event['details']}")

        print(f"\n📊 Total Events Captured: {len(self.events)}")
        print("="*50)

# Initialize the listener
spark = SparkSession.getActiveSession()
listener = SimpleSparkListener(spark)
```

### 2. Test Job with Manual Event Tracking

```python
# Cell 2: Test Spark Job with Listener
import random
from pyspark.sql.functions import col, count, avg, sum as spark_sum

def run_test_job_with_listener():
    """Run a test Spark job with manual listener tracking"""

    print("🏁 Starting Test Job...")
    listener.log_event("TEST_JOB_STARTED")

    try:
        # Job 1: Create sample data
        print("\n📊 Job 1: Creating sample data...")
        listener.on_job_start(1)

        # Stage 1: Generate data
        listener.on_stage_start(1, 100)
        df = spark.range(1, 100000).toDF("id")
        df = df.withColumn("value", col("id") * 2)
        df = df.withColumn("category", col("id") % 10)
        df.cache()  # This will trigger an action
        count1 = df.count()
        listener.on_stage_end(1, "SUCCESS")
        listener.on_job_end(1, "SUCCESS")

        print(f"✅ Created DataFrame with {count1} rows")

        # Job 2: Aggregation
        print("\n📊 Job 2: Running aggregations...")
        listener.on_job_start(2)

        # Stage 2: Aggregation
        listener.on_stage_start(2, 10)
        agg_df = df.groupBy("category").agg(
            count("id").alias("count"),
            avg("value").alias("avg_value"),
            spark_sum("value").alias("sum_value")
        )
        result = agg_df.collect()
        listener.on_stage_end(2, "SUCCESS")
        listener.on_job_end(2, "SUCCESS")

        print(f"✅ Aggregation completed with {len(result)} groups")

        # Job 3: Filtering
        print("\n📊 Job 3: Filtering data...")
        listener.on_job_start(3)

        # Stage 3: Filter
        listener.on_stage_start(3, 50)
        filtered_df = df.filter(col("value") > 50000)
        count2 = filtered_df.count()
        listener.on_stage_end(3, "SUCCESS")
        listener.on_job_end(3, "SUCCESS")

        print(f"✅ Filtered to {count2} rows")

        listener.log_event("TEST_JOB_COMPLETED", {
            "total_rows": count1,
            "filtered_rows": count2,
            "aggregation_groups": len(result)
        })

        print("\n🎉 Test job completed successfully!")
        return True

    except Exception as e:
        listener.log_event("TEST_JOB_FAILED", {"error": str(e)})
        print(f"❌ Test job failed: {e}")
        return False

# Run the test
success = run_test_job_with_listener()
```

### 3. Display Results

```python
# Cell 3: Show Listener Results
# Print the summary
listener.print_summary()

# Display events as a DataFrame for better visualization
import pandas as pd
from pyspark.sql import Row

if listener.events:
    # Convert events to Spark DataFrame
    events_df = spark.createDataFrame([Row(**event) for event in listener.events])

    print("\n📋 Events as DataFrame:")
    events_df.show(truncate=False)

    # Count events by type
    print("\n📈 Event Type Summary:")
    event_counts = events_df.groupBy("event_type").count().orderBy("count", ascending=False)
    event_counts.show()

    # Convert to Pandas for nicer display in Databricks
    pandas_df = events_df.toPandas()
    pandas_df['readable_time'] = pd.to_datetime(pandas_df['timestamp'], unit='ms')

    print("\n⏰ Timeline View:")
    display(pandas_df[['readable_time', 'event_type', 'details']].sort_values('timestamp'))
else:
    print("❌ No events captured!")
```

### 4. Advanced Test with Real Spark Context Monitoring

```python
# Cell 4: Advanced Listener with Real Spark Context Integration
class AdvancedSparkListener(SimpleSparkListener):
    def __init__(self, spark_session):
        super().__init__(spark_session)
        self.job_tracker = {}
        self.stage_tracker = {}

    def monitor_current_jobs(self):
        """Monitor currently running jobs"""
        try:
            status_tracker = self.sc.statusTracker()

            # Get active job IDs
            active_jobs = status_tracker.getActiveJobIds()
            print(f"🔄 Active Jobs: {list(active_jobs)}")

            # Get job information
            for job_id in active_jobs:
                job_info = status_tracker.getJobInfo(job_id)
                if job_info and job_id not in self.job_tracker:
                    self.job_tracker[job_id] = job_info
                    self.log_event("REAL_JOB_DETECTED", {
                        "job_id": job_id,
                        "status": job_info.status,
                        "num_stages": len(job_info.stageIds)
                    })

            # Get active stage information
            active_stages = status_tracker.getActiveStageIds()
            for stage_id in active_stages:
                stage_info = status_tracker.getStageInfo(stage_id)
                if stage_info and stage_id not in self.stage_tracker:
                    self.stage_tracker[stage_id] = stage_info
                    self.log_event("REAL_STAGE_DETECTED", {
                        "stage_id": stage_id,
                        "status": stage_info.status,
                        "num_tasks": stage_info.numTasks,
                        "num_active_tasks": stage_info.numActiveTasks,
                        "num_completed_tasks": stage_info.numCompletedTasks
                    })

        except Exception as e:
            self.log_event("MONITORING_ERROR", {"error": str(e)})

# Create advanced listener
advanced_listener = AdvancedSparkListener(spark)

def run_monitored_job():
    """Run job with real-time monitoring"""
    print("🔍 Starting monitored job...")

    # Monitor before job
    advanced_listener.monitor_current_jobs()

    # Run a compute-intensive job
    print("💪 Running compute-intensive operations...")

    # Create larger dataset
    large_df = spark.range(1, 1000000).toDF("id")
    large_df = large_df.withColumn("squared", col("id") * col("id"))
    large_df = large_df.withColumn("mod_result", col("id") % 100)

    # Monitor during job
    advanced_listener.monitor_current_jobs()

    # Force computation with multiple operations
    result1 = large_df.groupBy("mod_result").count().collect()
    advanced_listener.monitor_current_jobs()

    result2 = large_df.filter(col("squared") > 500000).count()
    advanced_listener.monitor_current_jobs()

    print(f"✅ Job completed. Groups: {len(result1)}, Filtered rows: {result2}")

    return result1, result2

# Run the monitored job
results = run_monitored_job()

# Show advanced results
print("\n🔍 Advanced Listener Results:")
advanced_listener.print_summary()
```

### 5. Simple Real-time Status Display

```python
# Cell 5: Real-time Status Display
import threading
import time

class RealTimeMonitor:
    def __init__(self, listener):
        self.listener = listener
        self.monitoring = False
        self.monitor_thread = None

    def start_monitoring(self, duration=30):
        """Start real-time monitoring for specified duration"""
        self.monitoring = True

        def monitor():
            end_time = time.time() + duration
            while self.monitoring and time.time() < end_time:
                try:
                    # Check Spark context status
                    sc = self.listener.sc
                    status_tracker = sc.statusTracker()

                    active_jobs = len(status_tracker.getActiveJobIds())
                    active_stages = len(status_tracker.getActiveStageIds())

                    if active_jobs > 0 or active_stages > 0:
                        print(f"⚡ {time.strftime('%H:%M:%S')} - Active Jobs: {active_jobs}, Active Stages: {active_stages}")

                    time.sleep(2)  # Check every 2 seconds

                except Exception as e:
                    print(f"❌ Monitoring error: {e}")
                    break

            print("🛑 Monitoring stopped")

        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
        print(f"🔄 Started real-time monitoring for {duration} seconds...")

    def stop_monitoring(self):
        """Stop monitoring"""
        self.monitoring = False

# Create monitor
monitor = RealTimeMonitor(advanced_listener)

# Start monitoring and run a job
monitor.start_monitoring(20)

# Run a job while monitoring
print("🚀 Running job with real-time monitoring...")
test_df = spark.range(1, 500000).toDF("number")
result = test_df.filter(col("number") % 1000 == 0).collect()
print(f"✅ Found {len(result)} numbers divisible by 1000")

# Wait a bit to see monitoring output
time.sleep(5)
monitor.stop_monitoring()
```

### 6. Final Verification

```python
# Cell 6: Verify Everything Works
print("🧪 FINAL VERIFICATION")
print("="*50)

# Check if we captured any events
print(f"📊 Simple Listener Events: {len(listener.events)}")
print(f"📊 Advanced Listener Events: {len(advanced_listener.events)}")

# Show last few events from each listener
if listener.events:
    print("\n📋 Last 3 Simple Listener Events:")
    for event in listener.events[-3:]:
        print(f"  • {event['event_type']}: {event['details']}")

if advanced_listener.events:
    print("\n📋 Last 3 Advanced Listener Events:")
    for event in advanced_listener.events[-3:]:
        print(f"  • {event['event_type']}: {event['details']}")

# Test basic Spark functionality
print(f"\n🔗 Spark Context Status: {spark.sparkContext.statusTracker()}")
print(f"📱 Application ID: {spark.sparkContext.applicationId}")
print(f"🏷️  Application Name: {spark.sparkContext.appName}")

print("\n✅ Spark Listener Test Completed!")
print("   If you see events above, your listener is working! 🎉")
```

## How to Use in Databricks

1. **Create a new Databricks notebook**
2. **Copy each cell above into separate cells**
3. **Run cells in order (1 → 2 → 3 → 4 → 5 → 6)**
4. **Watch the console output for listener events**

## Expected Output

You should see output like:
```
🚀 Listener initialized for: Databricks Shell
📱 Application ID: application_1234567890123_4567

⏰ 14:30:15 - JOB_STARTED
   📋 Details: {'job_id': 1}

⏰ 14:30:16 - STAGE_STARTED
   📋 Details: {'stage_id': 1, 'num_tasks': 100}

✅ Spark Listener Test Completed!
   If you see events above, your listener is working! 🎉
```

## Troubleshooting

If events aren't captured:
1. **Check Spark session**: Make sure `spark = SparkSession.getActiveSession()` returns a valid session
2. **Run with actions**: Ensure your Spark operations trigger actual computations (`.count()`, `.collect()`, etc.)
3. **Check console output**: Events are printed immediately for debugging

This simple test will confirm if Spark listeners work in your Databricks environment! 🚀