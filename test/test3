python# Cell 1: Simple Spark Listener Class
import time
import json
from pyspark.sql import SparkSession

class SimpleSparkListener:
    def __init__(self, spark_session):
        self.spark = spark_session
        self.sc = spark_session.sparkContext
        self.application_id = self.sc.applicationId
        self.application_name = self.sc.appName
        self.events = []

        print(f"🚀 Listener initialized for: {self.application_name}")
        print(f"📱 Application ID: {self.application_id}")

    def log_event(self, event_type, details=None):
        """Log events with timestamp"""
        timestamp = int(time.time() * 1000)
        event = {
            'timestamp': timestamp,
            'application_id': self.application_id,
            'event_type': event_type,
            'details': details or {}
        }
        self.events.append(event)

        # Print to console for immediate feedback
        print(f"⏰ {time.strftime('%H:%M:%S')} - {event_type}")
        if details:
            print(f"   📋 Details: {details}")

    def on_job_start(self, job_id):
        self.log_event("JOB_STARTED", {"job_id": job_id})

    def on_job_end(self, job_id, status):
        self.log_event("JOB_ENDED", {"job_id": job_id, "status": status})

    def on_stage_start(self, stage_id, num_tasks):
        self.log_event("STAGE_STARTED", {"stage_id": stage_id, "num_tasks": num_tasks})

    def on_stage_end(self, stage_id, status):
        self.log_event("STAGE_COMPLETED", {"stage_id": stage_id, "status": status})

    def get_events(self):
        """Return all captured events"""
        return self.events

    def print_summary(self):
        """Print a summary of all events"""
        print("\n" + "="*50)
        print("🔍 LISTENER SUMMARY")
        print("="*50)

        for event in self.events:
            time_str = time.strftime('%H:%M:%S', time.localtime(event['timestamp']/1000))
            print(f"{time_str} | {event['event_type']} | {event['details']}")

        print(f"\n📊 Total Events Captured: {len(self.events)}")
        print("="*50)

# Initialize the listener
spark = SparkSession.getActiveSession()
listener = SimpleSparkListener(spark)

==

# Cell 2: Test Spark Job with Listener
import random
from pyspark.sql.functions import col, count, avg, sum as spark_sum

def run_test_job_with_listener():
    """Run a test Spark job with manual listener tracking"""

    print("🏁 Starting Test Job...")
    listener.log_event("TEST_JOB_STARTED")

    try:
        # Job 1: Create sample data
        print("\n📊 Job 1: Creating sample data...")
        listener.on_job_start(1)

        # Stage 1: Generate data
        listener.on_stage_start(1, 100)
        df = spark.range(1, 100000).toDF("id")
        df = df.withColumn("value", col("id") * 2)
        df = df.withColumn("category", col("id") % 10)
        df.cache()  # This will trigger an action
        count1 = df.count()
        listener.on_stage_end(1, "SUCCESS")
        listener.on_job_end(1, "SUCCESS")

        print(f"✅ Created DataFrame with {count1} rows")

        # Job 2: Aggregation
        print("\n📊 Job 2: Running aggregations...")
        listener.on_job_start(2)

        # Stage 2: Aggregation
        listener.on_stage_start(2, 10)
        agg_df = df.groupBy("category").agg(
            count("id").alias("count"),
            avg("value").alias("avg_value"),
            spark_sum("value").alias("sum_value")
        )
        result = agg_df.collect()
        listener.on_stage_end(2, "SUCCESS")
        listener.on_job_end(2, "SUCCESS")

        print(f"✅ Aggregation completed with {len(result)} groups")

        # Job 3: Filtering
        print("\n📊 Job 3: Filtering data...")
        listener.on_job_start(3)

        # Stage 3: Filter
        listener.on_stage_start(3, 50)
        filtered_df = df.filter(col("value") > 50000)
        count2 = filtered_df.count()
        listener.on_stage_end(3, "SUCCESS")
        listener.on_job_end(3, "SUCCESS")

        print(f"✅ Filtered to {count2} rows")

        listener.log_event("TEST_JOB_COMPLETED", {
            "total_rows": count1,
            "filtered_rows": count2,
            "aggregation_groups": len(result)
        })

        print("\n🎉 Test job completed successfully!")
        return True

    except Exception as e:
        listener.log_event("TEST_JOB_FAILED", {"error": str(e)})
        print(f"❌ Test job failed: {e}")
        return False

# Run the test
success = run_test_job_with_listener()

=====================

# Complete setJobGroup Example Notebook
# Copy this entire code into a single Databricks notebook cell and run

# =============================================================================
# SETUP AND INTRODUCTION
# =============================================================================

import pyspark.sql.functions as f
from pyspark.sql.functions import col, count, avg, sum as spark_sum, max as spark_max, desc
import time

# Get Spark context
spark = SparkSession.getActiveSession()
sc = spark.sparkContext

print("🔥 COMPLETE setJobGroup DEMONSTRATION")
print("="*50)
print(f"Application ID: {sc.applicationId}")
print(f"Start Time: {time.strftime('%H:%M:%S')}")
print("\nThis notebook demonstrates the power of setJobGroup for organizing Spark operations")
print("Watch the Spark UI (Jobs tab) while this runs to see the difference!")
print("="*50)

# =============================================================================
# PART 1: WITHOUT JOB GROUPS (THE PROBLEM)
# =============================================================================

print("\n\n❌ PART 1: WITHOUT JOB GROUPS (Messy & Confusing)")
print("-" * 55)
print("Running operations without job groups...")
print("These will show up as generic 'Job 0', 'Job 1', etc. in Spark UI")

# Create sample data without job groups
data_sample = spark.range(1, 50000).toDF("id")
data_sample = data_sample.withColumn("value", data_sample.id * 2)

# These operations will have meaningless names in Spark UI
count1 = data_sample.count()
print(f"  📊 Created {count1:,} sample records")

filtered_sample = data_sample.filter(data_sample.value > 25000)
count2 = filtered_sample.count()
print(f"  🔍 Filtered to {count2:,} records")

result_sample = filtered_sample.groupBy().sum("value").collect()
total_sample = result_sample[0][0]
print(f"  📈 Sum of values: {total_sample:,}")

print("\n❗ Check Spark UI now - you'll see confusing job names like:")
print("   'Job 0: count at Dataset.scala:3116'")
print("   'Job 1: collect at RDD.scala:945'")

# =============================================================================
# PART 2: WITH JOB GROUPS (THE SOLUTION)
# =============================================================================

print("\n\n✅ PART 2: WITH JOB GROUPS (Organized & Clear)")
print("-" * 50)

# Wait a moment to separate in Spark UI
time.sleep(2)

# Phase 1: Data Creation with Job Group
print("\n🔄 PHASE 1: DATA CREATION")
print("-" * 25)
sc.setJobGroup("01_Data_Creation", "Creating and preparing sample dataset")

data = spark.range(1, 100000).toDF("id")
data = data.withColumn("value", col("id") * 2)
data = data.withColumn("category", col("id") % 10)
data.cache()  # Cache for multiple operations

count1 = data.count()
print(f"✅ Created {count1:,} records with categories")

# Always clear job group when phase is complete
sc.setJobGroup(None, None)

# Phase 2: Data Validation with Job Group
print("\n🔄 PHASE 2: DATA VALIDATION")
print("-" * 25)
sc.setJobGroup("02_Data_Validation", "Checking data quality and completeness")

# Check for nulls
null_count = data.filter(col("id").isNull()).count()
print(f"   Null values in ID: {null_count}")

# Check data integrity
min_val = data.agg({"id": "min"}).collect()[0][0]
max_val = data.agg({"id": "max"}).collect()[0][0]
print(f"   Data range: {min_val:,} to {max_val:,}")

# Check categories
category_count = data.select("category").distinct().count()
print(f"   Categories found: {category_count}")

print("✅ Data validation completed successfully")
sc.setJobGroup(None, None)

# Phase 3: Data Processing with Job Group
print("\n🔄 PHASE 3: DATA PROCESSING")
print("-" * 25)
sc.setJobGroup("03_Data_Processing", "Filtering and transforming business data")

# Filter data based on business rules
high_value_data = data.filter(col("value") > 50000)
high_value_count = high_value_data.count()
print(f"   High-value records: {high_value_count:,}")

# Create additional business metrics
enriched_data = high_value_data.withColumn("value_squared", col("value") * col("value"))
enriched_data = enriched_data.withColumn("is_even", col("id") % 2 == 0)

enriched_count = enriched_data.count()
print(f"✅ Processed and enriched {enriched_count:,} records")
sc.setJobGroup(None, None)

# Phase 4: Business Analytics with Job Group
print("\n🔄 PHASE 4: BUSINESS ANALYTICS")
print("-" * 30)
sc.setJobGroup("04_Business_Analytics", "Computing key business metrics and insights")

# Calculate category-wise statistics
analytics_df = enriched_data.groupBy("category") \
    .agg(
        count("id").alias("record_count"),
        avg("value").alias("avg_value"),
        spark_sum("value").alias("total_value"),
        spark_max("value").alias("max_value")
    ) \
    .orderBy(desc("record_count"))

# Collect results for business insights
analytics_results = analytics_df.collect()

print("✅ Business Analytics Results:")
print("   Top 3 Categories by Volume:")
for i, row in enumerate(analytics_results[:3], 1):
    print(f"     {i}. Category {row['category']}: {row['record_count']:,} records")
    print(f"        Avg Value: {row['avg_value']:,.0f}, Total: {row['total_value']:,}")

# Calculate overall business metrics
total_records = enriched_data.count()
total_value = enriched_data.agg(spark_sum("value")).collect()[0][0]
avg_value = enriched_data.agg(avg("value")).collect()[0][0]

print(f"\n   📊 Overall Business Metrics:")
print(f"      Total Records Processed: {total_records:,}")
print(f"      Total Business Value: {total_value:,}")
print(f"      Average Value per Record: {avg_value:,.0f}")

sc.setJobGroup(None, None)

# =============================================================================
# PART 3: FIRE DATA ANALYSIS (YOUR ACTUAL USE CASE)
# =============================================================================

print("\n\n🔥 PART 3: FIRE DATA ANALYSIS (Real Use Case)")
print("=" * 55)

# Wait to separate in Spark UI timeline
time.sleep(2)

# Phase 1: Fire Data Ingestion
print("\n🔄 FIRE DATA PHASE 1: DATA INGESTION")
print("-" * 35)
sc.setJobGroup("Fire_01_Data_Ingestion", "Loading San Francisco fire incident data")

# Load your original fire data
fire_df = spark.read \
    .format("csv") \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .load("/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv")

# Cache for multiple operations (performance optimization)
fire_df.cache()

total_fire_records = fire_df.count()
print(f"✅ Loaded {total_fire_records:,} fire incident records")

# Show data structure
columns = fire_df.columns
print(f"   Columns available: {len(columns)}")
print(f"   Key columns: {', '.join(columns[:5])}...")

sc.setJobGroup(None, None)

# Phase 2: Fire Data Quality Analysis
print("\n🔄 FIRE DATA PHASE 2: DATA QUALITY")
print("-" * 35)
sc.setJobGroup("Fire_02_Data_Quality", "Validating fire data completeness and integrity")

# Check for missing call types (critical field)
null_call_types = fire_df.filter(col("CallType").isNull()).count()
missing_percentage = (null_call_types / total_fire_records) * 100
print(f"   Missing CallType: {null_call_types:,} ({missing_percentage:.1f}%)")

# Check date range
print("   Analyzing incident date range...")
if "CallDate" in fire_df.columns:
    date_stats = fire_df.select("CallDate").summary("min", "max").collect()
    print(f"   Date range: {date_stats[0][1]} to {date_stats[1][1]}")

# Check for duplicate incidents
duplicate_count = total_fire_records - fire_df.dropDuplicates().count()
print(f"   Duplicate records: {duplicate_count:,}")

print("✅ Data quality assessment completed")
sc.setJobGroup(None, None)

# Phase 3: Call Type Analysis (Your Original Queries)
print("\n🔄 FIRE DATA PHASE 3: CALL TYPE ANALYSIS")
print("-" * 40)
sc.setJobGroup("Fire_03_Call_Analysis", "Analyzing emergency call types and patterns")

# Your original Q1 query with job group
print("   Running Q1: Distinct call types analysis...")
q1_df = fire_df.where(col("CallType").isNotNull()) \
             .select("CallType") \
             .distinct()

distinct_call_types = q1_df.count()
print(f"✅ Q1 Result: Found {distinct_call_types} distinct emergency call types")

# Your original Q2 query with job group
print("\n   Running Q2: Call type enumeration...")
q2_df = fire_df.where(col("CallType").isNotNull()) \
             .selectExpr("CallType as distinct_call_type") \
             .distinct()

# Show sample call types
print("   Sample Emergency Call Types:")
sample_call_types = q2_df.limit(7).collect()
for i, row in enumerate(sample_call_types, 1):
    print(f"     {i}. {row['distinct_call_type']}")

q2_count = q2_df.count()
print(f"✅ Q2 Result: Confirmed {q2_count} distinct call types")

sc.setJobGroup(None, None)

# Phase 4: Advanced Fire Analytics
print("\n🔄 FIRE DATA PHASE 4: ADVANCED ANALYTICS")
print("-" * 40)
sc.setJobGroup("Fire_04_Advanced_Analytics", "Computing fire department operational insights")

# Top call types by frequency (business critical)
print("   Computing top emergency call types...")
top_call_types = fire_df.filter(col("CallType").isNotNull()) \
                       .groupBy("CallType") \
                       .agg(count("*").alias("incident_count")) \
                       .orderBy(desc("incident_count")) \
                       .limit(5)

top_call_results = top_call_types.collect()
print("✅ Top 5 Emergency Call Types:")
for i, row in enumerate(top_call_results, 1):
    percentage = (row['incident_count'] / total_fire_records) * 100
    print(f"   {i}. {row['CallType']}: {row['incident_count']:,} incidents ({percentage:.1f}%)")

# Response time analysis (if available)
print("\n   Analyzing response patterns...")
if "Delay" in fire_df.columns:
    response_analysis = fire_df.filter(col("Delay").isNotNull() & (col("Delay") > 0)) \
                              .agg(
                                  avg("Delay").alias("avg_response_time"),
                                  spark_max("Delay").alias("max_response_time"),
                                  count("*").alias("records_with_response_data")
                              ).collect()[0]

    print(f"   Average Response Time: {response_analysis['avg_response_time']:.1f} minutes")
    print(f"   Maximum Response Time: {response_analysis['max_response_time']:.1f} minutes")
    print(f"   Records with Response Data: {response_analysis['records_with_response_data']:,}")
else:
    print("   Response time data not available in this dataset")

# Geographic analysis (if available)
if "City" in fire_df.columns:
    city_analysis = fire_df.filter(col("City").isNotNull()) \
                          .groupBy("City") \
                          .count() \
                          .orderBy(desc("count")) \
                          .limit(3)

    city_results = city_analysis.collect()
    print("\n   Top Cities by Incident Count:")
    for i, row in enumerate(city_results, 1):
        print(f"     {i}. {row['City']}: {row['count']:,} incidents")

print("✅ Advanced fire analytics completed")
sc.setJobGroup(None, None)

# =============================================================================
# PART 4: FINAL SUMMARY AND VERIFICATION
# =============================================================================

print("\n\n🎯 PART 4: SUMMARY & VERIFICATION")
print("=" * 40)

# Final summary with job group
sc.setJobGroup("Final_Summary", "Compiling execution summary and results")

print("🎉 ANALYSIS EXECUTION COMPLETED!")
print("\n📊 EXECUTION SUMMARY:")
print(f"   Sample Data Records: {count1:,}")
print(f"   Fire Incident Records: {total_fire_records:,}")
print(f"   Distinct Emergency Types: {distinct_call_types}")
print(f"   Most Common Emergency: {top_call_results[0]['CallType']} ({top_call_results[0]['incident_count']:,} incidents)")

print("\n🔍 SPARK UI VERIFICATION:")
print("1. Open your Spark UI (click 'Spark UI' in cluster details)")
print("2. Navigate to the 'Jobs' tab")
print("3. You should now see clearly organized job groups:")
print()
print("   ✅ ORGANIZED JOB GROUPS:")
print("      01_Data_Creation: Creating and preparing sample dataset")
print("      02_Data_Validation: Checking data quality and completeness")
print("      03_Data_Processing: Filtering and transforming business data")
print("      04_Business_Analytics: Computing key business metrics and insights")
print("      Fire_01_Data_Ingestion: Loading San Francisco fire incident data")
print("      Fire_02_Data_Quality: Validating fire data completeness and integrity")
print("      Fire_03_Call_Analysis: Analyzing emergency call types and patterns")
print("      Fire_04_Advanced_Analytics: Computing fire department operational insights")
print("      Final_Summary: Compiling execution summary and results")
print()
print("   ❌ INSTEAD OF CONFUSING:")
print("      Job 0: count at Dataset.scala:3116")
print("      Job 1: collect at RDD.scala:945")
print("      Job 2: show at Dataset.scala:2856")
print("      Job 3: collect at DataFrame.scala:3377")

print("\n💡 KEY BENEFITS DEMONSTRATED:")
print("   ✅ Clear job organization and tracking")
print("   ✅ Easy debugging when jobs fail")
print("   ✅ Better team communication")
print("   ✅ Professional monitoring and logging")
print("   ✅ Business-meaningful progress updates")

print(f"\n📱 APPLICATION DETAILS:")
print(f"   Application ID: {sc.applicationId}")
print(f"   Application Name: {sc.appName}")
print(f"   Completion Time: {time.strftime('%H:%M:%S')}")

sc.setJobGroup(None, None)

print("\n" + "="*60)
print("🎯 setJobGroup DEMONSTRATION COMPLETE!")
print("Check your Spark UI to see the dramatic difference in job organization!")
print("="*60)

=======================

 Complete Notebook Structure:
Part 1: Without Job Groups (The Problem)

Shows messy, confusing job names
Demonstrates the chaos in Spark UI

Part 2: With Job Groups (The Solution)

4 organized phases with clear job groups
Sample data processing with business context

Part 3: Fire Data Analysis (Your Real Use Case)

Your original fire data queries with job groups
4 phases: Ingestion → Quality → Analysis → Advanced Analytics
Includes your Q1 and Q2 queries

Part 4: Summary & Verification

Shows what you should see in Spark UI
Highlights the benefits

🚀 How to Use:

Copy the entire code into a single Databricks notebook cell
Run the cell and watch the console output
Open Spark UI while it's running (click "Spark UI" in cluster)
Go to Jobs tab to see organized job names vs chaos

📊 What You'll See in Spark UI:
Instead of:
❌ Job 0: count at Dataset.scala:3116
❌ Job 1: collect at RDD.scala:945
❌ Job 2: show at Dataset.scala:2856
You'll see:
✅ 01_Data_Creation: Creating and preparing sample dataset
✅ Fire_01_Data_Ingestion: Loading San Francisco fire incident data
✅ Fire_03_Call_Analysis: Analyzing emergency call types and patterns
✅ Fire_04_Advanced_Analytics: Computing fire department operational insights
🎉 Expected Console Output:
🔥 COMPLETE setJobGroup DEMONSTRATION
==================================================
✅ Loaded 175,296 fire incident records
✅ Q1 Result: Found 32 distinct emergency call types
✅ Top 5 Emergency Call Types:
   1. Medical Incident: 67,423 incidents (38.5%)
   2. Alarms: 24,194 incidents (13.8%)
   3. Structure Fire: 5,698 incidents (3.3%)

🎯 setJobGroup DEMONSTRATION COMPLETE!
Check your Spark UI to see the dramatic difference in job organization!
This single notebook gives your team a complete, hands-on demonstration of how setJobGroup transforms chaotic Spark monitoring into professional, organized operations!