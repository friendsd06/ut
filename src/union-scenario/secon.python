from pyspark.sql import SparkSession
from pyspark.sql.functions import col, rand, randn, when, lit, date_add, current_date, struct, concat
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, ArrayType
from functools import reduce
import time

# Initialize Spark Session with limited driver memory
spark = SparkSession.builder.appName("UnionMemoryErrorDemo") \
    .config("spark.driver.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "10") \
    .getOrCreate()

# Define a complex schema for loan application data
base_schema = StructType([
    StructField("application_id", StringType(), False),
    StructField("applicant_info", StructType([
        StructField("name", StringType(), False),
        StructField("age", IntegerType(), False),
        StructField("address", StringType(), False)
    ]), False),
    StructField("loan_details", StructType([
        StructField("amount", DoubleType(), False),
        StructField("term", IntegerType(), False),
        StructField("interest_rate", DoubleType(), False)
    ]), False),
    StructField("financial_info", StructType([
        StructField("annual_income", DoubleType(), False),
        StructField("debt_to_income_ratio", DoubleType(), False),
        StructField("credit_score", IntegerType(), False)
    ]), False),
    StructField("application_date", DateType(), False),
    StructField("historical_payments", ArrayType(DoubleType()), False)
])

# Add 200 additional string columns to the schema
for i in range(200):
    base_schema = base_schema.add(f"additional_string_{i}", StringType(), True)

# Function to create a large, complex DataFrame with 200 additional string columns
def create_large_loan_df(id, rows=500000):  # 500,000 rows
    base_df = spark.range(rows).select(
        col("id").cast(StringType()).alias("application_id"),
        struct(
            lit(f"Applicant {id}").alias("name"),
            (rand() * 40 + 20).cast(IntegerType()).alias("age"),
            lit("123 Main St").alias("address")
        ).alias("applicant_info"),
        struct(
            (rand() * 1000000 + 10000).alias("amount"),
            when(rand() > 0.5, 360).otherwise(180).alias("term"),
            (rand() * 5 + 3).alias("interest_rate")
        ).alias("loan_details"),
        struct(
            (rand() * 200000 + 30000).alias("annual_income"),
            (rand() * 0.4 + 0.1).alias("debt_to_income_ratio"),
            (randn() * 100 + 650).cast(IntegerType()).alias("credit_score")
        ).alias("financial_info"),
        date_add(current_date(), -(rand() * 365).cast(IntegerType())).alias("application_date"),
        (array([rand() * 1000 for _ in range(12)])).alias("historical_payments")
    )

    # Add 200 additional string columns
    for i in range(200):
        base_df = base_df.withColumn(f"additional_string_{i}",
                                     concat(lit(f"AdditionalData_{i}_"),
                                            (rand() * 1000000).cast(StringType())))

    return base_df

print("Creating large loan application DataFrames...")
df1 = create_large_loan_df(1)
df2 = create_large_loan_df(2)
df3 = create_large_loan_df(3)
dataframes = [df1, df2, df3]

# Function to time and execute union operations with an action
def time_operation_with_action(name, operation, dfs):
    print(f"\n{name}:")
    start = time.time()
    try:
        result = operation(dfs)
        count = result.count()  # This is the action that will trigger execution
        duration = time.time() - start
        print(f"Total applications: {count}, Time: {duration:.2f} seconds")
        result.show(2, truncate=True)
    except Exception as e:
        print(f"Error occurred: {str(e)}")

# Python's reduce function with count action (very likely to cause memory error now)
time_operation_with_action("Python's reduce function with count action",
                           lambda dfs: reduce(lambda a, b: a.union(b), dfs),
                           dataframes)