from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
from airflow.models import DAG
from datetime import datetime

class DatabricksJobWithStatus(DatabricksRunNowOperator):
    def execute(self, context):
        print(f"\nStarting {self.task_id}...")
        run_id = super().execute(context)
        status = self.get_run_status(run_id)
        print(f"{self.task_id} Status: {status['state']['life_cycle_state']}")
        return run_id

with DAG('databricks_jobs_chain',
         start_date=datetime(2024, 1, 1),
         schedule_interval=None) as dag:

    # First Databricks job
    job1 = DatabricksJobWithStatus(
        task_id='first_job',
        databricks_conn_id='databricks_default',
        job_id='job1_id'  # Replace with your first job ID
    )

    # Second Databricks job
    job2 = DatabricksJobWithStatus(
        task_id='second_job',
        databricks_conn_id='databricks_default',
        job_id='job2_id'  # Replace with your second job ID
    )

    # Set job2 to run after job1
    job1 >> job2

    submit_run = DatabricksSubmitRunOperator(
            task_id='submit_databricks_run',
            databricks_conn_id='databricks_default',
            json={
                'existing_cluster_id': 'cluster-1234-abcdef',  # Replace with your actual Cluster ID
                'notebook_task': {
                    'notebook_path': '/Users/your-username/your-notebook',
                    # No base_parameters since no parameters are needed
                },
                # Optionally, you can specify other run parameters like timeout, libraries, etc.
            },
        )

        submit_run




from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator
from airflow.models.connection import Connection
from datetime import datetime

# Create DAG
with DAG('databricks_notebook_pipeline',
         start_date=datetime(2024, 1, 1),
         schedule_interval=None,
         catchup=False) as dag:

    # Create connection programmatically
    databricks_conn = Connection(
        conn_id="databricks_default",
        conn_type="databricks",
        host="https://your-workspace.cloud.databricks.com",
        password="your-token"
    )

    start_task = DummyOperator(task_id='start_pipeline')

    # Use the connection in operator
    notebook_task = DatabricksSubmitRunOperator(
        task_id='run_notebook',
        databricks_conn_id=databricks_conn.conn_id,
        json={
            'notebook_task': {
                'notebook_path': '/Users/your-username/your-notebook'
            },
            'existing_cluster_id': 'your-cluster-id'
        }
    )

    end_task = DummyOperator(task_id='end_pipeline')

    start_task >> notebook_task >> end_task