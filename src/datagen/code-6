from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.sensors.external_task_sensor import ExternalTaskSensor
from airflow.utils.trigger_rule import TriggerRule
import logging
import json
import os
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Default arguments
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['your-email@example.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
    'start_date': datetime(2024, 1, 1)
}

# Create DAG
dag = DAG(
    'secondary_feed_processing',
    default_args=default_args,
    description='Secondary Feed Processing Pipeline',
    schedule_interval=timedelta(days=1),
    catchup=False,
    tags=['feed_processing']
)

# Helper Functions
def save_processing_data(data, subdirectory, filename):
    """Save processing data to JSON file"""
    directory = Path(f"processing_data/{subdirectory}")
    directory.mkdir(parents=True, exist_ok=True)
    filepath = directory / f"{filename}.json"
    with open(filepath, 'w') as f:
        json.dump(data, f)
    return str(filepath)

# Task Functions
def check_feed_processing(**context):
    """Check if feed processing is allowed"""
    try:
        feed_id = context['task_instance'].xcom_pull(key='feed_id')
        # Add your feed processing check logic here
        allow_processing = True

        logger.info(f"Feed processing allowed: {allow_processing} for feed {feed_id}")
        return 'move_to_processing' if allow_processing else 'block_processing'
    except Exception as e:
        logger.error(f"Feed processing check failed: {str(e)}")
        return 'block_processing'

def move_to_processing(**context):
    """Move file to processing directory"""
    try:
        file_path = context['dag_run'].conf.get('file_path', '')
        if not file_path:
            raise ValueError("No file path provided")

        processing_dir = Path("processing_data/incoming")
        processing_dir.mkdir(parents=True, exist_ok=True)

        new_path = processing_dir / os.path.basename(file_path)
        logger.info(f"Moving file from {file_path} to {new_path}")

        context['task_instance'].xcom_push(key='processing_path', value=str(new_path))
        return True
    except Exception as e:
        logger.error(f"File move failed: {str(e)}")
        raise

def validate_file(**context):
    """Validate the file"""
    try:
        file_path = context['task_instance'].xcom_pull(key='processing_path')
        # Add your file validation logic here
        validation_passed = True

        logger.info(f"File validation {'passed' if validation_passed else 'failed'}")
        return 'register_level_id' if validation_passed else 'block_file'
    except Exception as e:
        logger.error(f"File validation failed: {str(e)}")
        return 'block_file'

def register_level_id(**context):
    """Register and get level ID"""
    try:
        file_path = context['task_instance'].xcom_pull(key='processing_path')
        level_id = f"LEVEL_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        level_data = {
            'level_id': level_id,
            'file_path': file_path,
            'timestamp': datetime.now().isoformat()
        }

        save_processing_data(level_data, 'levels', level_id)
        context['task_instance'].xcom_push(key='level_id', value=level_id)
        logger.info(f"Registered level ID: {level_id}")
        return True
    except Exception as e:
        logger.error(f"Level ID registration failed: {str(e)}")
        raise

def run_processing(**context):
    """Run transformation, derivation, and validation"""
    try:
        level_id = context['task_instance'].xcom_pull(key='level_id')

        processing_data = {
            'level_id': level_id,
            'transformation_status': 'completed',
            'derivation_status': 'completed',
            'validation_status': 'completed',
            'timestamp': datetime.now().isoformat()
        }

        save_processing_data(processing_data, 'processed', level_id)
        logger.info(f"Processing completed for level {level_id}")
        return True
    except Exception as e:
        logger.error(f"Processing failed: {str(e)}")
        raise

def check_validation_threshold(**context):
    """Check if validation breached threshold"""
    try:
        level_id = context['task_instance'].xcom_pull(key='level_id')
        # Add your threshold check logic here
        threshold_breached = False

        logger.info(f"Threshold check for level {level_id}: {'breached' if threshold_breached else 'passed'}")
        return 'wait_for_dmc' if threshold_breached else 'persist_data'
    except Exception as e:
        logger.error(f"Threshold check failed: {str(e)}")
        return 'wait_for_dmc'

def check_dmc_activation(**context):
    """Check DMC activation status"""
    try:
        level_id = context['task_instance'].xcom_pull(key='level_id')
        # Add your DMC activation check logic here
        is_activated = True

        logger.info(f"DMC activation status for level {level_id}: {'activated' if is_activated else 'not activated'}")
        return 'validation_join' if is_activated else 'block_processing'
    except Exception as e:
        logger.error(f"DMC activation check failed: {str(e)}")
        return 'block_processing'

def persist_data(**context):
    """Persist data to table"""
    try:
        level_id = context['task_instance'].xcom_pull(key='level_id')

        persistence_data = {
            'level_id': level_id,
            'status': 'persisted',
            'timestamp': datetime.now().isoformat()
        }

        save_processing_data(persistence_data, 'final', level_id)
        logger.info(f"Data persisted for level {level_id}")
        return True
    except Exception as e:
        logger.error(f"Data persistence failed: {str(e)}")
        raise

def notify_event_hub(**context):
    """Notify event hub"""
    try:
        level_id = context['task_instance'].xcom_pull(key='level_id')

        notification_data = {
            'level_id': level_id,
            'status': 'completed',
            'timestamp': datetime.now().isoformat()
        }

        save_processing_data(notification_data, 'notifications', level_id)
        logger.info(f"Event hub notified for level {level_id}")
        return True
    except Exception as e:
        logger.error(f"Notification failed: {str(e)}")
        raise

def confirm_status(**context):
    """Confirm status allows processing"""
    try:
        level_id = context['task_instance'].xcom_pull(key='level_id')
        # Add your status confirmation logic here
        status_confirmed = True

        logger.info(f"Status confirmation for level {level_id}: {'confirmed' if status_confirmed else 'not confirmed'}")
        return True
    except Exception as e:
        logger.error(f"Status confirmation failed: {str(e)}")
        raise

# Task Definitions
secondary_feed = DummyOperator(
    task_id='secondary_feed',
    dag=dag
)

check_feed_processing_task = BranchPythonOperator(
    task_id='check_feed_processing',
    python_callable=check_feed_processing,
    dag=dag
)

move_processing = PythonOperator(
    task_id='move_to_processing',
    python_callable=move_to_processing,
    dag=dag
)

validate_file_task = BranchPythonOperator(
    task_id='validate_file',
    python_callable=validate_file,
    dag=dag
)

register_id = PythonOperator(
    task_id='register_level_id',
    python_callable=register_level_id,
    dag=dag
)

run_processing_task = PythonOperator(
    task_id='run_processing',
    python_callable=run_processing,
    dag=dag
)

check_threshold = BranchPythonOperator(
    task_id='check_validation_threshold',
    python_callable=check_validation_threshold,
    dag=dag
)

wait_for_dmc = ExternalTaskSensor(
    task_id='wait_for_dmc',
    external_dag_id='dmc_approval',
    external_task_id='approve_processing',
    timeout=3600,
    dag=dag
)

check_dmc_activation_task = BranchPythonOperator(
    task_id='check_dmc_activation',
    python_callable=check_dmc_activation,
    dag=dag
)

persist_task = PythonOperator(
    task_id='persist_data',
    python_callable=persist_data,
    dag=dag
)

notify_hub = PythonOperator(
    task_id='notify_event_hub',
    python_callable=notify_event_hub,
    dag=dag
)

confirm_status_task = PythonOperator(
    task_id='confirm_status',
    python_callable=confirm_status,
    dag=dag
)

block_processing = DummyOperator(
    task_id='block_processing',
    dag=dag
)

block_file = DummyOperator(
    task_id='block_file',
    dag=dag
)

# Join nodes
processing_join = DummyOperator(
    task_id='processing_join',
    trigger_rule=TriggerRule.ONE_SUCCESS,
    dag=dag
)

validation_join = DummyOperator(
    task_id='validation_join',
    trigger_rule=TriggerRule.ONE_SUCCESS,
    dag=dag
)

# Define task dependencies
# Initial flow
secondary_feed >> check_feed_processing_task
check_feed_processing_task >> [move_processing, block_processing]
move_processing >> validate_file_task
validate_file_task >> [register_id, block_file]

# Registration and processing flow
register_id >> run_processing_task

# Main processing flow
run_processing_task >> check_threshold
check_threshold >> [persist_task, wait_for_dmc]

# DMC approval path
wait_for_dmc >> check_dmc_activation_task
check_dmc_activation_task >> [validation_join, block_processing]

# Join paths and continue
persist_task >> processing_join
validation_join >> processing_join
processing_join >> notify_hub
notify_hub >> confirm_status_task

# Set trigger rules
persist_task.trigger_rule = TriggerRule.ONE_SUCCESS
processing_join.trigger_rule = TriggerRule.ONE_SUCCESS
notify_hub.trigger_rule = TriggerRule.ALL_SUCCESS

# DAG Documentation
dag.doc_md = """
# Secondary Feed Processing Pipeline

This DAG implements a secondary feed processing workflow with:

## Main Components:
1. Feed processing allowance check
2. File validation
3. Level ID registration
4. Processing (transformation, derivation, validation)
5. DMC activation handling
6. Data persistence
7. Status confirmation

## Usage:
Trigger the DAG with:
```python
{
    'file_path': '/path/to/your/file',
    'feed_id': 'your_feed_identifier'
}
```

## Error Handling:
- Email notifications on failure
- Automatic retries
- Comprehensive logging
"""

if __name__ == "__main__":
    dag.test()