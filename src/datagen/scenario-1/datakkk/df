from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.operators.python import PythonOperator
from airflow.models import Connection
from airflow import settings
from datetime import datetime, timedelta
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# MinIO Configuration
MINIO_CONFIG = {
    'conn_id': 'minio_conn',
    'access_key': 'minioadmin',     # Your MinIO access key
    'secret_key': 'minioadmin',     # Your MinIO secret key
    'endpoint_url': 'http://localhost:9000'  # Your MinIO server URL
}

S3_BUCKET = 'your-bucket-name'
S3_PREFIX = 'input/'

def setup_minio_connection():
    """Create MinIO connection in Airflow"""
    try:
        # Create connection object with MinIO specific configuration
        conn = Connection(
            conn_id=MINIO_CONFIG['conn_id'],
            conn_type='aws',
            login=MINIO_CONFIG['access_key'],
            password=MINIO_CONFIG['secret_key'],
            extra={
                'host': MINIO_CONFIG['endpoint_url'],
                'verify': False,  # For self-signed certificates
                'endpoint_url': MINIO_CONFIG['endpoint_url']
            }
        )

        # Get Airflow session
        session = settings.Session()

        # Check if connection exists
        existing_conn = session.query(Connection).filter(
            Connection.conn_id == MINIO_CONFIG['conn_id']
        ).first()

        if existing_conn:
            logger.info(f"Updating existing connection: {MINIO_CONFIG['conn_id']}")
            session.delete(existing_conn)
            session.commit()

        # Add new connection
        session.add(conn)
        session.commit()
        logger.info(f"MinIO connection '{MINIO_CONFIG['conn_id']}' created/updated successfully")

    except Exception as e:
        logger.error(f"Error setting up MinIO connection: {e}")
        raise

def process_detected_file(**context):
    """Process detected MinIO file"""
    try:
        s3_key = context['task_instance'].xcom_pull(task_ids='wait_for_file')
        logger.info(f"Processing file: {s3_key}")

        if not s3_key:
            raise ValueError("No file key received from sensor")

        return s3_key

    except Exception as e:
        logger.error(f"Error processing file: {e}")
        raise

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'provide_context': True
}

# Create and setup DAG
with DAG(
    'minio_sensor_dag',
    default_args=default_args,
    schedule_interval='*/5 * * * *',  # Every 5 minutes
    catchup=False
) as dag:

    # Setup MinIO Connection
    setup_connection = PythonOperator(
        task_id='setup_minio_connection',
        python_callable=setup_minio_connection
    )

    # MinIO File Sensor
    minio_sensor = S3KeySensor(
        task_id='wait_for_file',
        bucket_key=f"{S3_PREFIX}*.csv",
        wildcard_match=True,
        bucket_name=S3_BUCKET,
        aws_conn_id=MINIO_CONFIG['conn_id'],
        verify=False,  # Important for MinIO
        timeout=60 * 5,
        poke_interval=30,
        mode='poke'
    )

    # Process File
    process_file = PythonOperator(
        task_id='process_file',
        python_callable=process_detected_file
    )

    # Set dependencies
    setup_connection >> minio_sensor >> process_file