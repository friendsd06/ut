# ingestion_1_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def ingest_data_1(**context):
    """Function to perform ingestion for source 1"""
    print("Starting ingestion for source 1...")
    # Add your ingestion logic here
    print("Ingestion 1 completed successfully")
    # Pass success status to XCom
    context['task_instance'].xcom_push(key='ingestion_1_status', value='success')

dag_1 = DAG(
    'ingestion_1_dag',
    default_args=default_args,
    description='Data ingestion DAG for source 1',
    schedule_interval='0 0 * * *',
    catchup=False
)

ingest_task_1 = PythonOperator(
    task_id='ingest_data_1',
    python_callable=ingest_data_1,
    provide_context=True,
    dag=dag_1
)

trigger_orchestrator = TriggerDagRunOperator(
    task_id='trigger_orchestrator',
    trigger_dag_id='orchestrator_dag',
    conf={'trigger_source': 'ingestion_1'},
    dag=dag_1
)

ingest_task_1 >> trigger_orchestrator

# ingestion_2_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def ingest_data_2(**context):
    """Function to perform ingestion for source 2"""
    print("Starting ingestion for source 2...")
    # Add your ingestion logic here
    print("Ingestion 2 completed successfully")
    # Pass success status to XCom
    context['task_instance'].xcom_push(key='ingestion_2_status', value='success')

dag_2 = DAG(
    'ingestion_2_dag',
    default_args=default_args,
    description='Data ingestion DAG for source 2',
    schedule_interval='0 0 * * *',
    catchup=False
)

ingest_task_2 = PythonOperator(
    task_id='ingest_data_2',
    python_callable=ingest_data_2,
    provide_context=True,
    dag=dag_2
)

trigger_orchestrator = TriggerDagRunOperator(
    task_id='trigger_orchestrator',
    trigger_dag_id='orchestrator_dag',
    conf={'trigger_source': 'ingestion_2'},
    dag=dag_2
)

ingest_task_2 >> trigger_orchestrator

# ingestion_3_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def ingest_data_3(**context):
    """Function to perform ingestion for source 3"""
    print("Starting ingestion for source 3...")
    # Add your ingestion logic here
    print("Ingestion 3 completed successfully")
    # Pass success status to XCom
    context['task_instance'].xcom_push(key='ingestion_3_status', value='success')

dag_3 = DAG(
    'ingestion_3_dag',
    default_args=default_args,
    description='Data ingestion DAG for source 3',
    schedule_interval='0 0 * * *',
    catchup=False
)

ingest_task_3 = PythonOperator(
    task_id='ingest_data_3',
    python_callable=ingest_data_3,
    provide_context=True,
    dag=dag_3
)

trigger_orchestrator = TriggerDagRunOperator(
    task_id='trigger_orchestrator',
    trigger_dag_id='orchestrator_dag',
    conf={'trigger_source': 'ingestion_3'},
    dag=dag_3
)

ingest_task_3 >> trigger_orchestrator

# orchestrator_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.utils.trigger_rule import TriggerRule
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def check_completion(**context):
    """
    Check if all ingestion jobs are complete and decide whether to trigger enrichment
    """
    dag_run = context['dag_run']
    trigger_source = dag_run.conf.get('trigger_source')

    # Get DagRun object
    task_instance = context['task_instance']

    # Store the current trigger source status
    if trigger_source == 'ingestion_1':
        task_instance.xcom_push(key='ingestion_1_complete', value=True)
    elif trigger_source == 'ingestion_2':
        task_instance.xcom_push(key='ingestion_2_complete', value=True)
    elif trigger_source == 'ingestion_3':
        task_instance.xcom_push(key='ingestion_3_complete', value=True)

    # Check if all ingestions are complete
    ingestion_1_complete = task_instance.xcom_pull(key='ingestion_1_complete', dag_id='orchestrator_dag') or False
    ingestion_2_complete = task_instance.xcom_pull(key='ingestion_2_complete', dag_id='orchestrator_dag') or False
    ingestion_3_complete = task_instance.xcom_pull(key='ingestion_3_complete', dag_id='orchestrator_dag') or False

    if all([ingestion_1_complete, ingestion_2_complete, ingestion_3_complete]):
        return 'trigger_enrichment'
    return 'skip_enrichment'

def skip_enrichment(**context):
    """Function to log when enrichment is skipped"""
    print("Not all ingestion jobs are complete yet. Skipping enrichment.")

orchestrator_dag = DAG(
    'orchestrator_dag',
    default_args=default_args,
    description='Orchestrator DAG to coordinate ingestion and enrichment',
    schedule_interval=None,  # Only triggered by other DAGs
    catchup=False
)

check_completion_task = BranchPythonOperator(
    task_id='check_completion',
    python_callable=check_completion,
    provide_context=True,
    dag=orchestrator_dag
)

skip_enrichment_task = PythonOperator(
    task_id='skip_enrichment',
    python_callable=skip_enrichment,
    provide_context=True,
    dag=orchestrator_dag
)

trigger_enrichment = TriggerDagRunOperator(
    task_id='trigger_enrichment',
    trigger_dag_id='enrichment_dag',
    dag=orchestrator_dag
)

check_completion_task >> [trigger_enrichment, skip_enrichment_task]

# enrichment_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

def perform_enrichment(**context):
    """Function to perform enrichment after all ingestions complete"""
    print("Starting enrichment process...")
    # Add your enrichment logic here
    print("Enrichment process completed successfully")

enrichment_dag = DAG(
    'enrichment_dag',
    default_args=default_args,
    description='Enrichment DAG triggered after all ingestions complete',
    schedule_interval=None,  # Only triggered by orchestrator
    catchup=False
)

enrichment_task = PythonOperator(
    task_id='perform_enrichment',
    python_callable=perform_enrichment,
    provide_context=True,
    dag=enrichment_dag
)