
INSERT INTO dataproduct (productid, productname) VALUES ('LOAN_PERS_01', 'P_LOAN');


INSERT INTO dataproductline (productlineid, productid, productlinename)
VALUES
    -- Personal Loans
    ('PERS_UNSEC_01', 'LOAN_PERS_01', 'UNSECURED_PERSONAL_LOAN');


INSERT INTO datasets (datasetid, productlineid, datasetname)
VALUES
('com.fintech-banking.loan.customer.raw.v1', 'PERS_UNSEC_01', 'com.fintech-banking.loan.customer.raw.v1');


INSERT INTO pipeline (
    pipelineid,
    pipelinename,
    schedule_interval,
    start_date,
    catchup,
    max_active_runs,
    concurrency,
    retries,
    retry_delay_minutes,
    dag_timeout_minutes,
    owner,
    email_on_failure,
    email_on_retry,
    notification_emails,
    is_active
)
VALUES
    -- Loan Application Processing Pipeline
    (
        'loan_application_etl',
        'Loan Application ETL Pipeline',
        '0 */4 * * *',  -- Every 4 hours
        '2024-01-01 00:00:00',
        FALSE,
        1,
        2,
        3,
        10,
        120,
        'data_engineering',
        TRUE,
        TRUE,
        ARRAY['data_alerts@company.com'],
        TRUE
    ),
    -- Loan Risk Assessment Pipeline
    (
        'loan_risk_analytics',
        'Loan Risk Analytics Pipeline',
        '0 0 * * *',    -- Daily at midnight
        '2024-01-01 00:00:00',
        FALSE,
        1,
        3,
        3,
        15,
        180,
        'data_engineering',
        TRUE,
        TRUE,
        ARRAY['risk_team@company.com', 'data_alerts@company.com'],
        TRUE
    );

=====================================

INSERT INTO pipelinedatasets (pipelineid, datasetid, datasetentrytype)
VALUES
    -- External Data Ingestion Pipeline
    ('loan_application_etl', 'com.fintech-banking.loan.customer.raw.v1', 'IN'),

    -- Raw Data Processing Pipeline
    ('loan_risk_analytics', 'com.fintech-banking.loan.customer.raw.v1', 'IN'),




--------------------------------------------------

INSERT INTO tasks (
    taskid,
    taskname,
    taskgroup,
    pipelineid,
    operator_type,
    dependencytype,
    task_params,
    trigger_rule,
    task_timeout_minutes,
    task_retries,
    retry_delay,
    task_priority_weight,
    task_queue,
    pool,
    execution_timeout,
    doc,
    email_on_failure,
    email_on_retry,
    status,
    python_callable
)
VALUES
    -- Raw Data Ingestion Task
    (
        'task_raw_data_ingest',
        'Ingest Raw Dataset',
        NULL,
        'loan_application_etl',
        'PythonOperator',
        'SEQUENTIAL',
        '{
            "source_path": "/data/raw/dataset",
            "file_format": "csv",
            "validation_check": true
        }'::jsonb,
        'all_success',
        30,
        3,
        '00:05:00'::INTERVAL,
        1,
        'data_processing',
        'ingestion_pool',
        '00:45:00'::INTERVAL,
        'Task to ingest raw dataset from source system',
        TRUE,
        FALSE,
        NULL,
        'ingest_loan_application_data'
    ),

    -- Data Cleaning Task
    (
        'task_data_cleaning',
        'Clean Dataset',
        NULL,
        'loan_application_etl',
        'PythonOperator',
        'SEQUENTIAL',
        '{
            "remove_nulls": true,
            "remove_duplicates": true,
            "output_format": "parquet"
        }'::jsonb,
        'all_success',
        40,
        3,
        '00:08:00'::INTERVAL,
        2,
        'data_processing',
        'processing_pool',
        '00:50:00'::INTERVAL,
        'Task to clean and prepare dataset',
        TRUE,
        FALSE,
        NULL,
        'clean_loan_application_data'
    ),

    -- Feature Engineering Task
    (
        'task_feature_engineering',
        'Create Features',
        NULL,
        'loan_application_etl',
        'PythonOperator',
        'SEQUENTIAL',
        '{
            "feature_list": ["feature1", "feature2", "feature3"],
            "encoding_method": "one_hot",
            "scaling": true
        }'::jsonb,
        'all_success',
        35,
        3,
        '00:07:00'::INTERVAL,
        2,
        'data_processing',
        'processing_pool',
        '00:45:00'::INTERVAL,
        'Task to engineer features from cleaned dataset',
        TRUE,
        FALSE,
        NULL,
        'engineer_loan_features'
    ),

    -- Data Validation Task
    (
        'task_data_validation',
        'Validate Processed Data',
        NULL,
        'loan_application_etl',
        'PythonOperator',
        'SEQUENTIAL',
        '{
            "quality_checks": ["completeness", "accuracy", "consistency"],
            "threshold": 0.95,
            "generate_report": true
        }'::jsonb,
        'all_success',
        25,
        3,
        '00:05:00'::INTERVAL,
        2,
        'data_processing',
        'validation_pool',
        '00:30:00'::INTERVAL,
        'Task to validate processed dataset quality',
        TRUE,
        FALSE,
        NULL,
        'validate_loan_data'
    ),

    -- Data Export Task
    (
        'task_data_export',
        'Export Processed Data',
        NULL,
        'loan_application_etl',
        'PythonOperator',
        'SEQUENTIAL',
        '{
            "export_path": "/data/processed/",
            "format": "parquet",
            "compression": "snappy",
            "partition_by": ["date"]
        }'::jsonb,
        'all_success',
        30,
        3,
        '00:06:00'::INTERVAL,
        2,
        'data_processing',
        'export_pool',
        '00:40:00'::INTERVAL,
        'Task to export processed dataset to target location',
        TRUE,
        FALSE,
        NULL,
        'export_processed_loan_data'
    );

==========================================================================

-- Insert task mappings for each pipeline and dataset combination
INSERT INTO task_dataset_mapping (taskid, datasetid, pipelineid, created_at)
VALUES
    -- External Ingestion Pipeline Tasks
    ('task_raw_data_ingest', 'com.fintech-banking.loan.customer.raw.v1', 'loan_application_etl', '2024-01-01 00:00:00');



=======================


INSERT INTO taskdependencies (
    pipelineid,
    taskid,
    dependenttaskid,
    created_at
)
VALUES
    -- Raw Data Ingestion (initial task - no parent)
    (
        'loan_application_etl',
        'task_raw_data_ingest',
        NULL,
        CURRENT_TIMESTAMP
    ),

    -- Data Cleaning depends on Raw Data Ingestion
    (
        'loan_application_etl',
        'task_data_cleaning',
        'task_raw_data_ingest',
        CURRENT_TIMESTAMP
    ),

    -- Feature Engineering depends on Data Cleaning
    (
        'loan_application_etl',
        'task_feature_engineering',
        'task_data_cleaning',
        CURRENT_TIMESTAMP
    ),

    -- Data Validation depends on Feature Engineering
    (
        'loan_application_etl',
        'task_data_validation',
        'task_feature_engineering',
        CURRENT_TIMESTAMP
    ),

    -- Data Export depends on Data Validation
    (
        'loan_application_etl',
        'task_data_export',
        'task_data_validation',
        CURRENT_TIMESTAMP
    );


=============================================


INSERT INTO tasks (
    taskid,
    taskname,
    taskgroup,
    pipelineid,
    operator_type,
    dependencytype,
    task_params,
    trigger_rule,
    task_timeout_minutes,
    task_retries,
    retry_delay,
    task_priority_weight,
    task_queue,
    pool,
    execution_timeout,
    doc,
    email_on_failure,
    email_on_retry,
    status,
    python_callable
)
VALUES
    -- Initial Data Ingestion Task
    (
        'task_loan_data_ingestion',
        'Ingest Loan Application Data',
        NULL,
        'loan_risk_analytics',
        'PythonOperator',
        'SEQUENTIAL',
        '{
            "source_path": "/data/raw/loan_applications",
            "file_format": "parquet",
            "validation_check": true
        }'::jsonb,
        'all_success',
        30,
        3,
        '00:05:00'::INTERVAL,
        1,
        'risk_analytics',
        'ingestion_pool',
        '00:45:00'::INTERVAL,
        'Task to ingest loan application data for risk analysis',
        TRUE,
        FALSE,
        NULL,
        'ingest_loan_risk_data'
    ),

    -- Credit Score Analysis Task (Parallel)
    (
        'task_credit_score_analysis',
        'Analyze Credit Scores',
        NULL,
        'loan_risk_analytics',
        'PythonOperator',
        'PARALLEL',
        '{
            "score_types": ["FICO", "VantageScore"],
            "min_threshold": 650
        }'::jsonb,
        'all_success',
        25,
        3,
        '00:05:00'::INTERVAL,
        2,
        'risk_analytics',
        'analysis_pool',
        '00:30:00'::INTERVAL,
        'Task to analyze credit scores',
        TRUE,
        FALSE,
        NULL,
        'analyze_credit_scores'
    ),

    -- Income Verification Task (Parallel)
    (
        'task_income_verification',
        'Verify Income Data',
        NULL,
        'loan_risk_analytics',
        'PythonOperator',
        'PARALLEL',
        '{
            "income_sources": ["salary", "business", "investments"],
            "verification_docs": ["payslips", "tax_returns"]
        }'::jsonb,
        'all_success',
        25,
        3,
        '00:05:00'::INTERVAL,
        2,
        'risk_analytics',
        'verification_pool',
        '00:30:00'::INTERVAL,
        'Task to verify income documentation',
        TRUE,
        FALSE,
        NULL,
        'verify_income_data'
    ),

    -- Risk Score Calculation Task
    (
        'task_risk_score_calculation',
        'Calculate Risk Score',
        NULL,
        'loan_risk_analytics',
        'PythonOperator',
        'SEQUENTIAL',
        '{
            "risk_factors": ["credit_score", "income", "debt_ratio", "employment_history"],
            "weight_factors": {"credit_score": 0.4, "income": 0.3, "debt_ratio": 0.2, "employment_history": 0.1}
        }'::jsonb,
        'all_success',
        30,
        3,
        '00:07:00'::INTERVAL,
        2,
        'risk_analytics',
        'calculation_pool',
        '00:40:00'::INTERVAL,
        'Task to calculate final risk score',
        TRUE,
        FALSE,
        NULL,
        'calculate_risk_score'
    ),

    -- Risk Assessment Branch Task
    (
        'task_risk_assessment_branch',
        'Evaluate Risk Level',
        NULL,
        'loan_risk_analytics',
        'BranchPythonOperator',
        'CONDITIONAL',
        '{
            "conditions": {
                "high_risk": "risk_score < 0.4",
                "medium_risk": "risk_score >= 0.4 AND risk_score < 0.7",
                "low_risk": "risk_score >= 0.7"
            }
        }'::jsonb,
        'all_success',
        20,
        3,
        '00:05:00'::INTERVAL,
        2,
        'risk_analytics',
        'branch_pool',
        '00:25:00'::INTERVAL,
        'Task to evaluate risk level and branch to appropriate processing',
        TRUE,
        FALSE,
        NULL,
        'evaluate_risk_level'
    ),

    -- Additional Verification Task (For High Risk)
    (
        'task_additional_verification',
        'Perform Additional Verification',
        NULL,
        'loan_risk_analytics',
        'PythonOperator',
        'CONDITIONAL',
        '{
            "verification_steps": ["employment_verification", "asset_verification", "reference_checks"],
            "documentation_required": true
        }'::jsonb,
        'all_success',
        45,
        3,
        '00:10:00'::INTERVAL,
        3,
        'risk_analytics',
        'verification_pool',
        '01:00:00'::INTERVAL,
        'Additional verification for high-risk applications',
        TRUE,
        FALSE,
        NULL,
        'perform_additional_verification'
    ),

    -- Final Risk Report Task
    (
        'task_generate_risk_report',
        'Generate Risk Analysis Report',
        NULL,
        'loan_risk_analytics',
        'PythonOperator',
        'SEQUENTIAL',
        '{
            "report_format": "pdf",
            "include_sections": ["summary", "credit_analysis", "income_analysis", "risk_scores", "recommendations"],
            "store_location": "/data/risk_reports/"
        }'::jsonb,
        'all_success',
        30,
        3,
        '00:08:00'::INTERVAL,
        2,
        'risk_analytics',
        'reporting_pool',
        '00:45:00'::INTERVAL,
        'Task to generate comprehensive risk analysis report',
        TRUE,
        FALSE,
        NULL,
        'generate_risk_report'
    );

INSERT INTO taskdependencies (
    pipelineid,
    taskid,
    dependenttaskid,
    created_at
)
VALUES
    -- Initial ingestion task (no dependency)
    (
        'loan_risk_analytics',
        'task_loan_data_ingestion',
        NULL,
        CURRENT_TIMESTAMP
    ),

    -- Parallel tasks depending on ingestion
    (
        'loan_risk_analytics',
        'task_credit_score_analysis',
        'task_loan_data_ingestion',
        CURRENT_TIMESTAMP
    ),
    (
        'loan_risk_analytics',
        'task_income_verification',
        'task_loan_data_ingestion',
        CURRENT_TIMESTAMP
    ),

    -- Risk score calculation depends on credit score analysis
    -- (We'll use one task as the main dependency for sequential flow)
    (
        'loan_risk_analytics',
        'task_risk_score_calculation',
        'task_credit_score_analysis',
        CURRENT_TIMESTAMP
    ),

    -- Risk assessment branch depends on risk score calculation
    (
        'loan_risk_analytics',
        'task_risk_assessment_branch',
        'task_risk_score_calculation',
        CURRENT_TIMESTAMP
    ),

    -- Additional verification depends on risk assessment (conditional)
    (
        'loan_risk_analytics',
        'task_additional_verification',
        'task_risk_assessment_branch',
        CURRENT_TIMESTAMP
    ),

    -- Final report depends on risk assessment
    (
        'loan_risk_analytics',
        'task_generate_risk_report',
        'task_risk_assessment_branch',
        CURRENT_TIMESTAMP
    );