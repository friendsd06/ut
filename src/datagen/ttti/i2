# deposit_ingestion_dag.py

from airflow import DAG
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator
from airflow.models import Variable
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 1, 1),
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': True
}

with DAG(
    'deposit_ingestion_dag',
    default_args=default_args,
    schedule_interval='0 1 * * *',  # 1 AM UTC
    catchup=False,
    tags=['ingestion', 'deposit']
) as dag:

    # Wait for deposit file in S3
    wait_for_deposit_file = S3KeySensor(
        task_id='wait_for_deposit_file',
        bucket_name=Variable.get('s3_bucket_name'),
        bucket_key='deposits/{{ds}}/deposit_data.parquet',
        aws_conn_id='aws_default',
        poke_interval=300,  # 5 minutes
        timeout=3600  # 1 hour
    )

    # Process deposit data in Databricks
    process_deposit_data = DatabricksSubmitRunOperator(
        task_id='process_deposit_data',
        databricks_conn_id='databricks_default',
        json={
            'existing_cluster_id': Variable.get('databricks_cluster_id'),
            'notebook_task': {
                'notebook_path': '/deposits/process_deposit_data',
                'base_parameters': {
                    'date': '{{ds}}',
                    'source_path': 'deposits/{{ds}}/deposit_data.parquet'
                }
            }
        }
    )

    # Set dependencies
    wait_for_deposit_file >> process_deposit_data