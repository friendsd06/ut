<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
         http://maven.apache.org/xsd/maven-4.0.0.xsd">

    <modelVersion>4.0.0</modelVersion>

    <!-- ============================================= -->
    <!-- Project Coordinates                          -->
    <!-- ============================================= -->
    <groupId>com.databricks.dialect</groupId>
    <artifactId>databricks-jdbc-dialect</artifactId>
    <version>1.0.0</version>
    <packaging>jar</packaging>

    <name>Databricks JDBC Dialect</name>
    <description>JDBC Dialect for Apache Spark to handle all Databricks data types</description>
    <url>https://github.com/yourcompany/databricks-jdbc-dialect</url>

    <!-- ============================================= -->
    <!-- Properties                                   -->
    <!-- ============================================= -->
    <properties>
        <!-- Java Version -->
        <maven.compiler.source>11</maven.compiler.source>
        <maven.compiler.target>11</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>

        <!-- Spark and Scala Versions -->
        <spark.version>3.5.0</spark.version>
        <scala.version>2.12</scala.version>
        <scala.full.version>2.12.18</scala.full.version>

        <!-- Plugin Versions -->
        <maven.compiler.plugin.version>3.11.0</maven.compiler.plugin.version>
        <maven.jar.plugin.version>3.3.0</maven.jar.plugin.version>
        <maven.source.plugin.version>3.3.0</maven.source.plugin.version>
        <maven.javadoc.plugin.version>3.6.2</maven.javadoc.plugin.version>
        <maven.surefire.plugin.version>3.2.2</maven.surefire.plugin.version>
    </properties>

    <!-- ============================================= -->
    <!-- Dependencies                                 -->
    <!-- ============================================= -->
    <dependencies>
        <!-- Spark SQL - Provided Scope (will be available at runtime) -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
        </dependency>

        <!-- Spark Catalyst - Provided Scope -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-catalyst_${scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
        </dependency>

        <!-- Scala Library - Provided Scope -->
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.full.version}</version>
            <scope>provided</scope>
        </dependency>

        <!-- SLF4J API for Logging -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>2.0.9</version>
            <scope>provided</scope>
        </dependency>

        <!-- JUnit for Testing -->
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.13.2</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <!-- ============================================= -->
    <!-- Build Configuration                          -->
    <!-- ============================================= -->
    <build>
        <finalName>${project.artifactId}-${project.version}</finalName>

        <sourceDirectory>src/main/java</sourceDirectory>
        <testSourceDirectory>src/test/java</testSourceDirectory>

        <plugins>
            <!-- Maven Compiler Plugin -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>${maven.compiler.plugin.version}</version>
                <configuration>
                    <source>11</source>
                    <target>11</target>
                    <encoding>UTF-8</encoding>
                    <compilerArgs>
                        <arg>-Xlint:unchecked</arg>
                        <arg>-Xlint:deprecation</arg>
                    </compilerArgs>
                </configuration>
            </plugin>

            <!-- Maven JAR Plugin -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-jar-plugin</artifactId>
                <version>${maven.jar.plugin.version}</version>
                <configuration>
                    <archive>
                        <manifest>
                            <addDefaultImplementationEntries>true</addDefaultImplementationEntries>
                            <addDefaultSpecificationEntries>true</addDefaultSpecificationEntries>
                        </manifest>
                        <manifestEntries>
                            <Spark-Version>${spark.version}</Spark-Version>
                            <Scala-Version>${scala.version}</Scala-Version>
                            <Build-Time>${maven.build.timestamp}</Build-Time>
                        </manifestEntries>
                    </archive>
                </configuration>
            </plugin>

            <!-- Maven Source Plugin -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-source-plugin</artifactId>
                <version>${maven.source.plugin.version}</version>
                <executions>
                    <execution>
                        <id>attach-sources</id>
                        <goals>
                            <goal>jar-no-fork</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>

            <!-- Maven Javadoc Plugin -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-javadoc-plugin</artifactId>
                <version>${maven.javadoc.plugin.version}</version>
                <configuration>
                    <source>11</source>
                    <detectJavaApiLink>false</detectJavaApiLink>
                    <failOnError>false</failOnError>
                    <quiet>true</quiet>
                </configuration>
                <executions>
                    <execution>
                        <id>attach-javadocs</id>
                        <goals>
                            <goal>jar</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>

            <!-- Maven Surefire Plugin for Testing -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>${maven.surefire.plugin.version}</version>
                <configuration>
                    <skipTests>false</skipTests>
                    <includes>
                        <include>**/*Test.java</include>
                        <include>**/*Tests.java</include>
                    </includes>
                </configuration>
            </plugin>
        </plugins>
    </build>

    <!-- ============================================= -->
    <!-- Profiles for Different Spark Versions       -->
    <!-- ============================================= -->
    <profiles>
        <!-- Spark 3.5 (Default) -->
        <profile>
            <id>spark-3.5</id>
            <activation>
                <activeByDefault>true</activeByDefault>
            </activation>
            <properties>
                <spark.version>3.5.0</spark.version>
            </properties>
        </profile>

        <!-- Spark 3.4 -->
        <profile>
            <id>spark-3.4</id>
            <properties>
                <spark.version>3.4.2</spark.version>
            </properties>
        </profile>

        <!-- Spark 3.3 -->
        <profile>
            <id>spark-3.3</id>
            <properties>
                <spark.version>3.3.4</spark.version>
            </properties>
        </profile>

        <!-- Spark 3.2 -->
        <profile>
            <id>spark-3.2</id>
            <properties>
                <spark.version>3.2.4</spark.version>
            </properties>
        </profile>
    </profiles>

    <!-- ============================================= -->
    <!-- Repositories                                 -->
    <!-- ============================================= -->
    <repositories>
        <repository>
            <id>central</id>
            <name>Central Repository</name>
            <url>https://repo.maven.apache.org/maven2</url>
        </repository>
    </repositories>
</project>

==================
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
         http://maven.apache.org/xsd/maven-4.0.0.xsd">

    <modelVersion>4.0.0</modelVersion>

    <!-- ============================================= -->
    <!-- Your Main Project Coordinates                -->
    <!-- ============================================= -->
    <groupId>com.yourcompany</groupId>
    <artifactId>your-spark-application</artifactId>
    <version>1.0.0</version>
    <packaging>jar</packaging>

    <n>Your Spark Application</n>
    <description>Main application using Databricks JDBC Dialect</description>

    <!-- ============================================= -->
    <!-- Properties                                   -->
    <!-- ============================================= -->
    <properties>
        <maven.compiler.source>11</maven.compiler.source>
        <maven.compiler.target>11</maven.compiler.target>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>

        <!-- Versions -->
        <spark.version>3.5.0</spark.version>
        <scala.version>2.12</scala.version>
        <databricks.jdbc.version>2.6.36</databricks.jdbc.version>
        <databricks.dialect.version>1.0.0</databricks.dialect.version>

        <!-- Plugin Versions -->
        <maven.shade.plugin.version>3.5.1</maven.shade.plugin.version>
    </properties>

    <!-- ============================================= -->
    <!-- Dependencies                                 -->
    <!-- ============================================= -->
    <dependencies>
        <!-- ========================================= -->
        <!-- Databricks JDBC Dialect (Your Library)   -->
        <!-- ========================================= -->
        <dependency>
            <groupId>com.databricks.dialect</groupId>
            <artifactId>databricks-jdbc-dialect</artifactId>
            <version>${databricks.dialect.version}</version>
        </dependency>

        <!-- ========================================= -->
        <!-- Spark Dependencies                        -->
        <!-- ========================================= -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
        </dependency>

        <!-- ========================================= -->
        <!-- Databricks JDBC Driver                   -->
        <!-- ========================================= -->
        <dependency>
            <groupId>com.databricks</groupId>
            <artifactId>databricks-jdbc</artifactId>
            <version>${databricks.jdbc.version}</version>
        </dependency>

        <!-- Alternative Databricks JDBC Driver -->
        <!--
        <dependency>
            <groupId>com.simba.spark</groupId>
            <artifactId>SparkJDBC42</artifactId>
            <version>2.6.36.1050</version>
        </dependency>
        -->

        <!-- ========================================= -->
        <!-- Other Dependencies                        -->
        <!-- ========================================= -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>2.0.9</version>
        </dependency>

        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-slf4j2-impl</artifactId>
            <version>2.21.1</version>
        </dependency>

        <!-- Configuration -->
        <dependency>
            <groupId>com.typesafe</groupId>
            <artifactId>config</artifactId>
            <version>1.4.3</version>
        </dependency>

        <!-- Testing -->
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.13.2</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <!-- ============================================= -->
    <!-- Build Configuration                          -->
    <!-- ============================================= -->
    <build>
        <plugins>
            <!-- Maven Compiler Plugin -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.11.0</version>
                <configuration>
                    <source>11</source>
                    <target>11</target>
                </configuration>
            </plugin>

            <!-- Maven Shade Plugin for Uber JAR -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>${maven.shade.plugin.version}</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <createDependencyReducedPom>false</createDependencyReducedPom>
                            <filters>
                                <filter>
                                    <artifact>*:*</artifact>
                                    <excludes>
                                        <exclude>META-INF/*.SF</exclude>
                                        <exclude>META-INF/*.DSA</exclude>
                                        <exclude>META-INF/*.RSA</exclude>
                                    </excludes>
                                </filter>
                            </filters>
                            <transformers>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>com.yourcompany.MainApplication</mainClass>
                                </transformer>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>
                            </transformers>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>

    <!-- ============================================= -->
    <!-- Repositories                                 -->
    <!-- ============================================= -->
    <repositories>
        <!-- Maven Central -->
        <repository>
            <id>central</id>
            <n>Central Repository</n>
            <url>https://repo.maven.apache.org/maven2</url>
        </repository>

        <!-- Databricks Repository -->
        <repository>
            <id>databricks</id>
            <n>Databricks Repository</n>
            <url>https://repo.databricks.com/artifactory/maven-releases/</url>
        </repository>
    </repositories>
</project>
==============

# Databricks JDBC Dialect Setup Instructions

## Project Structure

You'll have two separate Maven projects:

```
databricks-jdbc-dialect/          # Dialect Library Project
├── pom.xml
├── src/
│   ├── main/
│   │   └── java/
│   │       └── com/
│   │           └── databricks/
│   │               └── dialect/
│   │                   └── DatabricksJdbcDialect.java
│   └── test/
│       └── java/
│           └── com/
│               └── databricks/
│                   └── dialect/
│                       └── DatabricksJdbcDialectTest.java

your-main-project/                # Your Application Project
├── pom.xml
├── src/
│   └── main/
│       └── java/
│           └── com/
│               └── yourcompany/
│                   └── YourApplication.java
```

## Step 1: Build the Databricks JDBC Dialect Library

### 1.1 Create the Dialect Project

```bash
# Create directory structure
mkdir -p databricks-jdbc-dialect/src/main/java/com/databricks/dialect
cd databricks-jdbc-dialect

# Copy the standalone POM
# Place the standalone-dialect-pom.xml content as pom.xml

# Copy the DatabricksJdbcDialect.java file to:
# src/main/java/com/databricks/dialect/DatabricksJdbcDialect.java
```

### 1.2 Build and Install to Local Maven Repository

```bash
# Build the dialect library
mvn clean compile

# Run tests (optional)
mvn test

# Package the JAR
mvn package

# Install to local Maven repository (~/.m2/repository)
mvn install
```

This will create:
- `target/databricks-jdbc-dialect-1.0.0.jar` - The dialect library
- `target/databricks-jdbc-dialect-1.0.0-sources.jar` - Source code
- `target/databricks-jdbc-dialect-1.0.0-javadoc.jar` - Documentation

The JAR will be installed in your local Maven repository at:
```
~/.m2/repository/com/databricks/dialect/databricks-jdbc-dialect/1.0.0/
```

### 1.3 Build for Different Spark Versions

```bash
# For Spark 3.4
mvn clean install -Pspark-3.4

# For Spark 3.3
mvn clean install -Pspark-3.3

# For Spark 3.2
mvn clean install -Pspark-3.2
```

## Step 2: Use the Dialect in Your Main Project

### 2.1 Add Dependency to Your Project's POM

In your main project's `pom.xml`, add the dialect dependency:

```xml
<dependency>
    <groupId>com.databricks.dialect</groupId>
    <artifactId>databricks-jdbc-dialect</artifactId>
    <version>1.0.0</version>
</dependency>
```

### 2.2 Register the Dialect in Your Code

```java
package com.yourcompany;

import com.databricks.dialect.DatabricksJdbcDialect;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.jdbc.JdbcDialects;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import java.util.Properties;

public class YourApplication {

    public static void main(String[] args) {
        // Step 1: Register the Databricks JDBC Dialect
        JdbcDialects.registerDialect(new DatabricksJdbcDialect());
        System.out.println("Databricks JDBC Dialect registered successfully!");

        // Step 2: Create Spark Session
        SparkSession spark = SparkSession.builder()
            .appName("YourDatabricksApp")
            .config("spark.sql.adaptive.enabled", "true")
            .getOrCreate();

        // Step 3: Setup connection properties
        Properties connectionProperties = new Properties();
        connectionProperties.setProperty("user", "token");
        connectionProperties.setProperty("password", System.getenv("DATABRICKS_TOKEN"));
        connectionProperties.setProperty("driver", "com.databricks.client.jdbc.Driver");
        connectionProperties.setProperty("ssl", "1");
        connectionProperties.setProperty("transportMode", "http");
        connectionProperties.setProperty("AuthMech", "3");
        connectionProperties.setProperty("fetchsize", "10000");
        connectionProperties.setProperty("batchsize", "10000");

        // Step 4: Build JDBC URL
        String jdbcUrl = String.format(
            "jdbc:databricks://%s:443/default;transportMode=http;ssl=1;httpPath=%s;AuthMech=3",
            System.getenv("DATABRICKS_WORKSPACE_URL"),
            System.getenv("DATABRICKS_HTTP_PATH")
        );

        // Step 5: Use the dialect - it will automatically handle all data types
        try {
            // Read data - dialect handles type conversion automatically
            Dataset<Row> df = spark.read()
                .jdbc(jdbcUrl, "your_table_with_complex_types", connectionProperties);

            df.printSchema();  // Will show correct Spark types
            df.show(10);

            // Write data - dialect preserves types
            df.write()
                .mode("overwrite")
                .jdbc(jdbcUrl, "new_table", connectionProperties);

        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            spark.close();
        }
    }
}
```

### 2.3 Build Your Main Project

```bash
cd your-main-project
mvn clean package
```

## Step 3: Deploy Options

### Option A: Include Dialect JAR in Spark Submit

```bash
spark-submit \
  --jars /path/to/databricks-jdbc-dialect-1.0.0.jar \
  --class com.yourcompany.YourApplication \
  target/your-application-1.0.0.jar
```

### Option B: Deploy to Databricks Cluster

1. Upload the dialect JAR to DBFS:
```python
# In Databricks notebook
dbutils.fs.cp("file:/tmp/databricks-jdbc-dialect-1.0.0.jar",
              "dbfs:/libs/databricks-jdbc-dialect-1.0.0.jar")
```

2. Add to cluster libraries:
   - Go to Clusters → Your Cluster → Libraries
   - Click "Install New"
   - Choose "DBFS" and enter: `dbfs:/libs/databricks-jdbc-dialect-1.0.0.jar`

### Option C: Deploy to Central Repository

If you have a company Maven repository (Nexus, Artifactory):

```bash
# Configure your repository in settings.xml
mvn deploy
```

Then other team members can use it by adding the dependency.

## Step 4: Verify the Dialect is Working

### Test Code to Verify All Types

```java
import org.apache.spark.sql.types.*;

public class DialectVerification {

    public static void verifyDialect(SparkSession spark, String jdbcUrl, Properties props) {
        // Create test table with all types
        String createTableSQL = """
            CREATE TABLE IF NOT EXISTS dialect_test (
                bool_col BOOLEAN,
                int_col INT,
                bigint_col BIGINT,
                decimal_col DECIMAL(10, 2),
                string_col STRING,
                date_col DATE,
                timestamp_col TIMESTAMP,
                array_col ARRAY<STRING>,
                map_col MAP<STRING, INT>,
                struct_col STRUCT<n: STRING, age: INT>
            ) USING DELTA
        """;

        spark.sql(createTableSQL);

        // Read the table
        Dataset<Row> df = spark.read().jdbc(jdbcUrl, "dialect_test", props);

        // Check schema - should show correct Spark types
        df.printSchema();

        // Verify types
        StructType schema = df.schema();
        for (StructField field : schema.fields()) {
            System.out.println("Field: " + field.n() +
                             ", Type: " + field.dataType().typeName());
        }
    }
}
```

## Environment Variables

Set these before running:

```bash
export DATABRICKS_WORKSPACE_URL="your-workspace.cloud.databricks.com"
export DATABRICKS_HTTP_PATH="/sql/1.0/warehouses/your-warehouse-id"
export DATABRICKS_TOKEN="your-personal-access-token"
```

## Troubleshooting

### Issue: Dialect not found
**Solution**: Ensure the dialect JAR is in the classpath and registered before any JDBC operations.

### Issue: Type conversion errors
**Solution**: Verify the dialect is registered and the Databricks JDBC driver version matches.

### Issue: Maven can't find the dialect dependency
**Solution**: Make sure you've run `mvn install` in the dialect project first.

### Issue: NoClassDefFoundError
**Solution**: Ensure all Spark dependencies have `provided` scope to avoid conflicts.

## Version Compatibility

| Dialect Version | Spark Version | Databricks Runtime |
|----------------|---------------|-------------------|
| 1.0.0          | 3.2.x - 3.5.x | 10.4 LTS - 14.x   |

## Support Matrix

| Data Type | Supported | Notes |
|-----------|-----------|-------|
| BOOLEAN | ✅ | |
| TINYINT | ✅ | |
| SMALLINT | ✅ | |
| INT | ✅ | |
| BIGINT | ✅ | |
| FLOAT | ✅ | |
| DOUBLE | ✅ | |
| DECIMAL | ✅ | Preserves precision |
| STRING | ✅ | |
| BINARY | ✅ | |
| DATE | ✅ | |
| TIMESTAMP | ✅ | |
| ARRAY | ✅ | Nested types supported |
| MAP | ✅ | Complex keys/values |
| STRUCT | ✅ | Nested structures |
| INTERVAL | ✅ | Maps to STRING |

==========================
Perfect! I've created a complete setup with:
1. Standalone Databricks JDBC Dialect POM (standalone-dialect-pom.xml)
This is a minimal, focused POM specifically for building the dialect library:

Only includes necessary Spark dependencies (all with provided scope)
Creates a lightweight JAR (~10-50KB)
Includes profiles for different Spark versions
Generates source and javadoc JARs

2. Your Main Project POM (main-project-pom.xml)
Shows how to use the dialect library in your actual project:

Includes the dialect as a dependency
Adds Databricks JDBC driver
Configured for your application needs

3. Complete Setup Instructions
Step-by-step guide covering:

Building the dialect library
Installing to local Maven repository
Using in your main project
Deployment options
Verification steps

Quick Start Process:
Step 1: Build Dialect Library
bash# In dialect project directory
mvn clean install
Step 2: Add to Your Project
xml<dependency>
    <groupId>com.databricks.dialect</groupId>
    <artifactId>databricks-jdbc-dialect</artifactId>
    <version>1.0.0</version>
</dependency>
Step 3: Register in Your Code
java// At application startup
JdbcDialects.registerDialect(new DatabricksJdbcDialect());
The dialect library JAR will be very small and focused, containing only the DatabricksJdbcDialect class. Your main project can then include this dialect along with all other dependencies it needs (Databricks JDBC driver, Spark, etc.).
This separation allows you to:

Version the dialect independently
Share it across multiple projects
Deploy it to a central repository
Keep your main project POM clean