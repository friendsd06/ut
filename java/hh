package com.example.s3demo.aws;
import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;
import software.amazon.awssdk.auth.credentials.StaticCredentialsProvider;
import software.amazon.awssdk.core.sync.RequestBody;
import software.amazon.awssdk.http.apache.ApacheHttpClient;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.S3Configuration;
import software.amazon.awssdk.services.s3.model.*;

import java.io.File;
import java.io.IOException;
import java.net.URI;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;
import java.time.Duration;
import java.time.Instant;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.stream.Collectors;

/**
 * Complete S3 Multipart Upload Implementation for LocalStack with AWS SDK 2.x
 * Configured to read from D: drive and upload to LocalStack
 */
public class S3MultipartUploadLocalstack {

    // Configuration parameters
    private static final long MIN_PART_SIZE = 5 * 1024 * 1024; // 5MB minimum part size
    private static final int MAX_PARTS = 10000; // S3 limit
    private static final int THREAD_POOL_SIZE = 10; // For parallel uploads
    private static final int MAX_RETRIES = 3;

    // LocalStack configuration
    private static final String LOCALSTACK_ENDPOINT = "https://localhost.localstack.cloud:4566";
    private static final String LOCAL_REGION = "us-east-1";
    private static final String ACCESS_KEY = "test";
    private static final String SECRET_KEY = "test";

    // Local directory to scan
    private static final String LOCAL_DIRECTORY = "D:\\data";

    // Declare S3 client at class level for reuse
    private final S3Client s3Client;

    // For logging
    private final boolean verboseLogging;
    private int logIndentLevel = 0;

    /**
     * Constructor - Configured for LocalStack
     */
    public S3MultipartUploadLocalstack(boolean verboseLogging) {
        this.verboseLogging = verboseLogging;

        log("Creating S3 client for LocalStack at: " + LOCALSTACK_ENDPOINT);

        // Create HTTP client with timeout settings
        ApacheHttpClient.Builder httpClientBuilder = ApacheHttpClient.builder()
                .socketTimeout(Duration.ofMinutes(5))
                .connectionTimeout(Duration.ofMinutes(5))
                .connectionAcquisitionTimeout(Duration.ofMinutes(2));

        // Create S3 configuration for LocalStack
        S3Configuration s3Configuration = S3Configuration.builder()
                .pathStyleAccessEnabled(true)  // Important for LocalStack
                .checksumValidationEnabled(false)
                .build();

        // Create S3 client configured for LocalStack
        this.s3Client = S3Client.builder()
                .endpointOverride(URI.create(LOCALSTACK_ENDPOINT))
                .region(Region.of(LOCAL_REGION))
                // LocalStack fixed credentials
                .credentialsProvider(StaticCredentialsProvider.create(
                        AwsBasicCredentials.create(ACCESS_KEY, SECRET_KEY)))
                .httpClient(httpClientBuilder.build())
                .serviceConfiguration(s3Configuration)
                .build();

        log("S3 client for LocalStack successfully created");
    }

    /**
     * Main method - Uploads all files from D: drive to LocalStack
     */
    public static void main(String[] args) {
        // Create our uploader instance with verbose logging
        S3MultipartUploadLocalstack uploader = new S3MultipartUploadLocalstack(true);

        try {
            // Create bucket on LocalStack if it doesn't exist
            String bucketName = "demo";
            uploader.createBucketIfNotExists(bucketName);

            // Scan the directory for files to upload
            File directory = new File(LOCAL_DIRECTORY);
            if (!directory.exists() || !directory.isDirectory()) {
                System.err.println("ERROR: Directory not found: " + LOCAL_DIRECTORY);
                System.err.println("Please create this directory and add files to upload.");
                return;
            }

            File[] filesToUpload = directory.listFiles();
            if (filesToUpload == null || filesToUpload.length == 0) {
                System.err.println("No files found in directory: " + LOCAL_DIRECTORY);
                return;
            }

            System.out.println("Found " + filesToUpload.length + " files to upload in " + LOCAL_DIRECTORY);

            // Upload each file
            for (File file : filesToUpload) {
                if (file.isFile()) {
                    String objectKey = file.getName();
                    String filePath = file.getAbsolutePath();

                    System.out.println("\n=== UPLOADING FILE: " + filePath + " ===");
                    System.out.println("Target S3 object: " + objectKey);

                    // Choose upload method based on file size
                    long fileSize = file.length();
                    if (fileSize < 10 * 1024 * 1024) { // Less than 10MB
                        // Use simple upload for small files
                        uploader.uploadSimple(bucketName, objectKey, filePath);
                    } else {
                        // Use parallel multipart upload for larger files
                        uploader.uploadFileParallel(bucketName, objectKey, filePath);
                    }
                }
            }

            // List all objects in the bucket to verify uploads
            uploader.listObjects(bucketName);

        } catch (Exception e) {
            System.err.println("ERROR: Upload failed with exception: " + e.getMessage());
            e.printStackTrace();
        } finally {
            // Clean up resources
            uploader.close();
        }
    }

    /**
     * Create a bucket if it doesn't already exist
     */
    public void createBucketIfNotExists(String bucketName) {
        try {
            // Check if bucket exists
            log("Checking if bucket exists: " + bucketName);
            try {
                s3Client.headBucket(HeadBucketRequest.builder().bucket(bucketName).build());
                log("Bucket already exists: " + bucketName);
                return;
            } catch (NoSuchBucketException e) {
                log("Bucket does not exist, creating it: " + bucketName);
            }

            // Create the bucket
            CreateBucketRequest createBucketRequest = CreateBucketRequest.builder()
                    .bucket(bucketName)
                    .build();

            s3Client.createBucket(createBucketRequest);
            log("Bucket created successfully: " + bucketName);

        } catch (S3Exception e) {
            log("ERROR creating bucket: " + e.getMessage(), true);
            throw e;
        }
    }

    /**
     * List all objects in a bucket
     */
    public void listObjects(String bucketName) {
        log("\n=== OBJECTS IN BUCKET: " + bucketName + " ===");

        try {
            ListObjectsV2Request listRequest = ListObjectsV2Request.builder()
                    .bucket(bucketName)
                    .build();

            ListObjectsV2Response response = s3Client.listObjectsV2(listRequest);

            if (response.hasContents()) {
                for (S3Object object : response.contents()) {
                    log(String.format("Object: %s (Size: %,d bytes, Last Modified: %s)",
                            object.key(), object.size(), object.lastModified()));
                }
            } else {
                log("No objects found in bucket.");
            }
        } catch (S3Exception e) {
            log("ERROR listing objects: " + e.getMessage(), true);
        }
    }

    /**
     * Simple upload for small files (not using multipart)
     */
    public void uploadSimple(String bucketName, String objectKey, String filePath) throws IOException {
        log("Using simple upload for small file: " + filePath);

        Path path = Paths.get(filePath);
        long fileSize = Files.size(path);

        try {
            log(String.format("Uploading %s (%,d bytes) to %s/%s",
                    filePath, fileSize, bucketName, objectKey));

            // Create the PutObject request
            PutObjectRequest putObjectRequest = PutObjectRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .build();

            // Upload the file
            s3Client.putObject(putObjectRequest, RequestBody.fromFile(path));

            log("Simple upload completed successfully!");

        } catch (S3Exception e) {
            log("ERROR: Upload failed: " + e.getMessage(), true);
            throw e;
        }
    }

    /**
     * Upload a file with parallel part uploads for better performance
     */
    public void uploadFileParallel(String bucketName, String objectKey, String filePath) throws Exception {
        log("Starting parallel multipart upload...");
        Instant startTime = Instant.now();

        Path path = Paths.get(filePath);

        // Step 1: Calculate optimal part size and count
        long fileSize = Files.size(path);
        long[] partDetails = calculatePartDetails(fileSize);
        long partSize = partDetails[0];
        long partCount = partDetails[1];

        log(String.format("File size: %,d bytes", fileSize));
        log(String.format("Part size: %,d bytes", partSize));
        log(String.format("Number of parts: %d", partCount));
        log(String.format("Thread pool size: %d", THREAD_POOL_SIZE));

        // Step 2: Initiate multipart upload
        String uploadId = initiateMultipartUpload(bucketName, objectKey);
        log("Upload initiated with ID: " + uploadId);

        // Create thread pool for parallel uploads
        ExecutorService executorService = Executors.newFixedThreadPool(THREAD_POOL_SIZE);
        log("Created thread pool with " + THREAD_POOL_SIZE + " threads");

        try {
            log("Creating upload tasks for all parts...");
            List<CompletableFuture<CompletedPart>> uploadFutures = new ArrayList<>();

            // Create futures for each part upload
            for (int partNumber = 1; partNumber <= partCount; partNumber++) {
                final long position = (partNumber - 1) * partSize;
                final long size = Math.min(partSize, fileSize - position);
                final int finalPartNumber = partNumber;

                logVerbose(String.format("Creating upload task for part %d (bytes %,d to %,d)",
                        finalPartNumber, position, position + size - 1));

                // Create future for this part
                CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(() -> {
                    try {
                        log(String.format("Thread %s: Starting upload of part %d (bytes %,d to %,d)",
                                Thread.currentThread().getName(), finalPartNumber,
                                position, position + size - 1));

                        // Upload with retries
                        CompletedPart part = uploadSinglePartWithRetry(
                                bucketName, objectKey, uploadId, path,
                                finalPartNumber, position, size, MAX_RETRIES);

                        log(String.format("Thread %s: Completed upload of part %d, ETag: %s",
                                Thread.currentThread().getName(), finalPartNumber, part.eTag()));

                        return part;
                    } catch (Exception e) {
                        log("ERROR in part " + finalPartNumber + ": " + e.getMessage(), true);
                        throw new RuntimeException(e);
                    }
                }, executorService);

                uploadFutures.add(future);
            }

            log("All upload tasks created. Waiting for completion...");

            // Wait for all uploads to complete
            CompletableFuture<Void> allUploadsFuture = CompletableFuture.allOf(
                    uploadFutures.toArray(new CompletableFuture[0]));

            // This will block until all uploads are done
            allUploadsFuture.join();
            log("All part uploads completed successfully");

            // Collect all completed parts
            List<CompletedPart> completedParts = uploadFutures.stream()
                    .map(CompletableFuture::join)
                    .collect(Collectors.toList());

            log("Collected ETags from all " + completedParts.size() + " parts");

            // Step 4: Complete multipart upload
            completeMultipartUpload(bucketName, objectKey, uploadId, completedParts);

            // Calculate and log total time
            Duration totalTime = Duration.between(startTime, Instant.now());
            log(String.format("Parallel upload complete! Total time: %d.%d seconds",
                    totalTime.getSeconds(), totalTime.getNano() / 1000000));

        } catch (Exception e) {
            // If anything goes wrong, abort the upload
            log("ERROR: Upload failed: " + e.getMessage(), true);
            abortMultipartUpload(bucketName, objectKey, uploadId);
            throw e;
        } finally {
            // Shut down the executor service
            log("Shutting down thread pool");
            executorService.shutdown();
        }
    }

    /**
     * Calculate optimal part size and part count based on file size
     */
    private long[] calculatePartDetails(long fileSize) {
        logStart("Calculating optimal part size");

        // Start with minimum part size
        long partSize = MIN_PART_SIZE;

        // If file is large enough to exceed max parts limit, increase part size
        if (fileSize > MIN_PART_SIZE * MAX_PARTS) {
            // Calculate minimum required part size to stay under part count limit
            partSize = (fileSize + MAX_PARTS - 1) / MAX_PARTS; // Ceiling division

            // Round up to nearest MB for cleaner numbers
            long MB = 1024 * 1024;
            partSize = ((partSize + MB - 1) / MB) * MB; // Round up to nearest MB

            log("File too large for minimum part size, adjusted to: " + partSize + " bytes");
        }

        // Calculate part count based on part size
        long partCount = (fileSize + partSize - 1) / partSize; // Ceiling division

        logEnd("Optimal part size calculated: " + partSize + " bytes, resulting in " + partCount + " parts");

        return new long[] { partSize, partCount };
    }

    /**
     * Step 2: Initiate multipart upload
     */
    private String initiateMultipartUpload(String bucketName, String objectKey) {
        logStart("Initiating multipart upload for " + objectKey);

        try {
            // Build the request
            logVerbose("Creating CreateMultipartUploadRequest");
            CreateMultipartUploadRequest createMultipartUploadRequest = CreateMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    // Optional settings below
                    // .contentType("application/octet-stream")
                    // .acl(ObjectCannedACL.PRIVATE)
                    .build();

            // Send the request
            logVerbose("Sending CreateMultipartUploadRequest to S3");
            CreateMultipartUploadResponse response = s3Client.createMultipartUpload(createMultipartUploadRequest);

            String uploadId = response.uploadId();
            logEnd("Multipart upload initiated, Upload ID: " + uploadId);

            return uploadId;
        } catch (S3Exception e) {
            logEnd("ERROR: Failed to initiate multipart upload: " + e.getMessage(), true);
            throw e;
        }
    }

    /**
     * Upload a single part of the file
     */
    private CompletedPart uploadSinglePart(
            String bucketName, String objectKey, String uploadId,
            Path filePath, int partNumber, long position, long size) throws IOException {

        logVerbose(String.format("Reading part %d data from file (position=%d, size=%d)",
                partNumber, position, size));

        // Read this part from the file
        ByteBuffer partData = readPartFromFile(filePath, position, size);

        logVerbose("Creating UploadPartRequest for part " + partNumber);
        UploadPartRequest uploadPartRequest = UploadPartRequest.builder()
                .bucket(bucketName)
                .key(objectKey)
                .uploadId(uploadId)
                .partNumber(partNumber)
                .build();

        logVerbose("Sending UploadPartRequest to S3");
        UploadPartResponse response = s3Client.uploadPart(
                uploadPartRequest, RequestBody.fromByteBuffer(partData));

        String eTag = response.eTag();
        logVerbose("Received ETag for part " + partNumber + ": " + eTag);

        return CompletedPart.builder()
                .partNumber(partNumber)
                .eTag(eTag)
                .build();
    }

    /**
     * Upload a single part with retry logic
     */
    private CompletedPart uploadSinglePartWithRetry(
            String bucketName, String objectKey, String uploadId,
            Path filePath, int partNumber, long position, long size,
            int maxRetries) throws IOException {

        int retryCount = 0;

        while (true) {
            try {
                if (retryCount > 0) {
                    log(String.format("Retry #%d for part %d", retryCount, partNumber));
                }

                return uploadSinglePart(bucketName, objectKey, uploadId,
                        filePath, partNumber, position, size);

            } catch (S3Exception e) {
                retryCount++;

                if (retryCount >= maxRetries) {
                    log(String.format("ERROR: Part %d failed after %d retries: %s",
                            partNumber, retryCount, e.getMessage()), true);
                    throw e;
                }

                // Calculate exponential backoff time
                long backoffMillis = (long) (Math.pow(2, retryCount) * 100);
                log(String.format("Part %d upload failed, retrying in %d ms. Error: %s",
                        partNumber, backoffMillis, e.getMessage()));

                try {
                    Thread.sleep(backoffMillis);
                } catch (InterruptedException ie) {
                    Thread.currentThread().interrupt();
                    throw new RuntimeException("Thread interrupted during backoff", ie);
                }
            }
        }
    }

    /**
     * Complete the multipart upload
     */
    private void completeMultipartUpload(
            String bucketName, String objectKey, String uploadId, List<CompletedPart> parts) {

        logStart("Completing multipart upload");

        try {
            // Sort parts by part number
            List<CompletedPart> sortedParts = parts.stream()
                    .sorted((p1, p2) -> Integer.compare(p1.partNumber(), p2.partNumber()))
                    .collect(Collectors.toList());

            // Log parts for verification
            if (verboseLogging) {
                logVerbose("Parts to be combined:");
                for (CompletedPart part : sortedParts) {
                    logVerbose(String.format("  Part %d: ETag %s", part.partNumber(), part.eTag()));
                }
            }

            logVerbose("Creating CompleteMultipartUploadRequest");
            CompleteMultipartUploadRequest completeRequest = CompleteMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .uploadId(uploadId)
                    .multipartUpload(CompletedMultipartUpload.builder()
                            .parts(sortedParts)
                            .build())
                    .build();

            logVerbose("Sending CompleteMultipartUploadRequest to S3");
            CompleteMultipartUploadResponse response = s3Client.completeMultipartUpload(completeRequest);

            logEnd(String.format("Multipart upload completed successfully!\nObject URL: %s\nETag: %s",
                    response.location(), response.eTag()));

        } catch (S3Exception e) {
            logEnd("ERROR: Failed to complete multipart upload: " + e.getMessage(), true);
            throw e;
        }
    }

    /**
     * Abort the multipart upload
     */
    private void abortMultipartUpload(String bucketName, String objectKey, String uploadId) {
        logStart("Aborting multipart upload");

        try {
            logVerbose("Creating AbortMultipartUploadRequest");
            AbortMultipartUploadRequest abortRequest = AbortMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .uploadId(uploadId)
                    .build();

            logVerbose("Sending AbortMultipartUploadRequest to S3");
            s3Client.abortMultipartUpload(abortRequest);

            logEnd("Multipart upload aborted successfully");
        } catch (S3Exception e) {
            logEnd("ERROR: Failed to abort multipart upload: " + e.getMessage(), true);
            // Just log and continue, don't throw - this is already in an error path
        }
    }

    /**
     * Read a specific part from the file
     */
    private ByteBuffer readPartFromFile(Path filePath, long position, long size) throws IOException {
        logVerbose(String.format("Reading %,d bytes from file at position %,d", size, position));

        // Allocate buffer for this part
        ByteBuffer buffer = ByteBuffer.allocate((int) size);

        // Open file channel and read the part
        try (FileChannel channel = FileChannel.open(filePath, StandardOpenOption.READ)) {
            // Position channel at the start position for this part
            channel.position(position);

            // Read data into buffer
            int bytesRead = channel.read(buffer);

            if (bytesRead != size) {
                logVerbose(String.format("WARNING: Read %d bytes but expected %d bytes",
                        bytesRead, size));
            }

            // Flip buffer to prepare for reading
            buffer.flip();

            logVerbose(String.format("Successfully read %,d bytes from file", bytesRead));
            return buffer;
        }
    }

    /**
     * Close resources
     */
    public void close() {
        log("Closing S3 client");
        if (s3Client != null) {
            s3Client.close();
        }
    }

    // Logging utilities

    private void log(String message) {
        log(message, false);
    }

    private void log(String message, boolean isError) {
        String indent = " ".repeat(logIndentLevel * 2);
        if (isError) {
            System.err.println(indent + message);
        } else {
            System.out.println(indent + message);
        }
    }

    private void logVerbose(String message) {
        if (verboseLogging) {
            String indent = " ".repeat(logIndentLevel * 2);
            System.out.println(indent + "  → " + message);
        }
    }

    private void logStart(String message) {
        log(message);
        logIndentLevel++;
    }

    private void logEnd(String message) {
        logEnd(message, false);
    }

    private void logEnd(String message, boolean isError) {
        logIndentLevel--;
        log(message, isError);
    }
}