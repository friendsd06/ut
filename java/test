```python
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, explode, flatten, explode_outer
from pyspark.sql.types import ArrayType, StructType
from typing import List, Dict, Optional
from dataclasses import dataclass
from abc import ABC, abstractmethod
import yaml
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class SourceConfig:
    """Configuration for data source"""
    table_name: Optional[str] = None
    query: Optional[str] = None

@dataclass
class WriteConfig:
    """Configuration for data writing"""
    path: str
    format: str = 'parquet'
    mode: str = 'overwrite'
    partition_by: Optional[List[str]] = None
    options: Dict[str, str] = None

class FlatteningStrategy(ABC):
    """Abstract base class for flattening strategies"""
    @abstractmethod
    def flatten(self, df: DataFrame, column: str) -> DataFrame:
        pass

class SimpleArrayStrategy(FlatteningStrategy):
    """Strategy for flattening simple arrays using explode"""
    def flatten(self, df: DataFrame, column: str) -> DataFrame:
        logger.info(f"Applying simple array strategy to column: {column}")
        return df.withColumn(column, explode(col(column)))

class NestedArrayStrategy(FlatteningStrategy):
    """Strategy for flattening nested arrays"""
    def flatten(self, df: DataFrame, column: str) -> DataFrame:
        logger.info(f"Applying nested array strategy to column: {column}")
        flattened = df.withColumn(column, flatten(col(column)))
        return flattened.withColumn(column, explode(col(column)))

class ArrayStructStrategy(FlatteningStrategy):
    """Strategy for flattening array of structs"""
    def flatten(self, df: DataFrame, column: str) -> DataFrame:
        logger.info(f"Applying array struct strategy to column: {column}")
        exploded = df.withColumn(column, explode(col(column)))

        # Get struct fields
        struct_fields = [f.name for f in df.schema[column].dataType.elementType.fields]

        # Create individual columns for struct fields
        for field in struct_fields:
            new_col = f"{column}_{field}"
            exploded = exploded.withColumn(new_col, col(f"{column}.{field}"))

        return exploded.drop(column)

class PreservingArrayStrategy(FlatteningStrategy):
    """Strategy for flattening arrays while preserving nulls"""
    def flatten(self, df: DataFrame, column: str) -> DataFrame:
        logger.info(f"Applying preserving array strategy to column: {column}")
        return df.withColumn(column, explode_outer(col(column)))

class StrategyFactory:
    """Factory for creating appropriate flattening strategy"""
    @staticmethod
    def get_strategy(df: DataFrame, column: str, strategy_type: Optional[str] = None) -> FlatteningStrategy:
        if strategy_type:
            strategies = {
                'simple': SimpleArrayStrategy(),
                'nested': NestedArrayStrategy(),
                'struct': ArrayStructStrategy(),
                'preserving': PreservingArrayStrategy()
            }
            return strategies.get(strategy_type, SimpleArrayStrategy())

        # Auto-detect strategy based on column type
        field = [f for f in df.schema.fields if f.name == column][0]

        if isinstance(field.dataType, ArrayType):
            element_type = field.dataType.elementType
            if isinstance(element_type, ArrayType):
                return NestedArrayStrategy()
            elif isinstance(element_type, StructType):
                return ArrayStructStrategy()
            else:
                return SimpleArrayStrategy()
        else:
            raise ValueError(f"Column {column} is not an array type")

class DataFlattener:
    """Enhanced flattener supporting multiple strategies"""
    def __init__(self):
        self.strategy_factory = StrategyFactory()

    def flatten_data(self, df: DataFrame, columns: List[str],
                    strategy_configs: Dict[str, str] = None) -> DataFrame:
        """
        Flatten specified columns using appropriate strategies

        Args:
            df: Input DataFrame
            columns: List of columns to flatten
            strategy_configs: Optional dictionary mapping columns to strategy types
                            ('simple', 'nested', 'struct', 'preserving')
        """
        if not columns:
            return df

        result_df = df
        strategy_configs = strategy_configs or {}

        for column in columns:
            if column not in result_df.columns:
                logger.warning(f"Column '{column}' not found in data, skipping")
                continue

            try:
                # Get appropriate strategy
                strategy_type = strategy_configs.get(column)
                strategy = self.strategy_factory.get_strategy(result_df, column, strategy_type)

                # Apply flattening
                result_df = strategy.flatten(result_df, column)

            except Exception as e:
                logger.error(f"Error flattening column {column}: {str(e)}")
                raise

        return result_df

# Rest of the classes remain unchanged
class ConfigReader:
    """Configuration reader with validation"""
    def __init__(self, config_input: str):
        self.config = yaml.safe_load(config_input)
        self._validate_config()

    def _validate_config(self):
        if not isinstance(self.config, dict):
            raise ValueError("Configuration must be a dictionary")

        required_fields = ['tables', 's3_settings']
        if not all(field in self.config for field in required_fields):
            raise ValueError(f"Missing required fields: {required_fields}")

    def get_table_config(self, table_name: str) -> Dict:
        return self.config['tables'].get(table_name, {})

    def get_s3_config(self) -> Dict:
        return self.config['s3_settings']

class DataReader:
    """Handles reading data from Databricks tables"""
    def read_data(self, source_config: SourceConfig) -> DataFrame:
        if source_config.query:
            logger.info(f"Reading data using SQL query")
            return spark.sql(source_config.query)
        elif source_config.table_name:
            logger.info(f"Reading data from table: {source_config.table_name}")
            return spark.table(source_config.table_name)
        else:
            raise ValueError("Either query or table_name must be provided")

class DataWriter:
    """Handles writing data to S3"""
    def __init__(self, s3_config: Dict):
        self.s3_config = s3_config

    def write_data(self, df: DataFrame, write_config: WriteConfig) -> None:
        base_path = self.s3_config['base_path']
        full_path = f"{base_path}/{write_config.path}"

        writer = df.write.mode(write_config.mode)

        if write_config.format == 'delta':
            writer = writer.option("mergeSchema", "true")
            writer = writer.option("optimizeWrite", "true")

        if write_config.partition_by:
            writer = writer.partitionBy(write_config.partition_by)

        if write_config.options:
            for key, value in write_config.options.items():
                writer = writer.option(key, value)

        logger.info(f"Writing data to: {full_path}")
        writer.format(write_config.format).save(full_path)

class BatchProcessor:
    """Handles batch processing of multiple tables"""
    def __init__(self, processor: 'DataProcessor'):
        self.processor = processor

    def process_tables(self, table_names: List[str]) -> Dict[str, str]:
        """Process multiple tables"""
        results = {}

        for table in table_names:
            try:
                logger.info(f"Processing table: {table}")

                # Get table configuration
                table_config = self.processor.config_reader.get_table_config(table)
                if not table_config:
                    raise ValueError(f"No configuration found for table: {table}")

                # Create configurations
                source_config = SourceConfig(**table_config.get('source', {}))
                write_config = WriteConfig(
                    path=table_config['s3_path'],
                    format=table_config.get('format', 'parquet'),
                    mode=table_config.get('mode', 'overwrite'),
                    partition_by=table_config.get('partition_by'),
                    options=table_config.get('options', {})
                )

                # Process data
                df = self.processor.reader.read_data(source_config)

                if table_config.get('flatten_columns'):
                    strategy_configs = table_config.get('flatten_strategies', {})
                    df = self.processor.flattener.flatten_data(
                        df,
                        table_config['flatten_columns'],
                        strategy_configs
                    )

                self.processor.writer.write_data(df, write_config)

                results[table] = "Success"
                logger.info(f"Successfully processed table: {table}")

            except Exception as e:
                results[table] = f"Failed: {str(e)}"
                logger.error(f"Error processing table {table}: {str(e)}")

        return results

class DataProcessor:
    """Main processor class"""
    def __init__(self, config_reader: ConfigReader):
        self.config_reader = config_reader
        self.reader = DataReader()
        self.writer = DataWriter(config_reader.get_s3_config())
        self.flattener = DataFlattener()
```

Usage example in Databricks notebook:

```python
# Define configuration with flattening strategies
config = """
tables:
  customer_orders_flat:
    source:
      table_name: customer_orders
    flatten_columns:
      - items
      - shipping_address
    flatten_strategies:
      items: struct
      shipping_address: simple
    s3_path: processed/customer_orders
    format: delta
    partition_by:
      - order_date
    options:
      mergeSchema: 'true'

s3_settings:
  base_path: '/tmp/test'
"""

# Initialize and run processor
config_reader = ConfigReader(config)
processor = DataProcessor(config_reader)
batch_processor = BatchProcessor(processor)

# Process tables
results = batch_processor.process_tables(['customer_orders_flat'])

# Check results
for table, status in results.items():
    print(f"Table: {table} - Status: {status}")

# Verify output
result_df = spark.sql("SELECT * FROM delta.`/tmp/test/processed/customer_orders`")
print("\nFlattened Data Schema:")
result_df.printSchema()
print("\nFlattened Data Sample:")
result_df.show(2, False)
```

The code now includes:
1. Multiple flattening strategies
2. Strategy auto-detection
3. Configurable strategy selection
4. Preserved backward compatibility
5. Enhanced error handling

Would you like me to:
1. Add more strategy types?
2. Show more usage examples?
3. Add specific error handling?
4. Explain any part in detail?