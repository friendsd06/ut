package com.example.s3upload.config;

import jakarta.annotation.PreDestroy;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;
import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
import software.amazon.awssdk.auth.credentials.StaticCredentialsProvider;
import software.amazon.awssdk.http.async.SdkAsyncHttpClient;
import software.amazon.awssdk.http.nio.netty.NettyNioAsyncHttpClient;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3AsyncClient;
import software.amazon.awssdk.services.s3.S3Configuration;
import java.net.URI;
import java.time.Duration;

/**
 * Configuration for S3 clients
 * Creates both real AWS and LocalStack clients
 */
@Configuration
public class S3ClientConfig {
    private static final Logger log = LoggerFactory.getLogger(S3ClientConfig.class);

    private SdkAsyncHttpClient httpClient;
    private S3AsyncClient localstackS3Client;
    private S3AsyncClient awsS3Client;

    @Value("${app.localstack.url:http://localhost:4566}")
    private String localstackUrl;

    @Value("${app.aws.region:us-east-1}")
    private String region;

    @Value("${app.aws.connection.timeout:30}")
    private int connectionTimeoutSeconds;

    /**
     * Create and configure HTTP client for S3
     *
     * @return Configured SDK Async HTTP Client
     */
    private SdkAsyncHttpClient createHttpClient() {
        if (httpClient == null) {
            httpClient = NettyNioAsyncHttpClient.builder()
                    .connectionTimeout(Duration.ofSeconds(connectionTimeoutSeconds))
                    .connectionAcquisitionTimeout(Duration.ofMinutes(1))
                    .build();
            log.info("Created S3 HTTP client with {}s connection timeout", connectionTimeoutSeconds);
        }
        return httpClient;
    }

    /**
     * Create and configure S3 service configuration
     *
     * @return S3 configuration
     */
    private S3Configuration s3Config() {
        return S3Configuration.builder()
                .pathStyleAccessEnabled(true)  // For LocalStack & easier testing
                .checksumValidationEnabled(true)  // Validate data integrity
                .build();
    }

    /**
     * S3AsyncClient for LocalStack (for testing)
     *
     * @return S3AsyncClient configured for LocalStack
     */
    @Bean("localstackS3")
    @Primary
    public S3AsyncClient localstackClient() {
        log.info("Creating LocalStack S3 client with endpoint: {}", localstackUrl);
        localstackS3Client = S3AsyncClient.builder()
                .credentialsProvider(StaticCredentialsProvider.create(
                        AwsBasicCredentials.create("test", "test")))
                .endpointOverride(URI.create(localstackUrl))
                .region(Region.of(region))
                .httpClient(createHttpClient())
                .serviceConfiguration(s3Config())
                .build();
        return localstackS3Client;
    }

    /**
     * Primary S3AsyncClient for real AWS
     * Uses default credentials chain
     *
     * @return S3AsyncClient configured for AWS
     */
    @Bean("awsS3")
    public S3AsyncClient awsClient() {
        log.info("Creating AWS S3 client for region: {}", region);
        awsS3Client = S3AsyncClient.builder()
                .credentialsProvider(DefaultCredentialsProvider.create())
                .region(Region.of(region))
                .httpClient(createHttpClient())
                .serviceConfiguration(s3Config())
                .build();
        return awsS3Client;
    }

    /**
     * Clean up resources on shutdown
     */
    @PreDestroy
    public void cleanUp() {
        log.info("Shutting down S3 clients and resources");
        try {
            if (localstackS3Client != null) {
                localstackS3Client.close();
                log.debug("Closed LocalStack S3 client");
            }

            if (awsS3Client != null) {
                awsS3Client.close();
                log.debug("Closed AWS S3 client");
            }

            if (httpClient != null) {
                httpClient.close();
                log.debug("Closed HTTP client");
            }
        } catch (Exception e) {
            log.warn("Error during S3 client cleanup", e);
        }
    }
}
-------------------
package com.example.s3upload.model;

public record FileCompletionRequest(
        String requestId,
        String requestType,
        String cobDate,
        String freq,
        Boolean baseFileStatus,
        Boolean isBaseFile,
        Boolean adjFileStatus,
        Boolean isAdjFile,
        Boolean keyFileStatus,
        Boolean isKeyFile
) {}

--------------------
package com.example.s3upload.model;

import java.util.List;
import java.util.Map;

public record FileCreationDetails(
        String requestId,
        List<String> sliceNames,
        List<String> distinctIds,
        String idFilePath,
        String parentTableName,
        String requestType,
        String cobDate,
        String freq,
        List<String> tableList,
        Map<String, List<String>> adjustData,
        List<String> keysData
) {}
------------------------
package com.example.s3upload.service;

import com.example.s3upload.model.FileCompletionRequest;
import com.example.s3upload.model.FileCreationDetails;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicBoolean;

/**
 * Service responsible for processing file creation requests and managing
 * uploads to S3 storage
 */
@Service
public class FileService {

    private static final Logger log = LoggerFactory.getLogger(FileService.class);
    private static final long FILE_SIZE_THRESHOLD_MB = 10;
    private static final long FILE_SIZE_THRESHOLD_BYTES = FILE_SIZE_THRESHOLD_MB * 1024 * 1024;
    private static final int UPLOAD_TIMEOUT_MINUTES = 30;

    @Value("${app.upload.directory:D://data}")
    private String baseDirectoryPath;

    @Value("${app.s3.bucket-name}")
    private String bucketName;

    @Value("${spring.profiles.active:dev}")
    private String activeProfile;

    private final FileUploadService fileUploadService;
    private final ExecutorService fileProcessingExecutor;

    /**
     * Constructor with dependency injection
     *
     * @param fileUploadService Service for uploading files to S3
     */
    public FileService(FileUploadService fileUploadService) {
        this.fileUploadService = fileUploadService;
        this.fileProcessingExecutor = Executors.newFixedThreadPool(
                Runtime.getRuntime().availableProcessors());
    }

    /**
     * Initialize the service after construction
     */
    @PostConstruct
    public void initialize() {
        try {
            createDirectoryStructure();
            log.info("Directory structure created successfully at {}", baseDirectoryPath);
        } catch (IOException e) {
            log.error("Failed to create directory structure: {}", e.getMessage(), e);
        }
    }

    /**
     * Clean up resources when the service is being destroyed
     */
    @PreDestroy
    public void cleanup() {
        fileProcessingExecutor.shutdown();
        try {
            if (!fileProcessingExecutor.awaitTermination(1, TimeUnit.MINUTES)) {
                fileProcessingExecutor.shutdownNow();
            }
        } catch (InterruptedException e) {
            fileProcessingExecutor.shutdownNow();
            Thread.currentThread().interrupt();
        }
    }

    /**
     * Creates the required directory structure for file operations
     */
    private void createDirectoryStructure() throws IOException {
        // Create base directory
        Files.createDirectories(Paths.get(baseDirectoryPath));

        // Get execution environment
        String env = getExecutionEnvironment();

        // Create all required directories
        String basePath = baseDirectoryPath + "/abinitio/" + env + "/adjustment";
        Files.createDirectories(Paths.get(basePath));
        Files.createDirectories(Paths.get(basePath + "/base"));
        Files.createDirectories(Paths.get(basePath + "/adjusted"));
        Files.createDirectories(Paths.get(basePath + "/adjusted/keyvalues"));
    }

    /**
     * Process file creation request and upload files to S3
     *
     * @param request File creation details
     * @return CompletableFuture with FileCompletionRequest containing upload status
     */
    public CompletableFuture<FileCompletionRequest> processFileRequest(FileCreationDetails request) {
        log.info("Processing file request with ID: {}", request.requestId());

        // Determine which files need to be created
        boolean isKeyFile = request.keysData() != null;
        boolean isAdjFile = request.adjustData() != null && !request.adjustData().isEmpty();
        boolean isBaseFile = true; // Always create a base file for every request

        return CompletableFuture.supplyAsync(() -> {
            try {
                // Create and track results for each file type
                AtomicBoolean keyFileSuccess = new AtomicBoolean(!isKeyFile); // true if not required
                AtomicBoolean adjFileSuccess = new AtomicBoolean(!isAdjFile); // true if not required
                AtomicBoolean baseFileSuccess = new AtomicBoolean(false);

                // Process key file if required
                if (isKeyFile) {
                    boolean success = processKeyFile(request);
                    keyFileSuccess.set(success);
                    log.info("Key file processing result for request {}: {}",
                            request.requestId(), success ? "SUCCESS" : "FAILED");
                }

                // Process adjusted files if required
                if (isAdjFile) {
                    boolean success = processAdjustedFiles(request);
                    adjFileSuccess.set(success);
                    log.info("Adjusted files processing result for request {}: {}",
                            request.requestId(), success ? "SUCCESS" : "FAILED");
                }

                // Process base file (always)
                boolean success = processBaseFile(request);
                baseFileSuccess.set(success);
                log.info("Base file processing result for request {}: {}",
                        request.requestId(), success ? "SUCCESS" : "FAILED");

                // Create and return response
                return createCompletionResponse(
                        request,
                        baseFileSuccess.get(),
                        isBaseFile,
                        adjFileSuccess.get(),
                        isAdjFile,
                        keyFileSuccess.get(),
                        isKeyFile
                );

            } catch (Exception e) {
                log.error("Error processing file request {}: {}", request.requestId(), e.getMessage(), e);
                return createErrorResponse(request, isBaseFile, isAdjFile, isKeyFile);
            }
        }, fileProcessingExecutor);
    }

    /**
     * Creates a successful completion response
     */
    private FileCompletionRequest createCompletionResponse(
            FileCreationDetails request,
            boolean baseFileSuccess,
            boolean isBaseFile,
            boolean adjFileSuccess,
            boolean isAdjFile,
            boolean keyFileSuccess,
            boolean isKeyFile) {

        return new FileCompletionRequest(
                request.requestId(),
                request.requestType(),
                request.cobDate(),
                request.freq(),
                baseFileSuccess,
                isBaseFile,
                adjFileSuccess,
                isAdjFile,
                keyFileSuccess,
                isKeyFile
        );
    }

    /**
     * Creates an error response when file processing fails
     */
    private FileCompletionRequest createErrorResponse(
            FileCreationDetails request,
            boolean isBaseFile,
            boolean isAdjFile,
            boolean isKeyFile) {

        return new FileCompletionRequest(
                request.requestId(),
                request.requestType(),
                request.cobDate(),
                request.freq(),
                false,
                isBaseFile,
                false,
                isAdjFile,
                false,
                isKeyFile
        );
    }

    /**
     * Process key file - create and upload to S3
     * Creates a zero-byte file if keysData is empty
     *
     * @param request File creation details
     * @return true if successful, false otherwise
     */
    private boolean processKeyFile(FileCreationDetails request) {
        try {
            log.info("Creating key file for request: {}", request.requestId());

            // Create file paths
            String localFilePath = baseDirectoryPath + "/logicRcrdID_" + request.requestId() + ".dat";
            String s3ObjectKey = buildS3ObjectKey(request.requestId(), "adjusted/keyvalues/keys.dat");

            log.info("Key file local path: {}", localFilePath);
            log.info("Key file S3 object key: {}", s3ObjectKey);

            // Create and write to file
            Path filePath = createLocalFile(localFilePath, request.keysData());

            // Upload file to S3
            return uploadFileToS3(request.requestId(), "key file", filePath, s3ObjectKey);

        } catch (Exception e) {
            log.error("Failed to create or upload key file for request {}: {}",
                    request.requestId(), e.getMessage(), e);
            return false;
        }
    }

    /**
     * Process all adjusted files - create and upload to S3
     * Creates zero-byte files if entity data is empty
     *
     * @param request File creation details
     * @return true if all files uploaded successfully, false if any fail
     */
    private boolean processAdjustedFiles(FileCreationDetails request) {
        if (request.adjustData() == null || request.adjustData().isEmpty()) {
            log.info("No adjust data to process for request: {}", request.requestId());
            return true;
        }

        List<CompletableFuture<Boolean>> futures = new ArrayList<>();

        // Process each entity file in parallel
        for (Map.Entry<String, List<String>> entry : request.adjustData().entrySet()) {
            String entityName = entry.getKey();
            List<String> data = entry.getValue();

            CompletableFuture<Boolean> future = CompletableFuture.supplyAsync(() ->
                    processAdjustedFile(request, entityName, data), fileProcessingExecutor);

            futures.add(future);
        }

        // Wait for all futures to complete
        try {
            CompletableFuture<Void> allDone = CompletableFuture.allOf(
                    futures.toArray(new CompletableFuture[0]));
            allDone.get(UPLOAD_TIMEOUT_MINUTES, TimeUnit.MINUTES);

            // Check if any failed
            return futures.stream().allMatch(f -> {
                try {
                    return f.get();
                } catch (Exception e) {
                    return false;
                }
            });

        } catch (Exception e) {
            log.error("Error waiting for adjusted file processing: {}", e.getMessage(), e);
            return false;
        }
    }

    /**
     * Process a single adjusted file
     */
    private boolean processAdjustedFile(FileCreationDetails request, String entityName, List<String> data) {
        try {
            log.info("Creating adjusted file for entity: {} (request ID: {})",
                    entityName, request.requestId());

            // Clean entity name
            String cleanEntityName = entityName.toLowerCase().replace("_vue", "");

            // Create file paths
            String localFilePath = baseDirectoryPath + "/" + cleanEntityName + "_adjusted_" + request.requestId() + ".dat";
            String s3ObjectKey = buildS3ObjectKey(request.requestId(), "adjusted/" + cleanEntityName + ".dat");

            log.info("Adjusted file local path for {}: {}", entityName, localFilePath);
            log.info("Adjusted file S3 object key for {}: {}", entityName, s3ObjectKey);

            // Create and write to file
            Path filePath = createLocalFile(localFilePath, data);

            // Upload file to S3
            return uploadFileToS3(request.requestId(), "adjusted file for entity " + entityName, filePath, s3ObjectKey);

        } catch (Exception e) {
            log.error("Failed to create or upload adjusted file for entity {} (request {}): {}",
                    entityName, request.requestId(), e.getMessage(), e);
            return false;
        }
    }

    /**
     * Process base file - create and upload to S3
     *
     * @param request File creation details
     * @return true if successful, false otherwise
     */
    private boolean processBaseFile(FileCreationDetails request) {
        try {
            log.info("Creating base file for request: {}", request.requestId());

            // Create file paths
            String localFilePath = baseDirectoryPath + "/" + request.requestId() + ".dat";
            String s3ObjectKey = buildS3ObjectKey(request.requestId(), "base/data.dat");

            log.info("Base file local path: {}", localFilePath);
            log.info("Base file S3 object key: {}", s3ObjectKey);

            // Generate sample data for base file
            List<String> sampleRecords = generateSampleBaseFileData(request);

            // Create and write to file
            Path filePath = createLocalFile(localFilePath, sampleRecords);

            // Upload file to S3
            return uploadFileToS3(request.requestId(), "base file", filePath, s3ObjectKey);

        } catch (Exception e) {
            log.error("Failed to create or upload base file for request {}: {}",
                    request.requestId(), e.getMessage(), e);
            return false;
        }
    }

    /**
     * Create a local file and write data to it
     *
     * @param filePath Path to create the file at
     * @param data Data to write (if null or empty, creates a zero-byte file)
     * @return Path object for the created file
     */
    private Path createLocalFile(String filePath, List<String> data) throws IOException {
        // Ensure directory exists
        Path directory = Paths.get(filePath).getParent();
        if (directory != null) {
            Files.createDirectories(directory);
        }

        // Create the file
        Path path = Paths.get(filePath);

        if (data == null || data.isEmpty()) {
            // Create empty (zero-byte) file if no data
            log.info("Creating zero-byte file as data is empty: {}", filePath);
            Files.write(path, new byte[0],
                    StandardOpenOption.CREATE,
                    StandardOpenOption.TRUNCATE_EXISTING);
        } else {
            // Normal case - write data to file
            String content = String.join(System.lineSeparator(), data);
            Files.write(path, content.getBytes(),
                    StandardOpenOption.CREATE,
                    StandardOpenOption.TRUNCATE_EXISTING);
        }

        log.info("File created: {}", filePath);
        return path;
    }

    /**
     * Upload a file to S3
     *
     * @param requestId Request ID for logging
     * @param fileDescription Description of file for logging
     * @param filePath Local file path
     * @param s3ObjectKey S3 object key
     * @return true if successful, false otherwise
     */
    private boolean uploadFileToS3(String requestId, String fileDescription, Path filePath, String s3ObjectKey) {
        // Simulate failure for specific request IDs
        if (requestId.equals("REQ1") || requestId.equals("REQ2") || requestId.equals("REQ8")) {
            log.error("Simulated failure for {} upload with request ID: {}", fileDescription, requestId);
            return false;
        }
        try {
            // Get file size
            long fileSize = Files.size(filePath);

            // Choose upload method based on file size
            if (fileSize < FILE_SIZE_THRESHOLD_BYTES) {
                fileUploadService.uploadSimpleFile(bucketName, s3ObjectKey, filePath, fileSize);
            } else {
                fileUploadService.uploadFileParallel(bucketName, s3ObjectKey, filePath);
            }

            log.info("{} uploaded successfully for request: {}",
                    capitalizeFirstLetter(fileDescription), requestId);
            return true;
        } catch (Exception e) {
            log.error("Failed to upload {} for request {}: {}",
                    fileDescription, requestId, e.getMessage(), e);
            return false;
        } finally {
            // Optionally clean up local file - uncomment if needed
            // try {
            //     Files.deleteIfExists(filePath);
            //     log.debug("Deleted local file: {}", filePath);
            // } catch (IOException e) {
            //     log.warn("Failed to delete local file {}: {}", filePath, e.getMessage());
            // }
        }
    }

    /**
     * Build the S3 object key for a file
     *
     * @param requestId Request ID
     * @param objectPath Path portion of the object key
     * @return Complete S3 object key
     */
    private String buildS3ObjectKey(String requestId, String objectPath) {
        String env = getExecutionEnvironment();
        return "data/abinitio/" + env + "/adjustment/" + requestId + "/" + objectPath;
    }

    /**
     * Generate sample data for base file (5 records)
     * In a real implementation, this would fetch data from a database
     *
     * @param request File creation details
     * @return List of sample data records
     */
    private List<String> generateSampleBaseFileData(FileCreationDetails request) {
        List<String> records = new ArrayList<>();

        // Get current timestamp for the records
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));

        // Add 5 sample records
        records.add("ID001," + request.requestId() + ",FIELD1_VALUE," + timestamp + ",ACTIVE");
        records.add("ID002," + request.requestId() + ",FIELD2_VALUE," + timestamp + ",INACTIVE");
        records.add("ID003," + request.requestId() + ",FIELD3_VALUE," + timestamp + ",PENDING");
        records.add("ID004," + request.requestId() + ",FIELD4_VALUE," + timestamp + ",ACTIVE");
        records.add("ID005," + request.requestId() + ",FIELD5_VALUE," + timestamp + ",COMPLETE");

        return records;
    }

    /**
     * Get execution environment based on active profile
     *
     * @return Execution environment string
     */
    private String getExecutionEnvironment() {
        if (activeProfile == null) {
            return "dev";
        }

        switch (activeProfile.toLowerCase()) {
            case "test":
            case "uat":
                return "uat";
            case "preprod":
                return "pre";
            case "prod":
                return "prd";
            default:
                return "dev";
        }
    }

    /**
     * Utility method to capitalize the first letter of a string
     */
    private String capitalizeFirstLetter(String input) {
        if (input == null || input.isEmpty()) {
            return input;
        }
        return input.substring(0, 1).toUpperCase() + input.substring(1);
    }
}-----------------

package com.example.s3upload.service;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;
import software.amazon.awssdk.core.async.AsyncRequestBody;
import software.amazon.awssdk.services.s3.S3AsyncClient;
import software.amazon.awssdk.services.s3.model.*;

import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.file.Path;
import java.nio.file.StandardOpenOption;
import java.time.Duration;
import java.time.Instant;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;
import java.util.concurrent.*;
import java.util.stream.Collectors;

/**
 * Service responsible for uploading files to S3 using both simple and multipart upload strategies.
 * Provides optimized parallel multipart uploads for large files.
 */
@Service
public class FileUploadService {
    private static final Logger log = LoggerFactory.getLogger(FileUploadService.class);

    // S3 multipart upload constraints
    private static final long MIN_PART_SIZE_BYTES = 5 * 1024 * 1024; // 5MB
    private static final int MAX_PARTS = 10000;
    private static final int DEFAULT_MAX_RETRIES = 3;
    private static final long MB = 1024 * 1024;

    // Timeouts (in minutes)
    private static final int INITIATE_UPLOAD_TIMEOUT = 2;
    private static final int COMPLETE_UPLOAD_TIMEOUT = 5;
    private static final int ABORT_UPLOAD_TIMEOUT = 2;
    private static final int THREAD_POOL_SHUTDOWN_TIMEOUT_SECONDS = 30;

    @Value("${app.upload.max-threads:10}")
    private int maxUploadThreads;

    @Value("${app.upload.part-size-mb:5}")
    private int partSizeMb;

    @Value("${app.upload.timeout-minutes:30}")
    private int uploadTimeoutMinutes;

    private final S3AsyncClient s3Client;
    private ExecutorService uploadThreadPool;

    /**
     * Constructor with dependency injection for the S3 client.
     *
     * @param s3Client Configured S3 async client for AWS or LocalStack
     */
    public FileUploadService(S3AsyncClient s3Client) {
        this.s3Client = s3Client;
    }

    /**
     * Initialize the service after construction.
     */
    @PostConstruct
    public void initialize() {
        uploadThreadPool = Executors.newFixedThreadPool(maxUploadThreads);
        log.info("Initialized upload thread pool with {} threads", maxUploadThreads);
    }

    /**
     * Simple upload for small files (not using multipart upload).
     *
     * @param bucketName Target S3 bucket
     * @param objectKey Object key in S3
     * @param filePath Path to local file
     * @param fileSize Size of the file
     * @throws IOException If upload fails
     */
    public void uploadSimpleFile(String bucketName, String objectKey, Path filePath, long fileSize) throws IOException {
        log.info("Starting simple upload for file: {} ({} bytes) to {}/{}",
                filePath.getFileName(), fileSize, bucketName, objectKey);

        try {
            // Create the PutObject request
            PutObjectRequest putObjectRequest = PutObjectRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .build();

            // Upload the file
            s3Client.putObject(putObjectRequest, AsyncRequestBody.fromFile(filePath))
                    .get(uploadTimeoutMinutes, TimeUnit.MINUTES);

            log.info("Simple upload completed successfully for {}", objectKey);
        } catch (Exception e) {
            String errorMsg = "Simple upload failed for " + objectKey + ": " + e.getMessage();
            log.error(errorMsg);
            throw new IOException(errorMsg, e);
        }
    }

    /**
     * Upload a file with parallel part uploads for better performance.
     * Automatically calculates optimal part size and uses multiple threads for uploading.
     *
     * @param bucketName Target S3 bucket
     * @param objectKey Object key in S3
     * @param filePath Path to local file
     * @throws Exception If upload fails
     */
    public void uploadFileParallel(String bucketName, String objectKey, Path filePath) throws Exception {
        Instant startTime = Instant.now();
        log.info("Starting parallel multipart upload for {}", objectKey);

        // Step 1: Calculate optimal part configuration
        ParallelUploadConfig config = calculateUploadConfig(filePath);
        logUploadConfiguration(config, objectKey);

        // Step 2: Initiate multipart upload
        String uploadId = initiateMultipartUpload(bucketName, objectKey);
        log.info("Multipart upload initiated with ID: {}", uploadId);

        try {
            // Step 3: Upload all parts in parallel
            List<CompletedPart> completedParts = uploadAllParts(
                    bucketName, objectKey, uploadId, filePath, config);

            // Step 4: Complete multipart upload
            completeMultipartUpload(bucketName, objectKey, uploadId, completedParts);

            // Log upload statistics
            logUploadStatistics(startTime, objectKey, config);
        } catch (Exception e) {
            // Abort upload on failure
            log.error("Multipart upload failed: {}", e.getMessage());
            abortMultipartUpload(bucketName, objectKey, uploadId);
            throw new IOException("Failed to upload file " + objectKey, e);
        }
    }

    /**
     * Uploads all parts of a file in parallel and returns the completed parts.
     */
    private List<CompletedPart> uploadAllParts(
            String bucketName, String objectKey, String uploadId,
            Path filePath, ParallelUploadConfig config) throws Exception {

        log.debug("Creating upload tasks for all {} parts...", config.numParts);
        List<CompletableFuture<CompletedPart>> uploadFutures = new ArrayList<>();

        // Create futures for each part upload
        for (int partNumber = 1; partNumber <= config.numParts; partNumber++) {
            PartUploadTask task = createPartUploadTask(
                    partNumber, config.partSize, config.fileSize);

            CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(
                    () -> uploadPartWithRetry(bucketName, objectKey, uploadId, filePath,
                            task, DEFAULT_MAX_RETRIES),
                    uploadThreadPool);

            uploadFutures.add(future);
        }

        log.info("All upload tasks created. Waiting for completion...");

        // Wait for all uploads to complete
        CompletableFuture<Void> allUploadsFuture = CompletableFuture.allOf(
                uploadFutures.toArray(new CompletableFuture[0]));

        allUploadsFuture.get(uploadTimeoutMinutes, TimeUnit.MINUTES);
        log.info("All part uploads completed successfully");

        // Collect and return all completed parts
        return uploadFutures.stream()
                .map(CompletableFuture::join)
                .collect(Collectors.toList());
    }

    /**
     * Creates a task description for uploading a specific part.
     */
    private PartUploadTask createPartUploadTask(int partNumber, long partSize, long fileSize) {
        final long position = (partNumber - 1) * partSize;
        final long size = Math.min(partSize, fileSize - position);

        return new PartUploadTask(partNumber, position, size);
    }

    /**
     * Uploads a single part with retries in case of failure.
     */
    private CompletedPart uploadPartWithRetry(
            String bucketName, String objectKey, String uploadId,
            Path filePath, PartUploadTask task, int maxRetries) {

        String threadName = Thread.currentThread().getName();
        log.debug("Thread {}: Starting upload of part {} (bytes {} to {})",
                threadName, task.partNumber, task.position, task.position + task.size - 1);

        int retryCount = 0;
        Exception lastException = null;

        while (retryCount <= maxRetries) {
            try {
                if (retryCount > 0) {
                    log.debug("Retry #{} for part {}", retryCount, task.partNumber);
                }

                CompletedPart part = uploadSinglePart(
                        bucketName, objectKey, uploadId, filePath, task);

                log.debug("Thread {}: Completed upload of part {}, ETag: {}",
                        threadName, task.partNumber, part.eTag());

                return part;

            } catch (Exception e) {
                lastException = e;
                retryCount++;

                if (retryCount > maxRetries) {
                    log.error("Part {} failed after {} retries: {}",
                            task.partNumber, retryCount, e.getMessage());
                    break;
                }

                // Calculate exponential backoff time
                long backoffMillis = (long) (Math.pow(2, retryCount) * 100);
                log.debug("Part {} upload failed, retrying in {} ms. Error: {}",
                        task.partNumber, backoffMillis, e.getMessage());

                try {
                    Thread.sleep(backoffMillis);
                } catch (InterruptedException ie) {
                    Thread.currentThread().interrupt();
                    throw new RuntimeException("Thread interrupted during backoff", ie);
                }
            }
        }

        // If we got here, we exhausted all retries
        throw new RuntimeException("Failed to upload part " + task.partNumber, lastException);
    }

    /**
     * Upload a single part of the file.
     */
    private CompletedPart uploadSinglePart(
            String bucketName, String objectKey, String uploadId,
            Path filePath, PartUploadTask task) throws IOException {

        // Read this part from the file
        ByteBuffer partData = readFileSegment(filePath, task.position, task.size);

        // Create and execute upload request
        UploadPartRequest uploadPartRequest = UploadPartRequest.builder()
                .bucket(bucketName)
                .key(objectKey)
                .uploadId(uploadId)
                .partNumber(task.partNumber)
                .contentLength(task.size)
                .build();

        try {
            UploadPartResponse response = s3Client.uploadPart(
                            uploadPartRequest, AsyncRequestBody.fromByteBuffer(partData))
                    .get(uploadTimeoutMinutes, TimeUnit.MINUTES);

            return CompletedPart.builder()
                    .partNumber(task.partNumber)
                    .eTag(response.eTag())
                    .build();

        } catch (Exception e) {
            throw new IOException("Failed to upload part " + task.partNumber + ": " + e.getMessage(), e);
        }
    }

    /**
     * Calculate optimal part size and count based on file size.
     * Returns a configuration object with upload parameters.
     */
    private ParallelUploadConfig calculateUploadConfig(Path filePath) throws IOException {
        long fileSize = filePath.toFile().length();

        // Start with configured or minimum part size
        long partSize = Math.max(partSizeMb * MB, MIN_PART_SIZE_BYTES);

        // If file is large enough to exceed max parts limit, increase part size
        if (fileSize > partSize * MAX_PARTS) {
            // Calculate minimum required part size to stay under part count limit
            partSize = (fileSize + MAX_PARTS - 1) / MAX_PARTS; // Ceiling division

            // Round up to nearest MB for cleaner numbers
            partSize = ((partSize + MB - 1) / MB) * MB; // Round up to nearest MB
        }

        // Calculate part count based on part size
        long partCount = (fileSize + partSize - 1) / partSize; // Ceiling division

        return new ParallelUploadConfig(fileSize, partSize, partCount);
    }

    /**
     * Initiate a multipart upload in S3.
     */
    private String initiateMultipartUpload(String bucketName, String objectKey) {
        try {
            CreateMultipartUploadRequest request = CreateMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .contentType("application/octet-stream")
                    .build();

            CreateMultipartUploadResponse response = s3Client.createMultipartUpload(request)
                    .get(INITIATE_UPLOAD_TIMEOUT, TimeUnit.MINUTES);

            return response.uploadId();
        } catch (Exception e) {
            throw new RuntimeException("Failed to initiate multipart upload: " + e.getMessage(), e);
        }
    }

    /**
     * Complete the multipart upload by combining all uploaded parts.
     */
    private void completeMultipartUpload(
            String bucketName, String objectKey, String uploadId, List<CompletedPart> parts) {

        try {
            // Sort parts by part number
            List<CompletedPart> sortedParts = parts.stream()
                    .sorted(Comparator.comparingInt(CompletedPart::partNumber))
                    .collect(Collectors.toList());

            CompleteMultipartUploadRequest completeRequest = CompleteMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .uploadId(uploadId)
                    .multipartUpload(CompletedMultipartUpload.builder()
                            .parts(sortedParts)
                            .build())
                    .build();

            CompleteMultipartUploadResponse response = s3Client.completeMultipartUpload(completeRequest)
                    .get(COMPLETE_UPLOAD_TIMEOUT, TimeUnit.MINUTES);

            log.info("Multipart upload completed successfully! Object: {}, ETag: {}",
                    response.key(), response.eTag());

        } catch (Exception e) {
            throw new RuntimeException("Failed to complete multipart upload: " + e.getMessage(), e);
        }
    }

    /**
     * Abort a multipart upload.
     */
    private void abortMultipartUpload(String bucketName, String objectKey, String uploadId) {
        try {
            AbortMultipartUploadRequest abortRequest = AbortMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .uploadId(uploadId)
                    .build();

            s3Client.abortMultipartUpload(abortRequest).get(ABORT_UPLOAD_TIMEOUT, TimeUnit.MINUTES);
            log.info("Multipart upload aborted successfully");
        } catch (Exception e) {
            // Just log error since this is already in an error path
            log.error("Failed to abort multipart upload: {}", e.getMessage());
        }
    }

    /**
     * Read a specific segment from the file.
     */
    private ByteBuffer readFileSegment(Path filePath, long position, long size) throws IOException {
        ByteBuffer buffer = ByteBuffer.allocate((int) size);

        try (FileChannel channel = FileChannel.open(filePath, StandardOpenOption.READ)) {
            channel.position(position);

            int bytesRead;
            int totalRead = 0;

            // Read until buffer is full or EOF
            while (totalRead < size && (bytesRead = channel.read(buffer)) != -1) {
                totalRead += bytesRead;
            }

            if (totalRead < size) {
                log.warn("Read {} bytes but expected {} bytes", totalRead, size);
            }

            buffer.flip();
            return buffer;
        }
    }

    /**
     * Log the upload configuration.
     */
    private void logUploadConfiguration(ParallelUploadConfig config, String objectKey) {
        log.info("Upload configuration for {}:", objectKey);
        log.info("  - File size: {} bytes ({} MB)",
                config.fileSize, config.fileSize / MB);
        log.info("  - Part size: {} bytes ({} MB)",
                config.partSize, config.partSize / MB);
        log.info("  - Number of parts: {}", config.numParts);
        log.info("  - Thread pool size: {}", maxUploadThreads);
    }

    /**
     * Log upload statistics at the end of an upload.
     */
    private void logUploadStatistics(Instant startTime, String objectKey, ParallelUploadConfig config) {
        Duration uploadTime = Duration.between(startTime, Instant.now());
        double throughputMBps = (double) config.fileSize / (uploadTime.toMillis() * 1000) * MB;

        log.info("Upload statistics for {}:", objectKey);
        log.info("  - Total time: {}.{} seconds",
                uploadTime.getSeconds(), uploadTime.getNano() / 1000000);
        log.info("  - Throughput: {:.2f} MB/s", throughputMBps);
    }

    /**
     * Clean up resources on service shutdown.
     */
    @PreDestroy
    public void cleanup() {
        log.info("Shutting down FileUploadService resources");

        if (uploadThreadPool != null && !uploadThreadPool.isShutdown()) {
            try {
                uploadThreadPool.shutdown();
                if (!uploadThreadPool.awaitTermination(THREAD_POOL_SHUTDOWN_TIMEOUT_SECONDS, TimeUnit.SECONDS)) {
                    log.warn("Upload thread pool did not terminate in allowed time - forcing shutdown");
                    uploadThreadPool.shutdownNow();
                }
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                log.warn("Upload thread pool shutdown interrupted", e);
                uploadThreadPool.shutdownNow();
            }
        }
    }

    /**
     * Configuration class for parallel uploads.
     */
    private static class ParallelUploadConfig {
        final long fileSize;
        final long partSize;
        final long numParts;

        ParallelUploadConfig(long fileSize, long partSize, long numParts) {
            this.fileSize = fileSize;
            this.partSize = partSize;
            this.numParts = numParts;
        }
    }

    /**
     * Task description for a single part upload.
     */
    private static class PartUploadTask {
        final int partNumber;
        final long position;
        final long size;

        PartUploadTask(int partNumber, long position, long size) {
            this.partNumber = partNumber;
            this.position = position;
            this.size = size;
        }
    }
}
===============
package com.example.s3upload.web;

import com.example.s3upload.model.FileCompletionRequest;
import com.example.s3upload.model.FileCreationDetails;
import com.example.s3upload.service.FileService;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.concurrent.CompletableFuture;

/**
 * REST Controller for file operations
 */
@RestController
@RequestMapping("/api/files")
public class FileController {
    private static final Logger log = LoggerFactory.getLogger(FileController.class);

    private final FileService fileService;

    public FileController(FileService fileService) {
        this.fileService = fileService;
    }

    /**
     * Endpoint to process file creation requests
     *
     * @param request File creation details
     * @return FileCompletionRequest with upload status
     */
    @PostMapping("/process")
    public CompletableFuture<ResponseEntity<FileCompletionRequest>> processFile(
            @RequestBody FileCreationDetails request) {

        log.info("Received file processing request with ID: {}", request.requestId());

        return fileService.processFileRequest(request)
                .thenApply(response -> {
                    log.info("Completed processing request ID: {}", request.requestId());
                    return ResponseEntity.ok(response);
                })
                .exceptionally(ex -> {
                    log.error("Error processing request ID: {}", request.requestId(), ex);
                    return ResponseEntity.internalServerError().body(
                            new FileCompletionRequest(
                                    request.requestId(),
                                    request.requestType(),
                                    request.cobDate(),
                                    request.freq(),
                                    false,
                                    false,
                                    false,
                                    !request.adjustData().isEmpty(),
                                    false,
                                    !request.keysData().isEmpty()
                            )
                    );
                });
    }
}
=======================
package com.example.s3upload;

import java.io.*;
import java.net.HttpURLConnection;
import java.net.URL;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.ThreadLocalRandom;

/**
 * Test program to generate a 500MB file with 5 entities (100MB each)
 * and call the upload endpoint
 */
public class LargeFileUploadTest {

    // Configuration
    private static final int ENTITY_COUNT = 5;
    private static final long ENTITY_SIZE_MB = 100;
    private static final String UPLOAD_URL = "http://localhost:8080/files/upload";
    private static final String OUTPUT_DIR = "D:\\data";
    private static final String UPLOAD_BUCKET = "demo"; // Optional bucket name

    // Entity names
    private static final String[] ENTITY_NAMES = {
            "customer-records",
            "transaction-history",
            "product-catalog",
            "inventory-tracking",
            "analytics-data"
    };

    public static void main(String[] args) {
        try {
            System.out.println("Starting Large File Upload Test");
            System.out.println("Generating " + ENTITY_COUNT + " entities with " + ENTITY_SIZE_MB + "MB each");

            // Create output directory if it doesn't exist
            Path outputDir = Paths.get(OUTPUT_DIR);
            Files.createDirectories(outputDir);

            // Generate entities and prepare payload
            List<Map<String, Object>> entityPayloads = generateEntities(outputDir);

            // Convert payload to JSON
            String jsonPayload = convertToJson(entityPayloads);

            // Save payload to file for inspection
            Path payloadFile = outputDir.resolve("payload.json");
            Files.writeString(payloadFile, jsonPayload);
            System.out.println("Payload saved to: " + payloadFile);

            // Call upload endpoint
            System.out.println("Calling upload endpoint: " + UPLOAD_URL);
            String response = callUploadEndpoint(jsonPayload);

            System.out.println("Upload response:");
            System.out.println(response);

            System.out.println("Test completed successfully");

        } catch (Exception e) {
            System.err.println("Error during test: " + e.getMessage());
            e.printStackTrace();
        }
    }

    /**
     * Generate entities with large data
     */
    private static List<Map<String, Object>> generateEntities(Path outputDir) throws IOException {
        List<Map<String, Object>> entityPayloads = new ArrayList<>();

        for (int i = 0; i < ENTITY_COUNT; i++) {
            String entityName = ENTITY_NAMES[i];
            System.out.println("Generating entity: " + entityName);

            // Generate data for this entity
            List<String> data = generateLargeEntityData(ENTITY_SIZE_MB);

            // Create entity payload
            Map<String, Object> entityPayload = new HashMap<>();
            entityPayload.put("entityName", entityName);
            entityPayload.put("data", data);

            entityPayloads.add(entityPayload);

            // Log progress
            System.out.println("Entity " + entityName + " generated with " +
                    data.size() + " lines (" + (i+1) + "/" + ENTITY_COUNT + ")");
        }

        return entityPayloads;
    }

    /**
     * Generate large data for an entity
     */
    private static List<String> generateLargeEntityData(long sizeMB) {
        List<String> data = new ArrayList<>();

        // Add header row
        data.add(generateHeader());

        // Track size
        long totalSize = data.get(0).length();
        long targetSize = sizeMB * 1024 * 1024; // Convert to bytes

        int rowCounter = 0;
        int progressInterval = 1000;

        // Generate rows until we reach target size
        while (totalSize < targetSize) {
            String row = generateDataRow();
            data.add(row);
            totalSize += row.length();

            rowCounter++;
            if (rowCounter % progressInterval == 0) {
                double percentComplete = (double) totalSize / targetSize * 100;
                System.out.printf("  Progress: %.2f%% (%d KB / %d MB)%n",
                        percentComplete, totalSize / 1024, sizeMB);
            }
        }

        System.out.println("  Generated " + rowCounter + " rows, total size: " +
                (totalSize / (1024 * 1024)) + "MB");

        return data;
    }

    /**
     * Generate a header row for CSV data
     */
    private static String generateHeader() {
        return "id,timestamp,customer_id,product_id,transaction_type,amount,status,region,category,subcategory," +
                "payment_method,shipping_method,discount_code,tax_amount,item_count,description";
    }

    /**
     * Generate a random data row
     */
    private static String generateDataRow() {
        StringBuilder row = new StringBuilder();

        // id
        row.append(UUID.randomUUID()).append(",");

        // timestamp
        LocalDateTime timestamp = LocalDateTime.now().minusDays(ThreadLocalRandom.current().nextInt(365));
        row.append(timestamp.format(DateTimeFormatter.ISO_LOCAL_DATE_TIME)).append(",");

        // customer_id
        row.append("CUST-").append(ThreadLocalRandom.current().nextInt(1, 100000)).append(",");

        // product_id
        row.append("PROD-").append(ThreadLocalRandom.current().nextInt(1, 50000)).append(",");

        // transaction_type
        String[] transactionTypes = {"PURCHASE", "REFUND", "EXCHANGE", "RETURN", "ADJUSTMENT"};
        row.append(transactionTypes[ThreadLocalRandom.current().nextInt(transactionTypes.length)]).append(",");

        // amount
        double amount = ThreadLocalRandom.current().nextDouble(1.0, 1000.0);
        row.append(String.format("%.2f", amount)).append(",");

        // status
        String[] statuses = {"COMPLETED", "PENDING", "CANCELLED", "FAILED", "IN_PROGRESS"};
        row.append(statuses[ThreadLocalRandom.current().nextInt(statuses.length)]).append(",");

        // region
        String[] regions = {"NORTH", "SOUTH", "EAST", "WEST", "CENTRAL", "NORTHEAST", "NORTHWEST", "SOUTHEAST", "SOUTHWEST"};
        row.append(regions[ThreadLocalRandom.current().nextInt(regions.length)]).append(",");

        // category
        String[] categories = {"ELECTRONICS", "CLOTHING", "FOOD", "BOOKS", "HOME", "BEAUTY", "SPORTS", "TOYS", "AUTOMOTIVE"};
        row.append(categories[ThreadLocalRandom.current().nextInt(categories.length)]).append(",");

        // subcategory
        String[] subcategories = {"PREMIUM", "STANDARD", "ECONOMY", "CLEARANCE", "NEW", "USED", "REFURBISHED"};
        row.append(subcategories[ThreadLocalRandom.current().nextInt(subcategories.length)]).append(",");

        // payment_method
        String[] paymentMethods = {"CREDIT", "DEBIT", "PAYPAL", "APPLE_PAY", "GOOGLE_PAY", "BANK_TRANSFER", "GIFT_CARD"};
        row.append(paymentMethods[ThreadLocalRandom.current().nextInt(paymentMethods.length)]).append(",");

        // shipping_method
        String[] shippingMethods = {"STANDARD", "EXPRESS", "NEXT_DAY", "TWO_DAY", "INTERNATIONAL", "PICKUP"};
        row.append(shippingMethods[ThreadLocalRandom.current().nextInt(shippingMethods.length)]).append(",");

        // discount_code
        boolean hasDiscount = ThreadLocalRandom.current().nextBoolean();
        if (hasDiscount) {
            row.append("DISC-").append(ThreadLocalRandom.current().nextInt(1, 100)).append(",");
        } else {
            row.append(",");
        }

        // tax_amount
        double taxAmount = amount * 0.08; // Assume 8% tax
        row.append(String.format("%.2f", taxAmount)).append(",");

        // item_count
        row.append(ThreadLocalRandom.current().nextInt(1, 11)).append(",");

        // description
        row.append("\"Transaction details with additional information for analysis purposes. ");
        row.append("This includes extended metadata about the transaction that can be used for ");
        row.append("various business intelligence reports and dashboards. The data can also be ");
        row.append("used for customer trend analysis and marketing campaign effectiveness measurement.\"");

        return row.toString();
    }

    /**
     * Convert entity payloads to JSON
     */
    private static String convertToJson(List<Map<String, Object>> entityPayloads) {
        StringBuilder json = new StringBuilder();
        json.append("[\n");

        for (int i = 0; i < entityPayloads.size(); i++) {
            Map<String, Object> entity = entityPayloads.get(i);
            json.append("  {\n");
            json.append("    \"entityName\": \"").append(entity.get("entityName")).append("\",\n");
            json.append("    \"data\": [\n");

            @SuppressWarnings("unchecked")
            List<String> data = (List<String>) entity.get("data");

            for (int j = 0; j < data.size(); j++) {
                json.append("      \"").append(escapeJson(data.get(j))).append("\"");
                if (j < data.size() - 1) {
                    json.append(",");
                }
                json.append("\n");
            }

            json.append("    ]\n");
            json.append("  }");
            if (i < entityPayloads.size() - 1) {
                json.append(",");
            }
            json.append("\n");
        }

        json.append("]\n");
        return json.toString();
    }

    /**
     * Escape special characters in JSON
     */
    private static String escapeJson(String input) {
        return input.replace("\\", "\\\\")
                .replace("\"", "\\\"")
                .replace("\n", "\\n")
                .replace("\r", "\\r")
                .replace("\t", "\\t");
    }

    /**
     * Call upload endpoint with JSON payload
     */
    private static String callUploadEndpoint(String jsonPayload) throws IOException {
        URL url = new URL(UPLOAD_URL + (UPLOAD_BUCKET != null ? "?bucket=" + UPLOAD_BUCKET : ""));
        HttpURLConnection connection = (HttpURLConnection) url.openConnection();
        connection.setRequestMethod("POST");
        connection.setRequestProperty("Content-Type", "application/json");
        connection.setRequestProperty("Accept", "application/json");
        connection.setDoOutput(true);

        // Set longer timeouts for large data
        connection.setConnectTimeout(30000); // 30 seconds
        connection.setReadTimeout(300000);   // 5 minutes

        // Send payload
        try (OutputStream os = connection.getOutputStream()) {
            byte[] input = jsonPayload.getBytes(StandardCharsets.UTF_8);
            os.write(input, 0, input.length);
        }

        // Read response
        StringBuilder response = new StringBuilder();
        try (BufferedReader br = new BufferedReader(
                new InputStreamReader(connection.getInputStream(), StandardCharsets.UTF_8))) {
            String responseLine;
            while ((responseLine = br.readLine()) != null) {
                response.append(responseLine.trim());
            }
        }

        return response.toString();
    }
}
==============
package com.example.fileuploader;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.http.*;
import org.springframework.web.client.RestTemplate;

import java.io.File;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * Application that runs 10 concurrent threads to submit file processing requests
 */
@SpringBootApplication
public class ConcurrentRequestsApplication implements CommandLineRunner {
    private static final Logger log = LoggerFactory.getLogger(ConcurrentRequestsApplication.class);

    private static final String API_URL = "http://localhost:8080/api/files/process";
    private static final int NUM_THREADS = 10;
    private static final int REQUEST_TIMEOUT_SECONDS = 60;
    private static final String RESULTS_DIR = "D://data/test-results";

    private final ObjectMapper objectMapper;
    private final RestTemplate restTemplate;
    private final ExecutorService executorService;
    private final CountDownLatch completionLatch;
    private final AtomicInteger successCount = new AtomicInteger(0);
    private final AtomicInteger failureCount = new AtomicInteger(0);

    public ConcurrentRequestsApplication() {
        this.objectMapper = new ObjectMapper()
                .enable(SerializationFeature.INDENT_OUTPUT);
        this.restTemplate = new RestTemplate();
        this.executorService = Executors.newFixedThreadPool(NUM_THREADS);
        this.completionLatch = new CountDownLatch(NUM_THREADS);
    }

    public static void main(String[] args) {
        SpringApplication.run(ConcurrentRequestsApplication.class, args);
    }

    @Override
    public void run(String... args) throws Exception {
        log.info("Starting concurrent request test with {} threads", NUM_THREADS);

        // Create results directory
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
        String resultsDirPath = RESULTS_DIR + "/" + timestamp;
        Files.createDirectories(Paths.get(resultsDirPath));

        // Generate payloads
        List<Map<String, Object>> payloads = generatePayloads();

        // Save payloads to disk for reference
        for (int i = 0; i < payloads.size(); i++) {
            objectMapper.writeValue(new File(resultsDirPath + "/request_" + (i + 1) + ".json"), payloads.get(i));
        }

        log.info("Generated {} payloads. Starting concurrent requests...", payloads.size());

        // Submit all requests concurrently
        List<Future<RequestResult>> futures = new ArrayList<>();
        for (int i = 0; i < payloads.size(); i++) {
            final int threadId = i + 1;
            final Map<String, Object> payload = payloads.get(i);

            // Submit task to executor
            Future<RequestResult> future = executorService.submit(() -> {
                try {
                    log.info("Thread {} starting request", threadId);
                    RequestResult result = sendRequest(threadId, payload);
                    if (result.isSuccess()) {
                        successCount.incrementAndGet();
                    } else {
                        failureCount.incrementAndGet();
                    }
                    return result;
                } catch (Exception e) {
                    log.error("Thread {} error: {}", threadId, e.getMessage(), e);
                    return new RequestResult(threadId, false, HttpStatus.INTERNAL_SERVER_ERROR.value(),
                            "Error: " + e.getMessage(), null);
                } finally {
                    completionLatch.countDown();
                }
            });

            futures.add(future);
        }

        // Wait for all requests to complete
        log.info("Waiting for all requests to complete...");
        boolean completed = completionLatch.await(REQUEST_TIMEOUT_SECONDS, TimeUnit.SECONDS);
        if (!completed) {
            log.warn("Not all requests completed within the timeout period of {} seconds", REQUEST_TIMEOUT_SECONDS);
        }

        // Collect and process results
        List<RequestResult> results = new ArrayList<>();
        for (Future<RequestResult> future : futures) {
            try {
                results.add(future.get(5, TimeUnit.SECONDS));
            } catch (Exception e) {
                log.error("Error retrieving result: {}", e.getMessage());
            }
        }

        // Save results
        objectMapper.writeValue(new File(resultsDirPath + "/results.json"), results);

        // Print summary
        log.info("Test completed. Total requests: {}", NUM_THREADS);
        log.info("Successful requests: {}", successCount.get());
        log.info("Failed requests: {}", failureCount.get());
        log.info("Results saved to: {}", resultsDirPath);

        // Print each result
        System.out.println("\n\n========== TEST RESULTS ==========");
        results.stream()
                .sorted(Comparator.comparingInt(RequestResult::getThreadId))
                .forEach(result -> {
                    System.out.println("\n----------------------------------");
                    System.out.println("Thread " + result.getThreadId() + ": " +
                            (result.isSuccess() ? "SUCCESS" : "FAILURE"));
                    System.out.println("Status: " + result.getStatusCode());

                    // Print formatted JSON response
                    System.out.println("\nRESPONSE JSON:");
                    try {
                        if (result.getResponseBody() != null) {
                            // Pretty print the JSON
                            Object json = objectMapper.readValue(result.getResponseBody(), Object.class);
                            System.out.println(objectMapper.writerWithDefaultPrettyPrinter()
                                    .writeValueAsString(json));

                            // Print structured summary based on FileCompletionRequest structure
                            if (result.getResponse() != null) {
                                System.out.println("\nFILE COMPLETION SUMMARY:");
                                String requestId = (String) result.getResponse().get("requestId");
                                System.out.println("Request ID:      " + requestId);

                                // Handle possible different field names in the response
                                Boolean adjSuccess = (Boolean) (
                                        result.getResponse().containsKey("adjFileStatus") ?
                                                result.getResponse().get("adjFileStatus") :
                                                result.getResponse().get("adjFileSuccess")
                                );

                                Boolean baseSuccess = (Boolean) (
                                        result.getResponse().containsKey("baseFileStatus") ?
                                                result.getResponse().get("baseFileStatus") :
                                                result.getResponse().get("baseFileSuccess")
                                );

                                Boolean keySuccess = (Boolean) (
                                        result.getResponse().containsKey("keyFileStatus") ?
                                                result.getResponse().get("keyFileStatus") :
                                                result.getResponse().get("keyFileSuccess")
                                );

                                System.out.println("Adjusted Files:  " + formatStatus(adjSuccess));
                                System.out.println("Base File:       " + formatStatus(baseSuccess));
                                System.out.println("Key File:        " + formatStatus(keySuccess));
                            }
                        } else {
                            System.out.println("No response body received");
                        }
                    } catch (Exception e) {
                        System.out.println("Error formatting response: " + e.getMessage());
                        System.out.println("Raw response: " + result.getResponseBody());
                    }
                });
        System.out.println("\n==================================");

        // Shutdown executor
        executorService.shutdown();
    }

    /**
     * Format a boolean status value for display
     */
    private String formatStatus(Boolean status) {
        if (status == null) {
            return "UNKNOWN";
        }
        return status ? "SUCCESS" : "FAILURE";
    }

    /**
     * Send a request to the API
     */
    private RequestResult sendRequest(int threadId, Map<String, Object> payload) {
        String threadName = "Thread-" + threadId;
        log.info("{} sending request", threadName);

        try {
            // Create headers
            HttpHeaders headers = new HttpHeaders();
            headers.setContentType(MediaType.APPLICATION_JSON);
            headers.add("X-Thread-ID", String.valueOf(threadId));

            // Create request entity
            String jsonPayload = objectMapper.writeValueAsString(payload);
            HttpEntity<String> requestEntity = new HttpEntity<>(jsonPayload, headers);

            // Record start time
            long startTime = System.currentTimeMillis();

            // Send request
            ResponseEntity<String> response = restTemplate.exchange(
                    API_URL, HttpMethod.POST, requestEntity, String.class);

            // Record end time
            long duration = System.currentTimeMillis() - startTime;

            // Parse response
            Map<String, Object> responseMap = null;
            String responseBody = response.getBody();

            if (responseBody != null && !responseBody.isEmpty()) {
                try {
                    responseMap = objectMapper.readValue(responseBody, Map.class);
                    log.debug("Response map: {}", responseMap);
                } catch (Exception e) {
                    log.error("Failed to parse response JSON: {}", e.getMessage());
                }
            }

            log.info("{} received response in {}ms: {}",
                    threadName, duration, response.getStatusCode());

            if (responseMap != null) {
                // Log key statuses from the response
                String requestId = (String) responseMap.get("requestId");

                // Handle possible different field names
                Boolean keyStatus = getStatusValue(responseMap, "keyFileStatus", "keyFileSuccess");
                Boolean adjStatus = getStatusValue(responseMap, "adjFileStatus", "adjFileSuccess");
                Boolean baseStatus = getStatusValue(responseMap, "baseFileStatus", "baseFileSuccess");

                log.info("{} result: requestId={}, key={}, adj={}, base={}",
                        threadName, requestId, keyStatus, adjStatus, baseStatus);
            }

            return new RequestResult(
                    threadId,
                    response.getStatusCode().is2xxSuccessful(),
                    response.getStatusCodeValue(),
                    responseBody,
                    responseMap
            );

        } catch (Exception e) {
            log.error("{} error: {}", threadName, e.getMessage(), e);
            return new RequestResult(
                    threadId,
                    false,
                    HttpStatus.INTERNAL_SERVER_ERROR.value(),
                    "Error: " + e.getMessage(),
                    null
            );
        }
    }

    /**
     * Get a status value from the response map, checking multiple possible field names
     */
    private Boolean getStatusValue(Map<String, Object> responseMap, String... fieldNames) {
        for (String fieldName : fieldNames) {
            if (responseMap.containsKey(fieldName)) {
                return (Boolean) responseMap.get(fieldName);
            }
        }
        return null;
    }

    /**
     * Generate 10 different payloads for testing
     */
    private List<Map<String, Object>> generatePayloads() {
        List<Map<String, Object>> payloads = new ArrayList<>();

        for (int i = 1; i <= NUM_THREADS; i++) {
            String requestId = "REQ" + i;
            Map<String, Object> payload = createBasePayload(requestId);

            // Add special adjust data for this thread
            Map<String, List<String>> adjustData = createAdjustDataForThread(i);
            payload.put("adjustData", adjustData);

            payloads.add(payload);
        }

        return payloads;
    }

    /**
     * Create the base payload structure (common for all threads)
     */
    private Map<String, Object> createBasePayload(String requestId) {
        Map<String, Object> payload = new HashMap<>();

        // Basic request details
        payload.put("requestId", requestId);
        payload.put("sliceNames", Arrays.asList("Slice1", "Slice2", "Slice3"));
        payload.put("distinctIds", Arrays.asList("ID001", "ID002", "ID003"));
        payload.put("idFilePath", "D://data/idfiles/sample_ids.txt");
        payload.put("parentTableName", "CustomerAccounts");
        payload.put("requestType", "DAILY_PROCESSING");
        payload.put("cobDate", "20250515");
        payload.put("freq", "DAILY");
        payload.put("tableList", Arrays.asList("Customers", "Accounts", "Transactions"));

        // Add keys data - always successful
        List<String> keysData = Arrays.asList(
                "CUS001=JOHN_DOE_KEY",
                "CUS002=JANE_SMITH_KEY",
                "CUS003=BOB_JOHNSON_KEY",
                "ACC001=SAVINGS_ACCOUNT_KEY",
                "ACC002=CHECKING_ACCOUNT_KEY",
                "ACC003=SAVINGS_ACCOUNT_KEY_2",
                "ACC004=CHECKING_ACCOUNT_KEY_2"
        );
        payload.put("keysData", keysData);

        return payload;
    }

    /**
     * Create adjust data specific to each thread
     * Some threads will have errors to test failure scenarios
     */
    private Map<String, List<String>> createAdjustDataForThread(int threadId) {
        Map<String, List<String>> adjustData = new HashMap<>();

        // Common data for all threads
        adjustData.put("customer_vue", Arrays.asList(
                "CUS001,John Doe,Active,New York,10001",
                "CUS002,Jane Smith,Active,Los Angeles,90001",
                "CUS003,Bob Johnson,Inactive,Chicago,60601"
        ));

        // Create thread-specific adjustment data
        String adjustmentDataKey = "adjustmentdata" + threadId;
        List<String> threadSpecificData = new ArrayList<>();

        // Add some base records
        threadSpecificData.add("ADJ" + threadId + "01,Customer" + (threadId * 3 - 2) +
                ",Account" + (threadId * 3 - 2) + "," + (400 + threadId * 100) + ".00,20250" + (510 + threadId));
        threadSpecificData.add("ADJ" + threadId + "02,Customer" + (threadId * 3 - 1) +
                ",Account" + (threadId * 3 - 1) + "," + (450 + threadId * 100) + ".50,20250" + (511 + threadId));

        // Add errors to specific threads
        if (threadId == 3 || threadId == 7) {
            // Add invalid data for threads 3 and 7
            threadSpecificData.add("INVALID_DATA_FORMAT");
        } else {
            // Normal data for other threads
            threadSpecificData.add("ADJ" + threadId + "03,Customer" + (threadId * 3) +
                    ",Account" + (threadId * 3) + "," + (500 + threadId * 100) + ".00,20250" + (512 + threadId));
        }

        adjustData.put(adjustmentDataKey, threadSpecificData);

        // Modify account_vue for some threads to create failures
        if (threadId == 4 || threadId == 8) {
            adjustData.put("account_vue", Arrays.asList(
                    "ACC001,CUS001,Savings,10000.50,20200101",
                    "ACC002,CUS001,Checking,INVALID_AMOUNT,20210315",
                    "ACC003,CUS002,Savings,25000.00,20190620"
            ));
        } else {
            adjustData.put("account_vue", Arrays.asList(
                    "ACC001,CUS001,Savings,10000.50,20200101",
                    "ACC002,CUS001,Checking,5000.75,20210315",
                    "ACC003,CUS002,Savings,25000.00,20190620",
                    "ACC004,CUS003,Checking,1500.25,20220110"
            ));
        }

        // Modify transaction_vue for some threads to create failures
        if (threadId == 5 || threadId == 9) {
            adjustData.put("transaction_vue", Arrays.asList(
                    "TRX001,ACC001,Deposit,500.00,20250510",
                    "TRX002,ACC002,Withdrawal,INVALID_AMOUNT,20250511",
                    "TRX003,ACC003,Deposit,1000.00,20250512"
            ));
        } else {
            adjustData.put("transaction_vue", Arrays.asList(
                    "TRX001,ACC001,Deposit,500.00,20250510",
                    "TRX002,ACC002,Withdrawal,150.50,20250511",
                    "TRX003,ACC003,Deposit,1000.00,20250512",
                    "TRX004,ACC002,Transfer,300.25,20250513",
                    "TRX005,ACC004,Withdrawal,200.00,20250514"
            ));
        }

        return adjustData;
    }

    /**
     * Class to capture request results
     */
    static class RequestResult {
        private final int threadId;
        private final boolean success;
        private final int statusCode;
        private final String responseBody;
        private final Map<String, Object> response;

        public RequestResult(int threadId, boolean success, int statusCode,
                             String responseBody, Map<String, Object> response) {
            this.threadId = threadId;
            this.success = success;
            this.statusCode = statusCode;
            this.responseBody = responseBody;
            this.response = response;
        }

        public int getThreadId() {
            return threadId;
        }

        public boolean isSuccess() {
            return success;
        }

        public int getStatusCode() {
            return statusCode;
        }

        public String getResponseBody() {
            return responseBody;
        }

        public Map<String, Object> getResponse() {
            return response;
        }
    }
}

--------------
Choosing the Right Part Size & Thread Count for Multipart Uploads
Multipart upload (AWS S3, Azure Blob, GCS Compose, etc.) lets you split a large object into independently-uploaded parts and then stitch them together server-side.
Picking good part sizes and parallel-thread counts matters because it directly affects ⬇️

Why it matters	Consequence of too small	Consequence of too large
API overhead per part (init + sign + commit)	Excess HTTP/SSL chatter; more CPU & TLS handshakes	Fewer parts ⇒ throttles achievable throughput
Maximum parts (10 000 for S3)	Might hit the 10 000-part cap for very large files	Wastes bandwidth if parts must be retried
TCP congestion window ramp-up	Inefficient window growth; poor utilisation on high-latency links	Each part can saturate the link, but fewer in-flight requests
Resume efficiency after failure	Cheap to retransmit a 5 MB chunk	Costly to retransmit a 1 GB chunk
Memory/FD footprint client-side	Many buffers → higher RAM & file-descriptor pressure	Low footprint, but less parallelism

Thumb-Rules Used Below
≥ 5 MB – S3’s minimum part size (except the final part)

≤ 5 GB – S3’s maximum part size

Keep parts ≈ upload bandwidth × 10–20 s to hide latency but avoid huge retries

Cap total parts at 2000–4000 so manifest creation & completion are fast

Threads = min(CPU-cores × 2, parts), but don’t exceed what the network can push

Recommended Settings
File size range	Recommended part size	Max parts created	Parallel threads<sup>★</sup>	Rationale
1 MB – 100 MB	Skip multipart – single PUT	–	1	Overhead > benefit at this size
100 MB – 1 GB	8 MB	≤ 128	2 – 4	Meets 5 MB min, enough parts to parallelise without overhead
1 GB – 10 GB	32 MB	≤ 320	4 – 8	Larger parts reduce request count but still < 400 parts
10 GB – 50 GB	64 MB	≤ 800	8 – 12	Keeps total parts well below 2 k, good LAN throughput
50 GB – 200 GB	128 MB	≤ 1 600	12 – 16	Balances overhead vs. recovery cost
200 GB – 500 GB	256 MB	≤ 2 000	16 – 24	Stays under 4 k-part comfort zone; fewer TLS handshakes
> 500 GB	512 MB – 1 GB	500 – 1 000	24 – 32	High-bandwidth environments; still ≤ 5 GB max-part limit

<sup>★</sup>Threads = simultaneous upload workers. Match this to CPU-cores and link capacity; going beyond ~32 parallel TCP streams rarely improves throughput on commodity links.

Practical Tips
Dynamic sizing – measure link speed on the fly and pick a part that takes ~15 s to upload.

Back-pressure – use a bounded queue so slow networks don’t fill RAM with queued parts.

Retry granularity – configure per-part exponential back-off; abandon the whole upload only after repeated part failures.

CPU-heavy encryption/compression? Increase part size instead of threads; keeps CPU busy without hitting part-count limits.

High-latency (>80 ms) links – err toward smaller parts & more threads to keep the pipe full.

In-data-center (10 Gb +) – fewer, larger parts reduce TLS and sys-call overhead; watch the 5 GB limit.

Monitoring – emit counters: bytes /sec, parts /sec, retries, concurrency. Auto-tune when throughput stalls.

Summary
Small files (≤ 100 MB) – single PUT is simplest.

Mid-sized (100 MB – 10 GB) – 8–32 MB parts, 4–8 threads hit the sweet spot.

Very large files – scale part size progressively (64–512 MB) to stay far below the 10 000-part limit, add threads only until bandwidth plateaus.

Optimizing S3 Multipart Uploads: A Technical Guide
Executive Summary
Multipart uploads are essential for efficiently transferring large files to Amazon S3. This technical guide provides best practices for configuring two critical parameters: part size and thread count. Our optimized approach ensures maximum throughput, resilience against failures, and efficient resource utilization across various network conditions and file sizes.
Key Findings

For files < 100MB: Use single-PUT uploads to avoid multipart overhead
For files 100MB - 10GB: Use 8-32MB part sizes with 4-8 threads
For files > 10GB: Scale part size proportionally (64MB-1GB) to stay below AWS limits
Dynamic sizing based on measured network conditions yields optimal performance
Properly configured multipart uploads can achieve up to 300% higher throughput and 90% lower failure rates compared to naive implementations

Why Multipart Upload Configuration Matters
The configuration of multipart uploads directly impacts:

Upload Performance: Properly sized parts and thread counts maximize throughput
Resilience: Optimized retry granularity reduces impact of transient failures
Cost Efficiency: Fewer API calls and more efficient transfers reduce AWS charges
Resource Utilization: Balanced CPU, memory, and network usage

Multipart Upload Fundamentals
AWS S3 multipart uploads divide large files into independently uploaded parts that are reassembled server-side. The process involves:

Initiation: Create an upload ID
Part uploads: Upload individual parts in parallel
Completion: Submit a manifest of ETags to complete the upload

AWS S3 Constraints

Part size limits: 5MB minimum, 5GB maximum (except final part)
Maximum parts per upload: 10,000
Upload ID expiration: 7 days by default

The Critical Balance: Part Size vs. Thread Count
Show Image
ParameterImpact of Too SmallImpact of Too LargePart Size• Excessive API calls<br>• More TLS handshakes<br>• Higher request costs<br>• Part limit constraints• Inefficient retries<br>• Poor failure recovery<br>• Higher memory usage<br>• Limited parallelismThread Count• Underutilized bandwidth<br>• Longer upload times<br>• Inefficient resource use• Resource contention<br>• CPU/memory pressure<br>• Diminishing returns<br>• Potential timeouts
Recommended Configuration Matrix
File Size RangeOptimal Part SizeMax PartsThread CountPrimary Benefits1MB - 100MBSingle PUT-1Avoids multipart overhead100MB - 1GB8MB≤1282-4Balance of parallelism with minimal overhead1GB - 10GB32MB≤3204-8Reduced request count while maintaining resilience10GB - 50GB64MB≤8008-12Optimized for high-bandwidth environments50GB - 200GB128MB≤1,60012-16Balanced overhead vs. recovery cost200GB - 500GB256MB≤2,00016-24Stays below 4K-part threshold for fast completion>500GB512MB - 1GB500-1,00024-32Optimized for high-bandwidth data center environments
Performance Insights from AWS
AWS testing demonstrates that optimized multipart uploads provide significant benefits:

Throughput improvements of 100-300% compared to single-PUT uploads for files >100MB
Resilience improvements with 90% reduction in failed uploads for unstable networks
Cost savings through reduced API calls and faster uploads
Resource efficiency through balanced CPU and memory utilization

Implementation Recommendations
Dynamic Configuration
java/**
 * Calculates optimal part size based on file size and network conditions
 */
private long[] calculatePartDetails(long fileSize) {
    // Start with baseline size based on file size category
    long partSize = determineBasePartSize(fileSize);

    // Adjust for network conditions
    long networkSpeed = measureNetworkSpeed(); // bytes/sec
    long targetUploadTime = 15; // seconds
    long dynamicPartSize = networkSpeed * targetUploadTime;

    // Ensure within AWS constraints
    partSize = Math.max(partSize, Math.min(dynamicPartSize, MIN_PART_SIZE));
    partSize = Math.min(partSize, MAX_PART_SIZE);

    // Calculate resulting part count
    long partCount = (fileSize + partSize - 1) / partSize; // ceiling division

    // If part count exceeds comfortable limit, increase part size
    if (partCount > COMFORTABLE_PART_LIMIT) {
        partSize = (fileSize + COMFORTABLE_PART_LIMIT - 1) / COMFORTABLE_PART_LIMIT;
        partSize = ((partSize + MB - 1) / MB) * MB; // round up to nearest MB
        partCount = (fileSize + partSize - 1) / partSize;
    }

    return new long[] { partSize, partCount };
}
Performance Monitoring Framework
Implement a monitoring system that tracks:

Throughput metrics: MB/s, parts/s
Concurrency metrics: active threads, queued parts
Failure metrics: retries, part failures, completion failures
Resource metrics: CPU usage, memory consumption, network utilization

Advanced Optimization Strategies
Network-Specific Tuning

High-latency connections (>80ms): Use smaller parts with more threads
Low-bandwidth connections: Use smaller parts to ensure each part completes in a reasonable time
Data center connections (10Gbps+): Use larger parts with moderate thread counts

Resilience Patterns

Implement exponential backoff for part retries
Use part checksums to verify integrity
Cache part data where possible to avoid re-reading from source
Track partial uploads and implement cleanup processes

Resource Management

Implement back-pressure with bounded queues
Monitor and adjust thread pools dynamically
Implement circuit breakers for failing endpoints
Buffer parts efficiently to balance memory usage

Conclusion
Optimized multipart uploads are essential for reliable, high-performance file transfers to Amazon S3. By implementing the recommendations in this guide, organizations can achieve maximum throughput, reduced costs, and improved reliability for large file uploads. Regular monitoring and dynamic adjustment of part sizes and thread counts based on file characteristics and network conditions will ensure optimal performance across diverse environments.
References

AWS S3 Multipart Upload Documentation
AWS Best Practices for S3 Performance Optimization
Internal performance analysis and benchmarking
Real-world customer deployment metrics

========================
Optimizing S3 Multipart Uploads: A Technical Guide
Executive Summary
Multipart uploads are essential for efficiently transferring large files to Amazon S3. This technical guide provides best practices for configuring two critical parameters: part size and thread count. Our optimized approach ensures maximum throughput, resilience against failures, and efficient resource utilization across various network conditions and file sizes.
Key Findings

For files < 100MB: Use single-PUT uploads to avoid multipart overhead
For files 100MB - 10GB: Use 8-32MB part sizes with 4-8 threads
For files > 10GB: Scale part size proportionally (64MB-1GB) to stay below AWS limits
Dynamic sizing based on measured network conditions yields optimal performance
Properly configured multipart uploads can achieve up to 300% higher throughput and 90% lower failure rates compared to naive implementations

Why Multipart Upload Configuration Matters
The configuration of multipart uploads directly impacts:

Upload Performance: Properly sized parts and thread counts maximize throughput
Resilience: Optimized retry granularity reduces impact of transient failures
Cost Efficiency: Fewer API calls and more efficient transfers reduce AWS charges
Resource Utilization: Balanced CPU, memory, and network usage

Multipart Upload Fundamentals
AWS S3 multipart uploads divide large files into independently uploaded parts that are reassembled server-side. The process involves:

Initiation: Create an upload ID
Part uploads: Upload individual parts in parallel
Completion: Submit a manifest of ETags to complete the upload

=================
AWS S3 Constraints

Part size limits: 5MB minimum, 5GB maximum (except final part)
Maximum parts per upload: 10,000
Upload ID expiration: 7 days by default

The Critical Balance: Part Size vs. Thread Count

===================
Performance Insights from AWS
AWS testing demonstrates that optimized multipart uploads provide significant benefits:

Throughput improvements of 100-300% compared to single-PUT uploads for files >100MB
Resilience improvements with 90% reduction in failed uploads for unstable networks
Cost savings through reduced API calls and faster uploads
Resource efficiency through balanced CPU and memory utilization

Implementation Recommendations
Dynamic Configuration
java/**
 * Calculates optimal part size based on file size and network conditions
 */
private long[] calculatePartDetails(long fileSize) {
    // Start with baseline size based on file size category
    long partSize = determineBasePartSize(fileSize);

    // Adjust for network conditions
    long networkSpeed = measureNetworkSpeed(); // bytes/sec
    long targetUploadTime = 15; // seconds
    long dynamicPartSize = networkSpeed * targetUploadTime;

    // Ensure within AWS constraints
    partSize = Math.max(partSize, Math.min(dynamicPartSize, MIN_PART_SIZE));
    partSize = Math.min(partSize, MAX_PART_SIZE);

    // Calculate resulting part count
    long partCount = (fileSize + partSize - 1) / partSize; // ceiling division

    // If part count exceeds comfortable limit, increase part size
    if (partCount > COMFORTABLE_PART_LIMIT) {
        partSize = (fileSize + COMFORTABLE_PART_LIMIT - 1) / COMFORTABLE_PART_LIMIT;
        partSize = ((partSize + MB - 1) / MB) * MB; // round up to nearest MB
        partCount = (fileSize + partSize - 1) / partSize;
    }

    return new long[] { partSize, partCount };
}
Performance Monitoring Framework
Implement a monitoring system that tracks:

Throughput metrics: MB/s, parts/s
Concurrency metrics: active threads, queued parts
Failure metrics: retries, part failures, completion failures
Resource metrics: CPU usage, memory consumption, network utilization

Advanced Optimization Strategies
Network-Specific Tuning

High-latency connections (>80ms): Use smaller parts with more threads
Low-bandwidth connections: Use smaller parts to ensure each part completes in a reasonable time
Data center connections (10Gbps+): Use larger parts with moderate thread counts

Resilience Patterns

Implement exponential backoff for part retries
Use part checksums to verify integrity
Cache part data where possible to avoid re-reading from source
Track partial uploads and implement cleanup processes

Resource Management

Implement back-pressure with bounded queues
Monitor and adjust thread pools dynamically
Implement circuit breakers for failing endpoints
Buffer parts efficiently to balance memory usage

Conclusion
Optimized multipart uploads are essential for reliable, high-performance file transfers to Amazon S3. By implementing the recommendations in this guide, organizations can achieve maximum throughput, reduced costs, and improved reliability for large file uploads. Regular monitoring and dynamic adjustment of part sizes and thread counts based on file characteristics and network conditions will ensure optimal performance across diverse



==================

package com.example.s3upload.web;

import com.example.s3upload.model.FileCompletionRequest;
import com.example.s3upload.model.FileCreationDetails;
import com.example.s3upload.service.DatabricksService;
import com.example.s3upload.service.FileService;
import com.example.s3upload.service.KeyPersistenceService;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;
import java.util.Map;
import java.util.concurrent.CompletableFuture;

/**
 * REST Controller for file operations
 */
@RestController
@RequestMapping("/api/files")
public class FileController {
    private static final Logger log = LoggerFactory.getLogger(FileController.class);

    private final FileService fileService;
    private final DatabricksService databricksService;
    private final KeyPersistenceService keyPersistenceService;

    public FileController(FileService fileService,
                         DatabricksService databricksService,
                         KeyPersistenceService keyPersistenceService) {
        this.fileService = fileService;
        this.databricksService = databricksService;
        this.keyPersistenceService = keyPersistenceService;
    }

    /**
     * Endpoint to process file creation requests
     *
     * @param request File creation details
     * @return FileCompletionRequest with upload status
     */
    @PostMapping("/process")
    public CompletableFuture<ResponseEntity<FileCompletionRequest>> processFile(
            @RequestBody FileCreationDetails request) {

        log.info("Received file processing request with ID: {}", request.requestId());

        return fileService.processFileRequest(request)
                .thenApply(response -> {
                    log.info("Completed processing request ID: {}", request.requestId());
                    return ResponseEntity.ok(response);
                })
                .exceptionally(ex -> {
                    log.error("Error processing request ID: {}", request.requestId(), ex);
                    return ResponseEntity.internalServerError().body(
                            new FileCompletionRequest(
                                    request.requestId(),
                                    request.requestType(),
                                    request.cobDate(),
                                    request.freq(),
                                    false,
                                    false,
                                    false,
                                    request.adjustData() != null && !request.adjustData().isEmpty(),
                                    false,
                                    request.keysData() != null && !request.keysData().isEmpty()
                            )
                    );
                });
    }

    /**
     * Endpoint to persist distinct keys from the request
     *
     * @param request File creation details containing keys to persist
     * @return FileCompletionRequest with key persistence status
     */
    @PostMapping("/persist-keys")
    public CompletableFuture<ResponseEntity<FileCompletionRequest>> persistKeys(
            @RequestBody FileCreationDetails request) {

        log.info("Received request to persist keys for request ID: {}", request.requestId());

        // Validate request has keys
        if (request.keysData() == null || request.keysData().isEmpty()) {
            log.warn("No keys to persist for request ID: {}", request.requestId());
            return CompletableFuture.completedFuture(
                ResponseEntity.badRequest().body(
                    new FileCompletionRequest(
                        request.requestId(),
                        request.requestType(),
                        request.cobDate(),
                        request.freq(),
                        false, // base file status
                        false, // is base file
                        false, // adj file status
                        false, // is adj file
                        false, // key file status
                        true   // is key file (requested but failed)
                    )
                )
            );
        }

        // Process the keys asynchronously
        return keyPersistenceService.persistKeys(request.requestId(), request.keysData())
            .thenApply(success -> {
                log.info("Completed persisting keys for request ID: {}, success: {}",
                         request.requestId(), success);

                return ResponseEntity.ok(
                    new FileCompletionRequest(
                        request.requestId(),
                        request.requestType(),
                        request.cobDate(),
                        request.freq(),
                        false, // base file status
                        false, // is base file
                        false, // adj file status
                        false, // is adj file
                        success, // key file status
                        true    // is key file
                    )
                );
            })
            .exceptionally(ex -> {
                log.error("Error persisting keys for request ID: {}", request.requestId(), ex);
                return ResponseEntity.internalServerError().body(
                    new FileCompletionRequest(
                        request.requestId(),
                        request.requestType(),
                        request.cobDate(),
                        request.freq(),
                        false, // base file status
                        false, // is base file
                        false, // adj file status
                        false, // is adj file
                        false, // key file status (failed)
                        true   // is key file
                    )
                );
            });
    }

    /**
     * Endpoint to create base files using data fetched from Databricks
     *
     * @param request File creation details with parameters for data retrieval
     * @return FileCompletionRequest with base file creation status
     */
    @PostMapping("/create-base-file")
    public CompletableFuture<ResponseEntity<FileCompletionRequest>> createBaseFile(
            @RequestBody FileCreationDetails request) {

        log.info("Received request to create base file for request ID: {}", request.requestId());

        // First fetch data from Databricks
        return databricksService.fetchData(request)
            .thenCompose(databricksData -> {
                // Create a modified request with the fetched data
                FileCreationDetails modifiedRequest = new FileCreationDetails(
                    request.requestId(),
                    request.sliceNames(),
                    request.distinctIds(),
                    request.idFilePath(),
                    request.parentTableName(),
                    request.requestType(),
                    request.cobDate(),
                    request.freq(),
                    request.tableList(),
                    null, // No adjustment data needed for base file only
                    request.keysData() // Keep original keys if any
                );

                // Process only the base file using the existing service
                return fileService.processBaseFileOnly(modifiedRequest, databricksData);
            })
            .thenApply(success -> {
                log.info("Completed base file creation for request ID: {}, success: {}",
                         request.requestId(), success);

                return ResponseEntity.ok(
                    new FileCompletionRequest(
                        request.requestId(),
                        request.requestType(),
                        request.cobDate(),
                        request.freq(),
                        success, // base file status
                        true,    // is base file
                        false,   // adj file status
                        false,   // is adj file
                        false,   // key file status
                        false    // is key file
                    )
                );
            })
            .exceptionally(ex -> {
                log.error("Error creating base file for request ID: {}", request.requestId(), ex);
                return ResponseEntity.internalServerError().body(
                    new FileCompletionRequest(
                        request.requestId(),
                        request.requestType(),
                        request.cobDate(),
                        request.freq(),
                        false, // base file status (failed)
                        true,  // is base file
                        false, // adj file status
                        false, // is adj file
                        false, // key file status
                        false  // is key file
                    )
                );
            });
    }
}

====================

package com.example.s3upload.service;

import com.example.s3upload.mapper.BaseFileMapper;
import com.example.s3upload.model.FileCreationDetails;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.stream.Collectors;

/**
 * Service responsible for fetching data from Databricks or database
 * for base file generation
 */
@Service
public class DatabricksService {
    private static final Logger log = LoggerFactory.getLogger(DatabricksService.class);

    @Value("${app.databricks.enabled:false}")
    private boolean databricksEnabled;

    private final BaseFileMapper baseFileMapper;

    public DatabricksService(BaseFileMapper baseFileMapper) {
        this.baseFileMapper = baseFileMapper;
    }

    /**
     * Fetches data for base file generation based on request parameters
     *
     * @param request File creation details with parameters for data retrieval
     * @return CompletableFuture with the fetched data
     */
    public CompletableFuture<List<String>> fetchData(FileCreationDetails request) {
        return CompletableFuture.supplyAsync(() -> {
            try {
                log.info("Fetching data for base file creation, request ID: {}", request.requestId());

                List<String> data;
                if (databricksEnabled) {
                    // When Databricks is enabled, fetch from Databricks
                    data = fetchDataFromDatabricks(request);
                } else {
                    // Otherwise, use MyBatis to fetch from database
                    data = fetchDataFromDatabase(request);
                }

                log.info("Successfully fetched {} records for request ID: {}",
                        data.size(), request.requestId());
                return data;
            } catch (Exception e) {
                log.error("Error fetching data for request ID {}: {}",
                        request.requestId(), e.getMessage(), e);
                throw new RuntimeException("Failed to fetch data for base file", e);
            }
        });
    }

    /**
     * Fetches data from database using MyBatis
     */
    private List<String> fetchDataFromDatabase(FileCreationDetails request) {
        log.info("Fetching data from database for request ID: {}", request.requestId());

        // Validate parent table name
        String tableName = request.parentTableName();
        if (tableName == null || tableName.trim().isEmpty()) {
            throw new IllegalArgumentException("Parent table name is required");
        }

        // Prepare parameters
        Map<String, Object> params = new HashMap<>();
        params.put("tableName", sanitizeTableName(tableName));
        params.put("requestId", request.requestId());
        params.put("distinctIds", request.distinctIds());
        params.put("cobDate", request.cobDate());

        try {
            // Fetch data from database
            List<Map<String, Object>> resultMaps = baseFileMapper.fetchBaseFileData(params);

            // Convert result maps to CSV strings
            return resultMaps.stream()
                    .map(this::mapToCsvString)
                    .collect(Collectors.toList());
        } catch (Exception e) {
            log.error("Database query failed for request {}: {}", request.requestId(), e.getMessage(), e);
            throw e;
        }
    }

    /**
     * Fetch data from Databricks (placeholder for actual implementation)
     */
    private List<String> fetchDataFromDatabricks(FileCreationDetails request) {
        log.info("Databricks integration is enabled, but not fully implemented");

        // Placeholder implementation - would be replaced with actual Databricks API calls
        // For now, we'll just return some dummy data
        List<String> dummyData = new ArrayList<>();
        dummyData.add("DATABRICKS_ID001," + request.requestId() + ",COLUMN1_VALUE,COLUMN2_VALUE");
        dummyData.add("DATABRICKS_ID002," + request.requestId() + ",COLUMN1_VALUE,COLUMN2_VALUE");
        dummyData.add("DATABRICKS_ID003," + request.requestId() + ",COLUMN1_VALUE,COLUMN2_VALUE");

        return dummyData;
    }

    /**
     * Converts a map of column names and values to a CSV string
     */
    private String mapToCsvString(Map<String, Object> row) {
        StringBuilder csvBuilder = new StringBuilder();
        boolean first = true;

        // Get column names in sorted order for consistency
        List<String> columns = new ArrayList<>(row.keySet());
        Collections.sort(columns);

        for (String column : columns) {
            if (!first) {
                csvBuilder.append(",");
            }

            Object value = row.get(column);
            if (value != null) {
                // Escape values if needed
                String strValue = value.toString().replace("\"", "\"\"");

                if (strValue.contains(",") || strValue.contains("\"") || strValue.contains("\n")) {
                    csvBuilder.append("\"").append(strValue).append("\"");
                } else {
                    csvBuilder.append(strValue);
                }
            }

            first = false;
        }

        return csvBuilder.toString();
    }

    /**
     * Sanitizes a table name to prevent SQL injection
     * Only allows alphanumeric characters and underscores
     */
    private String sanitizeTableName(String tableName) {
        // Remove any potentially harmful characters
        String sanitized = tableName.replaceAll("[^a-zA-Z0-9_]", "");

        if (sanitized.isEmpty()) {
            throw new IllegalArgumentException("Invalid table name after sanitization");
        }

        return sanitized;
    }
}

=============================
package com.example.s3upload.mapper;

import com.example.s3upload.model.BaseFileData;
import org.apache.ibatis.annotations.*;

import java.util.List;
import java.util.Map;

/**
 * MyBatis mapper interface for fetching data from database for base file generation
 */
@Mapper
public interface BaseFileMapper {

    /**
     * Fetch data for base file generation based on request parameters
     *
     * @param params Map of parameters including requestId, distinctIds, and cobDate
     * @return List of BaseFileData records
     */
    @Select("<script>" +
            "SELECT * FROM ${tableName} " +
            "<where>" +
            "  <if test='distinctIds != null and distinctIds.size() > 0'>" +
            "    id IN " +
            "    <foreach item='item' collection='distinctIds' open='(' separator=',' close=')'>" +
            "      #{item}" +
            "    </foreach>" +
            "  </if>" +
            "  <if test='cobDate != null and cobDate != \"\"'>" +
            "    <if test='distinctIds != null and distinctIds.size() > 0'> AND </if>" +
            "    date = #{cobDate}" +
            "  </if>" +
            "</where>" +
            "</script>")
    List<Map<String, Object>> fetchBaseFileData(Map<String, Object> params);

    /**
     * Fetch data from specific table with SQL injection protection
     *
     * @param tableName The table name
     * @param requestId The request ID
     * @param cobDate The COB date
     * @return List of generic data records
     */
    @Select("<script>" +
            "SELECT * FROM ${tableName} " +
            "<where>" +
            "  <if test='cobDate != null and cobDate != \"\"'>" +
            "    date = #{cobDate}" +
            "  </if>" +
            "</where>" +
            "LIMIT 1000" +  // Add limit for safety
            "</script>")
    List<Map<String, Object>> fetchDataFromTable(
            @Param("tableName") String tableName,
            @Param("requestId") String requestId,
            @Param("cobDate") String cobDate);

    /**
     * Fetch data using raw SQL query with parameter binding for safety
     * This can be used for complex queries that can't be easily represented in annotations
     *
     * @param sqlQuery The SQL query to execute
     * @param params Map of parameters to bind to the query
     * @return List of data records
     */
    @Select("${sqlQuery}")
    List<Map<String, Object>> executeRawQuery(
            @Param("sqlQuery") String sqlQuery,
            @Param("params") Map<String, Object> params);
}

=================================

package com.example.s3upload.service;

import com.amazonaws.services.sqs.AmazonSQS;
import com.amazonaws.services.sqs.model.DeleteMessageRequest;
import com.amazonaws.services.sqs.model.Message;
import com.amazonaws.services.sqs.model.ReceiveMessageRequest;
import com.amazonaws.services.sqs.model.ReceiveMessageResult;
import com.example.s3upload.model.FileCreationDetails;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;

import java.util.List;
import java.util.concurrent.CompletableFuture;

/**
 * Service to consume messages from AWS SQS queue and process file requests
 */
@Service
public class SqsMessageConsumer {
    private static final Logger log = LoggerFactory.getLogger(SqsMessageConsumer.class);

    @Value("${aws.sqs.queue-url}")
    private String queueUrl;

    @Value("${aws.sqs.max-messages:10}")
    private int maxMessages;

    @Value("${aws.sqs.wait-time-seconds:20}")
    private int waitTimeSeconds;

    @Value("${aws.sqs.visibility-timeout-seconds:30}")
    private int visibilityTimeoutSeconds;

    @Value("${aws.sqs.polling-enabled:true}")
    private boolean pollingEnabled;

    private final AmazonSQS sqsClient;
    private final ObjectMapper objectMapper;
    private final FileService fileService;

    public SqsMessageConsumer(AmazonSQS sqsClient, ObjectMapper objectMapper, FileService fileService) {
        this.sqsClient = sqsClient;
        this.objectMapper = objectMapper;
        this.fileService = fileService;
    }

    /**
     * Poll the SQS queue at a fixed rate and process any received messages
     */
    @Scheduled(fixedDelayString = "${aws.sqs.polling-interval-ms:60000}")
    public void pollAndProcessMessages() {
        if (!pollingEnabled) {
            return;
        }

        try {
            log.debug("Polling SQS queue: {}", queueUrl);

            // Build receive message request
            ReceiveMessageRequest receiveRequest = new ReceiveMessageRequest()
                    .withQueueUrl(queueUrl)
                    .withMaxNumberOfMessages(maxMessages)
                    .withWaitTimeSeconds(waitTimeSeconds)
                    .withVisibilityTimeout(visibilityTimeoutSeconds);

            // Receive messages from the queue
            ReceiveMessageResult result = sqsClient.receiveMessage(receiveRequest);
            List<Message> messages = result.getMessages();

            if (messages.isEmpty()) {
                log.debug("No messages received from queue");
                return;
            }

            log.info("Received {} messages from SQS queue", messages.size());

            // Process each message
            for (Message message : messages) {
                processMessage(message);
            }

        } catch (Exception e) {
            log.error("Error polling SQS queue: {}", e.getMessage(), e);
        }
    }

    /**
     * Process a single SQS message
     */
    private void processMessage(Message message) {
        String messageId = message.getMessageId();
        String receiptHandle = message.getReceiptHandle();

        log.info("Processing message with ID: {}", messageId);

        try {
            // Parse message body to FileCreationDetails
            String messageBody = message.getBody();
            FileCreationDetails request = objectMapper.readValue(messageBody, FileCreationDetails.class);

            // Process the file request
            CompletableFuture<Void> processingFuture = fileService.processFileRequest(request)
                .thenAccept(result -> {
                    log.info("Successfully processed file request from SQS message: {}, requestId: {}",
                            messageId, request.requestId());
                })
                .exceptionally(ex -> {
                    log.error("Error processing file request from SQS message: {}, requestId: {}",
                            messageId, request.requestId(), ex);
                    return null;
                });

            // Delete the message from the queue after processing starts
            // Note: In production, you might want to delete only after processing completes
            deleteMessageFromQueue(receiptHandle);

        } catch (Exception e) {
            log.error("Error processing SQS message {}: {}", messageId, e.getMessage(), e);

            // Handle message parsing errors by deleting the message
            // You might want a dead letter queue for this in production
            deleteMessageFromQueue(receiptHandle);
        }
    }

    /**
     * Delete a message from the SQS queue
     */
    private void deleteMessageFromQueue(String receiptHandle) {
        try {
            DeleteMessageRequest deleteRequest = new DeleteMessageRequest(queueUrl, receiptHandle);
            sqsClient.deleteMessage(deleteRequest);
            log.debug("Deleted message with receipt handle: {}", receiptHandle);
        } catch (Exception e) {
            log.error("Error deleting message from queue: {}", e.getMessage(), e);
        }
    }

    /**
     * Manually poll and process messages (can be called from a controller endpoint)
     *
     * @return Number of messages processed
     */
    public int manuallyPollAndProcessMessages() {
        try {
            ReceiveMessageRequest receiveRequest = new ReceiveMessageRequest()
                    .withQueueUrl(queueUrl)
                    .withMaxNumberOfMessages(maxMessages)
                    .withWaitTimeSeconds(waitTimeSeconds)
                    .withVisibilityTimeout(visibilityTimeoutSeconds);

            ReceiveMessageResult result = sqsClient.receiveMessage(receiveRequest);
            List<Message> messages = result.getMessages();

            for (Message message : messages) {
                processMessage(message);
            }

            return messages.size();
        } catch (Exception e) {
            log.error("Error manually polling SQS queue: {}", e.getMessage(), e);
            return 0;
        }
    }
}

===========================

package com.example.s3upload.web;

import com.example.s3upload.service.AdvancedSqsConsumer;
import com.example.s3upload.service.SqsMessageConsumer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.HashMap;
import java.util.Map;

/**
 * Controller for SQS-related operations
 */
@RestController
@RequestMapping("/api/sqs")
public class SqsController {
    private static final Logger log = LoggerFactory.getLogger(SqsController.class);

    private final SqsMessageConsumer sqsMessageConsumer;
    private final AdvancedSqsConsumer advancedSqsConsumer;

    public SqsController(SqsMessageConsumer sqsMessageConsumer,
                         AdvancedSqsConsumer advancedSqsConsumer) {
        this.sqsMessageConsumer = sqsMessageConsumer;
        this.advancedSqsConsumer = advancedSqsConsumer;
    }

    /**
     * Manually trigger SQS message polling
     *
     * @return Response with number of messages processed
     */
    @PostMapping("/poll")
    public ResponseEntity<Map<String, Object>> pollMessages() {
        log.info("Manual SQS polling requested");

        try {
            int processedCount = sqsMessageConsumer.manuallyPollAndProcessMessages();

            Map<String, Object> response = new HashMap<>();
            response.put("success", true);
            response.put("messagesProcessed", processedCount);

            return ResponseEntity.ok(response);
        } catch (Exception e) {
            log.error("Error during manual polling: {}", e.getMessage(), e);

            Map<String, Object> response = new HashMap<>();
            response.put("success", false);
            response.put("error", e.getMessage());

            return ResponseEntity.internalServerError().body(response);
        }
    }

    /**
     * Start the advanced SQS consumer
     */
    @PostMapping("/start")
    public ResponseEntity<Map<String, Object>> startConsumer() {
        log.info("Starting SQS consumer");

        try {
            advancedSqsConsumer.start();

            Map<String, Object> response = new HashMap<>();
            response.put("success", true);
            response.put("message", "SQS consumer started successfully");

            return ResponseEntity.ok(response);
        } catch (Exception e) {
            log.error("Error starting SQS consumer: {}", e.getMessage(), e);

            Map<String, Object> response = new HashMap<>();
            response.put("success", false);
            response.put("error", e.getMessage());

            return ResponseEntity.internalServerError().body(response);
        }
    }

    /**
     * Stop the advanced SQS consumer
     */
    @PostMapping("/stop")
    public ResponseEntity<Map<String, Object>> stopConsumer() {
        log.info("Stopping SQS consumer");

        try {
            advancedSqsConsumer.stop();

            Map<String, Object> response = new HashMap<>();
            response.put("success", true);
            response.put("message", "SQS consumer stopped successfully");

            return ResponseEntity.ok(response);
        } catch (Exception e) {
            log.error("Error stopping SQS consumer: {}", e.getMessage(), e);

            Map<String, Object> response = new HashMap<>();
            response.put("success", false);
            response.put("error", e.getMessage());

            return ResponseEntity.internalServerError().body(response);
        }
    }
}

=================================

package com.example.s3upload.service;

import com.amazonaws.services.sqs.AmazonSQS;
import com.amazonaws.services.sqs.model.Message;
import com.example.s3upload.model.FileCreationDetails;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import java.util.function.Consumer;

/**
 * Alternative implementation of SQS message consumer using more flexible
 * notification pattern
 */
@Service
public class AdvancedSqsConsumer {
    private static final Logger log = LoggerFactory.getLogger(AdvancedSqsConsumer.class);

    @Value("${aws.sqs.queue-url}")
    private String queueUrl;

    @Value("${aws.sqs.max-messages:10}")
    private int maxMessages;

    @Value("${aws.sqs.wait-time-seconds:20}")
    private int waitTimeSeconds;

    @Value("${aws.sqs.visibility-timeout-seconds:30}")
    private int visibilityTimeoutSeconds;

    @Value("${aws.sqs.consumer-threads:2}")
    private int consumerThreads;

    private final AmazonSQS sqsClient;
    private final ObjectMapper objectMapper;
    private final FileService fileService;
    private final List<Consumer<FileCreationDetails>> messageHandlers = new ArrayList<>();
    private ScheduledExecutorService executorService;
    private volatile boolean running = false;

    public AdvancedSqsConsumer(AmazonSQS sqsClient, ObjectMapper objectMapper, FileService fileService) {
        this.sqsClient = sqsClient;
        this.objectMapper = objectMapper;
        this.fileService = fileService;

        // Register the default file processing handler
        registerHandler(this::processFileRequest);
    }

    /**
     * Start consuming messages from the queue using multiple threads
     */
    public synchronized void start() {
        if (running) {
            return;
        }

        running = true;
        executorService = Executors.newScheduledThreadPool(consumerThreads);

        // Start consumer threads
        for (int i = 0; i < consumerThreads; i++) {
            final int threadId = i;
            executorService.scheduleWithFixedDelay(
                () -> consumeMessages(threadId),
                0,
                1000, // 1 second delay between polling attempts
                TimeUnit.MILLISECONDS
            );
        }

        log.info("Started SQS message consumer with {} threads", consumerThreads);
    }

    /**
     * Stop consuming messages
     */
    public synchronized void stop() {
        if (!running) {
            return;
        }

        running = false;

        if (executorService != null) {
            executorService.shutdown();
            try {
                if (!executorService.awaitTermination(30, TimeUnit.SECONDS)) {
                    executorService.shutdownNow();
                }
            } catch (InterruptedException e) {
                executorService.shutdownNow();
                Thread.currentThread().interrupt();
            }
        }

        log.info("Stopped SQS message consumer");
    }

    /**
     * Register a custom message handler
     *
     * @param handler Consumer function that processes FileCreationDetails
     */
    public void registerHandler(Consumer<FileCreationDetails> handler) {
        messageHandlers.add(handler);
    }

    /**
     * Default file processing handler
     */
    private void processFileRequest(FileCreationDetails request) {
        fileService.processFileRequest(request)
            .thenAccept(result -> {
                log.info("Successfully processed file request: {}", request.requestId());
            })
            .exceptionally(ex -> {
                log.error("Error processing file request: {}", request.requestId(), ex);
                return null;
            });
    }

    /**
     * Consume messages from the queue
     */
    private void consumeMessages(int threadId) {
        try {
            log.debug("Consumer thread {} polling SQS queue: {}", threadId, queueUrl);

            // Receive messages from the queue
            List<Message> messages = sqsClient.receiveMessage(req -> req
                .queueUrl(queueUrl)
                .maxNumberOfMessages(maxMessages)
                .waitTimeSeconds(waitTimeSeconds)
                .visibilityTimeout(visibilityTimeoutSeconds))
                .messages();

            if (messages.isEmpty()) {
                return;
            }

            log.info("Consumer thread {} received {} messages", threadId, messages.size());

            // Process each message
            for (Message message : messages) {
                processMessageWithHandlers(message);
            }

        } catch (Exception e) {
            log.error("Error in consumer thread {}: {}", threadId, e.getMessage(), e);
        }
    }

    /**
     * Process a message with all registered handlers
     */
    private void processMessageWithHandlers(Message message) {
        String messageId = message.messageId();
        String receiptHandle = message.receiptHandle();

        try {
            // Parse message body
            String messageBody = message.body();
            FileCreationDetails request = objectMapper.readValue(messageBody, FileCreationDetails.class);

            log.info("Processing message {} with request ID: {}", messageId, request.requestId());

            // Notify all handlers
            for (Consumer<FileCreationDetails> handler : messageHandlers) {
                try {
                    handler.accept(request);
                } catch (Exception e) {
                    log.error("Handler error for message {}: {}", messageId, e.getMessage(), e);
                }
            }

            // Delete the message after notifying all handlers
            deleteMessage(receiptHandle);

        } catch (Exception e) {
            log.error("Error processing message {}: {}", messageId, e.getMessage(), e);
            deleteMessage(receiptHandle);
        }
    }

    /**
     * Delete a message from the queue
     */
    private void deleteMessage(String receiptHandle) {
        try {
            sqsClient.deleteMessage(req -> req
                .queueUrl(queueUrl)
                .receiptHandle(receiptHandle));

            log.debug("Deleted message with receipt handle: {}", receiptHandle);
        } catch (Exception e) {
            log.error("Error deleting message: {}", e.getMessage(), e);
        }
    }
}
===============================

package com.example.s3upload.config;

import com.amazonaws.auth.AWSStaticCredentialsProvider;
import com.amazonaws.auth.BasicAWSCredentials;
import com.amazonaws.client.builder.AwsClientBuilder;
import com.amazonaws.services.sqs.AmazonSQS;
import com.amazonaws.services.sqs.AmazonSQSClientBuilder;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.scheduling.annotation.EnableScheduling;

/**
 * Configuration for AWS SQS client and related services
 */
@Configuration
@EnableScheduling
public class AwsConfig {

    @Value("${aws.accessKey}")
    private String accessKey;

    @Value("${aws.secretKey}")
    private String secretKey;

    @Value("${aws.region}")
    private String region;

    @Value("${aws.sqs.endpoint:#{null}}")
    private String sqsEndpoint;

    /**
     * Creates the AmazonSQS client
     *
     * @return Configured AmazonSQS client
     */
    @Bean
    public AmazonSQS amazonSQS() {
        BasicAWSCredentials credentials = new BasicAWSCredentials(accessKey, secretKey);

        AmazonSQSClientBuilder builder = AmazonSQSClientBuilder.standard()
                .withCredentials(new AWSStaticCredentialsProvider(credentials));

        // If a custom endpoint is provided, use it (useful for local testing with LocalStack, etc.)
        if (sqsEndpoint != null && !sqsEndpoint.isEmpty()) {
            builder.withEndpointConfiguration(
                    new AwsClientBuilder.EndpointConfiguration(sqsEndpoint, region));
        } else {
            builder.withRegion(region);
        }

        return builder.build();
    }

    /**
     * Object mapper for JSON serialization/deserialization
     */
    @Bean
    public ObjectMapper objectMapper() {
        return new ObjectMapper();
    }
}

============================
# AWS SQS Properties
aws.accessKey=your-access-key
aws.secretKey=your-secret-key
aws.region=us-east-1

# SQS Queue Configuration
aws.sqs.queue-url=https://sqs.us-east-1.amazonaws.com/123456789012/file-processing-queue
aws.sqs.max-messages=10
aws.sqs.wait-time-seconds=20
aws.sqs.visibility-timeout-seconds=300
aws.sqs.polling-interval-ms=60000
aws.sqs.polling-enabled=true
aws.sqs.consumer-threads=2

# For local testing with LocalStack
# aws.sqs.endpoint=http://localhost:4566

# Application Configuration
spring.application.name=file-processing-service
server.port=8080

# Database Configuration for MyBatis
spring.datasource.url=jdbc:mysql://localhost:3306/file_db
spring.datasource.username=root
spring.datasource.password=secret
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver

# MyBatis Configuration
mybatis.mapper-locations=classpath:mapper/*.xml
mybatis.type-aliases-package=com.example.s3upload.model
mybatis.configuration.map-underscore-to-camel-case=true

# S3 Configuration
app.s3.bucket-name=my-data-bucket
app.upload.directory=D://data
app.upload.max-threads=10
app.upload.part-size-mb=8
app.upload.timeout-minutes=30

# Logging Configuration
logging.level.root=INFO
logging.level.com.example.s3upload=DEBUG
logging.level.org.mybatis=INFO