import logging
from dataclasses import dataclass
from typing import List, Dict, Optional

import yaml
from pyspark.sql import SparkSession, DataFrame

# ----------------------------------------------------------------
# Configure Logging
# ----------------------------------------------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ----------------------------------------------------------------
# Configuration Data Classes
# ----------------------------------------------------------------
@dataclass
class SourceConfig:
    """Configuration for data source."""
    table_name: Optional[str] = None
    query: Optional[str] = None

@dataclass
class WriteConfig:
    """Configuration for data writing."""
    path: str
    format: str = 'parquet'
    mode: str = 'overwrite'
    partition_by: Optional[List[str]] = None
    options: Dict[str, str] = None

# ----------------------------------------------------------------
# Configuration Reader
# ----------------------------------------------------------------
class ConfigReader:
    """Reads and validates pipeline configuration from YAML."""
    def __init__(self, config_input: str):
        """
        Args:
            config_input (str): YAML configuration as a string.
        """
        self.config = yaml.safe_load(config_input)
        self._validate_config()

    def _validate_config(self):
        if not isinstance(self.config, dict):
            raise ValueError("Configuration must be a dictionary")

        required_fields = ['tables', 's3_settings', 'databricks_settings']
        missing = [field for field in required_fields if field not in self.config]
        if missing:
            raise ValueError(f"Missing required fields: {missing}")

    def get_table_config(self, table_name: str) -> Dict:
        """Retrieve a specific table's configuration."""
        return self.config['tables'].get(table_name, {})

    def get_s3_config(self) -> Dict:
        """Retrieve S3 configuration (e.g., base path)."""
        return self.config['s3_settings']

    def get_databricks_config(self) -> Dict:
        """
        Retrieve Databricks connection details,
        including host and token.
        """
        return self.config['databricks_settings']

# ----------------------------------------------------------------
# Databricks Connector
# ----------------------------------------------------------------
class DatabricksConnector:
    """
    Handles creation of Spark session to connect with Databricks
    using a token-based approach.
    """

    def __init__(self, databricks_config: Dict):
        """
        Args:
            databricks_config (Dict): A dictionary containing
                                      'host' and 'token'.
        """
        self.host = databricks_config.get('host')
        self.token = databricks_config.get('token')

        if not self.host or not self.token:
            raise ValueError("Databricks 'host' and 'token' must be provided.")

        self._spark = None  # Will hold the SparkSession instance

    def get_spark_session(self) -> SparkSession:
        """
        Returns a SparkSession configured for Databricks access.
        """
        if self._spark is None:
            logger.info("Initializing SparkSession with Databricks token-based auth...")
            # Note: The actual config keys may vary depending on environment or runtime version.
            self._spark = (
                SparkSession.builder
                .appName("DatabricksTokenApp")
                # Example setting for token-based auth; adapt for your environment.
                .config("spark.databricks.service.address", self.host)
                .config("spark.databricks.service.token", self.token)
                .getOrCreate()
            )
        return self._spark

# ----------------------------------------------------------------
# Data Reader
# ----------------------------------------------------------------
class DataReader:
    """
    Handles reading data from Databricks (via Spark).
    """

    def __init__(self, spark: SparkSession):
        """
        Args:
            spark (SparkSession): A pre-configured Spark session
                                  for Databricks.
        """
        self.spark = spark

    def read_data(self, source_config: SourceConfig) -> DataFrame:
        """
        Reads data from a Databricks table or via a SQL query.

        Args:
            source_config (SourceConfig): Contains table name or query.

        Returns:
            DataFrame: Spark DataFrame with the requested data.
        """
        if source_config.query:
            logger.info(f"Reading data using SQL query: {source_config.query}")
            return self.spark.sql(source_config.query)
        elif source_config.table_name:
            logger.info(f"Reading data from table: {source_config.table_name}")
            return self.spark.table(source_config.table_name)
        else:
            raise ValueError("Either 'query' or 'table_name' must be provided.")

# ----------------------------------------------------------------
# Data Writer
# ----------------------------------------------------------------
class DataWriter:
    """
    Handles writing data to S3 (or other storage).
    """

    def __init__(self, s3_config: Dict):
        """
        Args:
            s3_config (Dict): Must at least contain 'base_path' for S3.
        """
        self.s3_config = s3_config

        if 'base_path' not in self.s3_config:
            raise ValueError("S3 config must contain 'base_path'")

    def write_data(self, df: DataFrame, write_config: WriteConfig) -> None:
        """
        Writes the DataFrame to a specified location in S3.

        Args:
            df (DataFrame): Data to be written.
            write_config (WriteConfig): Output path, format, mode, etc.
        """
        base_path = self.s3_config['base_path']
        full_path = f"{base_path}/{write_config.path}"

        writer = df.write.mode(write_config.mode)

        if write_config.format == 'delta':
            # Delta-specific options
            writer = writer.option("mergeSchema", "true")
            writer = writer.option("optimizeWrite", "true")

        if write_config.partition_by:
            writer = writer.partitionBy(write_config.partition_by)

        if write_config.options:
            for key, value in write_config.options.items():
                writer = writer.option(key, value)

        logger.info(f"Writing data to: {full_path} with format={write_config.format}")
        writer.format(write_config.format).save(full_path)

# ----------------------------------------------------------------
# Data Processor
# ----------------------------------------------------------------
class DataProcessor:
    """
    Main orchestrator class that holds:
      - ConfigReader
      - Spark session (via DatabricksConnector)
      - DataReader & DataWriter
    """

    def __init__(self, config_reader: ConfigReader):
        """
        Args:
            config_reader (ConfigReader): Contains pipeline config.
        """
        self.config_reader = config_reader

        # Initialize Spark session via DatabricksConnector
        databricks_conf = self.config_reader.get_databricks_config()
        self.connector = DatabricksConnector(databricks_conf)
        spark = self.connector.get_spark_session()

        # Create Reader & Writer
        self.reader = DataReader(spark)
        self.writer = DataWriter(self.config_reader.get_s3_config())

    def process_table(self, table_name: str) -> str:
        """
        Process a single table: read data -> (optionally transform) -> write out.
        """
        logger.info(f"Processing table: {table_name}")

        # Get table config
        table_config = self.config_reader.get_table_config(table_name)
        if not table_config:
            raise ValueError(f"No configuration found for table: {table_name}")

        # Build SourceConfig & WriteConfig
        source_config = SourceConfig(**table_config.get('source', {}))
        write_config = WriteConfig(
            path=table_config['s3_path'],
            format=table_config.get('format', 'parquet'),
            mode=table_config.get('mode', 'overwrite'),
            partition_by=table_config.get('partition_by'),
            options=table_config.get('options', {})
        )

        # Read & Write
        df = self.reader.read_data(source_config)
        # Any transformations can go here (currently omitted).
        self.writer.write_data(df, write_config)

        logger.info(f"Successfully processed table: {table_name}")
        return "Success"

# ----------------------------------------------------------------
# Batch Processor
# ----------------------------------------------------------------
class BatchProcessor:
    """
    Handles batch processing of multiple tables in a single pipeline run.
    """

    def __init__(self, processor: DataProcessor):
        """
        Args:
            processor (DataProcessor): Orchestrator containing
                                      reader & writer logic.
        """
        self.processor = processor

    def process_tables(self, table_names: List[str]) -> Dict[str, str]:
        """
        Process multiple tables and return a summary of outcomes.

        Args:
            table_names (List[str]): List of table names to process.

        Returns:
            Dict[str, str]: Mapping of table name -> "Success"/"Failed: Reason"
        """
        results = {}

        for table_name in table_names:
            try:
                logger.info(f"Batch processing table: {table_name}")
                result = self.processor.process_table(table_name)
                results[table_name] = result
            except Exception as e:
                logger.error(f"Error processing table {table_name}: {e}")
                results[table_name] = f"Failed: {str(e)}"

        return results



tables:
  my_table:
    source:
      table_name: "database.my_table"
    s3_path: "output/my_table"
    format: "parquet"
    mode: "overwrite"
    partition_by:
      - "date_col"
    options:
      compression: "snappy"
  another_table:
    source:
      query: "SELECT * FROM database.another_table WHERE event_date = '2024-12-24'"
    s3_path: "output/another_table"
    format: "delta"
    mode: "append"
s3_settings:
  base_path: "s3://my-bucket"
databricks_settings:
  host: "https://my-workspace.databricks.com"
  token: "my-secret-token"



if __name__ == "__main__":
    # Load config (could be from a file, here it's just a string)
    with open("config.yaml", "r") as f:
        config_str = f.read()

    config_reader = ConfigReader(config_str)
    data_processor = DataProcessor(config_reader)
    batch_processor = BatchProcessor(data_processor)

    # List of tables to process
    tables_to_run = ["my_table", "another_table"]
    results = batch_processor.process_tables(tables_to_run)
    print(results)

