from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, explode, flatten
from pyspark.sql.types import ArrayType
from typing import List, Dict, Optional
from dataclasses import dataclass
import yaml
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class SourceConfig:
    """Configuration for data source"""
    table_name: Optional[str] = None
    query: Optional[str] = None

@dataclass
class WriteConfig:
    """Configuration for data writing"""
    path: str
    format: str = 'parquet'
    mode: str = 'overwrite'
    partition_by: Optional[List[str]] = None
    options: Dict[str, str] = None

class ConfigReader:
    """Configuration reader with validation"""
    def __init__(self, config_input: str, is_file: bool = False):
        self.config = self._load_config(config_input, is_file)

    def _load_config(self, config_input: str, is_file: bool) -> Dict:
        try:
            if is_file:
                with open(config_input, 'r') as file:
                    config = yaml.safe_load(file)
            else:
                config = yaml.safe_load(config_input)

            self._validate_config(config)
            return config
        except Exception as e:
            raise ValueError(f"Error loading configuration: {str(e)}")

    def _validate_config(self, config: Dict):
        if not isinstance(config, dict):
            raise ValueError("Configuration must be a dictionary")

        required_fields = ['tables', 's3_settings']
        if not all(field in config for field in required_fields):
            raise ValueError(f"Missing required fields: {required_fields}")

        if 'base_path' not in config['s3_settings']:
            raise ValueError("'base_path' is required in s3_settings")

    def get_table_config(self, table_name: str) -> Dict:
        return self.config['tables'].get(table_name, {})

    def get_s3_config(self) -> Dict:
        return self.config['s3_settings']

class DataReader:
    """Handles reading data from Spark"""
    def __init__(self, spark: SparkSession):
        self.spark = spark

    def read_data(self, source_config: SourceConfig) -> DataFrame:
        if source_config.query:
            logger.info(f"Reading data using SQL query")
            return self.spark.sql(source_config.query)
        elif source_config.table_name:
            logger.info(f"Reading data from table: {source_config.table_name}")
            return self.spark.table(source_config.table_name)
        else:
            raise ValueError("Either query or table_name must be provided")

class DataWriter:
    """Handles writing data to S3"""
    def __init__(self, s3_config: Dict):
        self.s3_config = s3_config

    def write_data(self, df: DataFrame, write_config: WriteConfig) -> None:
        base_path = self.s3_config['base_path']
        full_path = f"{base_path}/{write_config.path}"

        writer = df.write.mode(write_config.mode)

        # Configure writer
        if write_config.partition_by:
            writer = writer.partitionBy(write_config.partition_by)

        if write_config.options:
            for key, value in write_config.options.items():
                writer = writer.option(key, value)

        logger.info(f"Writing data to: {full_path}")
        writer.format(write_config.format).save(full_path)

class DataFlattener:
    """Handles flattening of nested structures"""
    def flatten_data(self, df: DataFrame, columns: List[str]) -> DataFrame:
        if not columns:
            return df

        result_df = df
        for column in columns:
            if column not in result_df.columns:
                logger.warning(f"Column '{column}' not found in data, skipping")
                continue

            field = [f for f in result_df.schema.fields if f.name == column][0]
            if isinstance(field.dataType, ArrayType):
                if isinstance(field.dataType.elementType, ArrayType):
                    logger.info(f"Flattening nested array: {column}")
                    result_df = result_df.withColumn(column, flatten(col(column)))
                logger.info(f"Exploding array: {column}")
                result_df = result_df.withColumn(column, explode(col(column)))

        return result_df

class BatchProcessor:
    """Handles batch processing of multiple tables"""
    def __init__(self, processor: 'DataProcessor'):
        self.processor = processor

    def process_tables(self, table_names: List[str]) -> Dict[str, str]:
        """Process multiple tables"""
        results = {}

        for table in table_names:
            try:
                logger.info(f"Processing table: {table}")

                # Get table configuration
                table_config = self.processor.config_reader.get_table_config(table)
                if not table_config:
                    raise ValueError(f"No configuration found for table: {table}")

                # Create configurations
                source_config = SourceConfig(**table_config.get('source', {}))
                write_config = WriteConfig(
                    path=table_config['s3_path'],
                    format=table_config.get('format', 'parquet'),
                    mode=table_config.get('mode', 'overwrite'),
                    partition_by=table_config.get('partition_by'),
                    options=table_config.get('options', {})
                )

                # Process data
                df = self.processor.reader.read_data(source_config)

                if table_config.get('flatten_columns'):
                    df = self.processor.flattener.flatten_data(df, table_config['flatten_columns'])

                self.processor.writer.write_data(df, write_config)

                results[table] = "Success"
                logger.info(f"Successfully processed table: {table}")

            except Exception as e:
                results[table] = f"Failed: {str(e)}"
                logger.error(f"Error processing table {table}: {str(e)}")

        return results

class DataProcessor:
    """Main processor class"""
    def __init__(self, config_reader: ConfigReader):
        self.config_reader = config_reader
        self.reader = DataReader(SparkSession.active)
        self.writer = DataWriter(config_reader.get_s3_config())
        self.flattener = DataFlattener()

class ProcessorFactory:
    """Factory for creating processor instances"""
    @staticmethod
    def create_processor(config_input: str, is_file: bool = False) -> tuple[DataProcessor, BatchProcessor]:
        config_reader = ConfigReader(config_input, is_file)
        processor = DataProcessor(config_reader)
        batch_processor = BatchProcessor(processor)
        return processor, batch_processor

# Example usage
def process_example():
    # Configuration
    config = """
    tables:
      customer_orders:
        source:
          table_name: orders
        flatten_columns:
          - items
        s3_path: processed/orders
        format: delta
        partition_by:
          - date

      product_inventory:
        source:
          query: SELECT * FROM inventory WHERE active = true
        flatten_columns:
          - locations
        s3_path: processed/inventory
        format: parquet
        partition_by:
          - category

    s3_settings:
      base_path: s3://your-bucket
    """

    # Create processors
    processor, batch_processor = ProcessorFactory.create_processor(config)

    # Process multiple tables
    tables = ['customer_orders', 'product_inventory']
    results = batch_processor.process_tables(tables)

    # Print results
    for table, status in results.items():
        print(f"Table: {table} - Status: {status}")

# Use in Databricks notebook
# process_example()