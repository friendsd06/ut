package com.example.s3upload.config;

import jakarta.annotation.PreDestroy;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;
import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider;
import software.amazon.awssdk.auth.credentials.StaticCredentialsProvider;
import software.amazon.awssdk.http.async.SdkAsyncHttpClient;
import software.amazon.awssdk.http.nio.netty.NettyNioAsyncHttpClient;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3AsyncClient;
import software.amazon.awssdk.services.s3.S3Configuration;
import java.net.URI;
import java.time.Duration;

/**
 * Configuration for S3 clients
 * Creates both real AWS and LocalStack clients
 */
@Configuration
public class S3ClientConfig {
    private static final Logger log = LoggerFactory.getLogger(S3ClientConfig.class);

    private SdkAsyncHttpClient httpClient;
    private S3AsyncClient localstackS3Client;
    private S3AsyncClient awsS3Client;

    @Value("${app.localstack.url:http://localhost:4566}")
    private String localstackUrl;

    @Value("${app.aws.region:us-east-1}")
    private String region;

    @Value("${app.aws.connection.timeout:30}")
    private int connectionTimeoutSeconds;

    /**
     * Create and configure HTTP client for S3
     *
     * @return Configured SDK Async HTTP Client
     */
    private SdkAsyncHttpClient createHttpClient() {
        if (httpClient == null) {
            httpClient = NettyNioAsyncHttpClient.builder()
                    .connectionTimeout(Duration.ofSeconds(connectionTimeoutSeconds))
                    .connectionAcquisitionTimeout(Duration.ofMinutes(1))
                    .build();
            log.info("Created S3 HTTP client with {}s connection timeout", connectionTimeoutSeconds);
        }
        return httpClient;
    }

    /**
     * Create and configure S3 service configuration
     *
     * @return S3 configuration
     */
    private S3Configuration s3Config() {
        return S3Configuration.builder()
                .pathStyleAccessEnabled(true)  // For LocalStack & easier testing
                .checksumValidationEnabled(true)  // Validate data integrity
                .build();
    }

    /**
     * S3AsyncClient for LocalStack (for testing)
     *
     * @return S3AsyncClient configured for LocalStack
     */
    @Bean("localstackS3")
    @Primary
    public S3AsyncClient localstackClient() {
        log.info("Creating LocalStack S3 client with endpoint: {}", localstackUrl);
        localstackS3Client = S3AsyncClient.builder()
                .credentialsProvider(StaticCredentialsProvider.create(
                        AwsBasicCredentials.create("test", "test")))
                .endpointOverride(URI.create(localstackUrl))
                .region(Region.of(region))
                .httpClient(createHttpClient())
                .serviceConfiguration(s3Config())
                .build();
        return localstackS3Client;
    }

    /**
     * Primary S3AsyncClient for real AWS
     * Uses default credentials chain
     *
     * @return S3AsyncClient configured for AWS
     */
    @Bean("awsS3")
    public S3AsyncClient awsClient() {
        log.info("Creating AWS S3 client for region: {}", region);
        awsS3Client = S3AsyncClient.builder()
                .credentialsProvider(DefaultCredentialsProvider.create())
                .region(Region.of(region))
                .httpClient(createHttpClient())
                .serviceConfiguration(s3Config())
                .build();
        return awsS3Client;
    }

    /**
     * Clean up resources on shutdown
     */
    @PreDestroy
    public void cleanUp() {
        log.info("Shutting down S3 clients and resources");
        try {
            if (localstackS3Client != null) {
                localstackS3Client.close();
                log.debug("Closed LocalStack S3 client");
            }

            if (awsS3Client != null) {
                awsS3Client.close();
                log.debug("Closed AWS S3 client");
            }

            if (httpClient != null) {
                httpClient.close();
                log.debug("Closed HTTP client");
            }
        } catch (Exception e) {
            log.warn("Error during S3 client cleanup", e);
        }
    }
}
-------------------
package com.example.s3upload.model;

public record FileCompletionRequest(
        String requestId,
        String requestType,
        String cobDate,
        String freq,
        Boolean baseFileStatus,
        Boolean isBaseFile,
        Boolean adjFileStatus,
        Boolean isAdjFile,
        Boolean keyFileStatus,
        Boolean isKeyFile
) {}

--------------------
package com.example.s3upload.model;

import java.util.List;
import java.util.Map;

public record FileCreationDetails(
        String requestId,
        List<String> sliceNames,
        List<String> distinctIds,
        String idFilePath,
        String parentTableName,
        String requestType,
        String cobDate,
        String freq,
        List<String> tableList,
        Map<String, List<String>> adjustData,
        List<String> keysData
) {}
------------------------
package com.example.s3upload.service;

import com.example.s3upload.model.FileCompletionRequest;
import com.example.s3upload.model.FileCreationDetails;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicBoolean;

/**
 * Service responsible for processing file creation requests and managing
 * uploads to S3 storage
 */
@Service
public class FileService {

    private static final Logger log = LoggerFactory.getLogger(FileService.class);
    private static final long FILE_SIZE_THRESHOLD_MB = 10;
    private static final long FILE_SIZE_THRESHOLD_BYTES = FILE_SIZE_THRESHOLD_MB * 1024 * 1024;
    private static final int UPLOAD_TIMEOUT_MINUTES = 30;

    @Value("${app.upload.directory:D://data}")
    private String baseDirectoryPath;

    @Value("${app.s3.bucket-name}")
    private String bucketName;

    @Value("${spring.profiles.active:dev}")
    private String activeProfile;

    private final FileUploadService fileUploadService;
    private final ExecutorService fileProcessingExecutor;

    /**
     * Constructor with dependency injection
     *
     * @param fileUploadService Service for uploading files to S3
     */
    public FileService(FileUploadService fileUploadService) {
        this.fileUploadService = fileUploadService;
        this.fileProcessingExecutor = Executors.newFixedThreadPool(
                Runtime.getRuntime().availableProcessors());
    }

    /**
     * Initialize the service after construction
     */
    @PostConstruct
    public void initialize() {
        try {
            createDirectoryStructure();
            log.info("Directory structure created successfully at {}", baseDirectoryPath);
        } catch (IOException e) {
            log.error("Failed to create directory structure: {}", e.getMessage(), e);
        }
    }

    /**
     * Clean up resources when the service is being destroyed
     */
    @PreDestroy
    public void cleanup() {
        fileProcessingExecutor.shutdown();
        try {
            if (!fileProcessingExecutor.awaitTermination(1, TimeUnit.MINUTES)) {
                fileProcessingExecutor.shutdownNow();
            }
        } catch (InterruptedException e) {
            fileProcessingExecutor.shutdownNow();
            Thread.currentThread().interrupt();
        }
    }

    /**
     * Creates the required directory structure for file operations
     */
    private void createDirectoryStructure() throws IOException {
        // Create base directory
        Files.createDirectories(Paths.get(baseDirectoryPath));

        // Get execution environment
        String env = getExecutionEnvironment();

        // Create all required directories
        String basePath = baseDirectoryPath + "/abinitio/" + env + "/adjustment";
        Files.createDirectories(Paths.get(basePath));
        Files.createDirectories(Paths.get(basePath + "/base"));
        Files.createDirectories(Paths.get(basePath + "/adjusted"));
        Files.createDirectories(Paths.get(basePath + "/adjusted/keyvalues"));
    }

    /**
     * Process file creation request and upload files to S3
     *
     * @param request File creation details
     * @return CompletableFuture with FileCompletionRequest containing upload status
     */
    public CompletableFuture<FileCompletionRequest> processFileRequest(FileCreationDetails request) {
        log.info("Processing file request with ID: {}", request.requestId());

        // Determine which files need to be created
        boolean isKeyFile = request.keysData() != null;
        boolean isAdjFile = request.adjustData() != null && !request.adjustData().isEmpty();
        boolean isBaseFile = true; // Always create a base file for every request

        return CompletableFuture.supplyAsync(() -> {
            try {
                // Create and track results for each file type
                AtomicBoolean keyFileSuccess = new AtomicBoolean(!isKeyFile); // true if not required
                AtomicBoolean adjFileSuccess = new AtomicBoolean(!isAdjFile); // true if not required
                AtomicBoolean baseFileSuccess = new AtomicBoolean(false);

                // Process key file if required
                if (isKeyFile) {
                    boolean success = processKeyFile(request);
                    keyFileSuccess.set(success);
                    log.info("Key file processing result for request {}: {}",
                            request.requestId(), success ? "SUCCESS" : "FAILED");
                }

                // Process adjusted files if required
                if (isAdjFile) {
                    boolean success = processAdjustedFiles(request);
                    adjFileSuccess.set(success);
                    log.info("Adjusted files processing result for request {}: {}",
                            request.requestId(), success ? "SUCCESS" : "FAILED");
                }

                // Process base file (always)
                boolean success = processBaseFile(request);
                baseFileSuccess.set(success);
                log.info("Base file processing result for request {}: {}",
                        request.requestId(), success ? "SUCCESS" : "FAILED");

                // Create and return response
                return createCompletionResponse(
                        request,
                        baseFileSuccess.get(),
                        isBaseFile,
                        adjFileSuccess.get(),
                        isAdjFile,
                        keyFileSuccess.get(),
                        isKeyFile
                );

            } catch (Exception e) {
                log.error("Error processing file request {}: {}", request.requestId(), e.getMessage(), e);
                return createErrorResponse(request, isBaseFile, isAdjFile, isKeyFile);
            }
        }, fileProcessingExecutor);
    }

    /**
     * Creates a successful completion response
     */
    private FileCompletionRequest createCompletionResponse(
            FileCreationDetails request,
            boolean baseFileSuccess,
            boolean isBaseFile,
            boolean adjFileSuccess,
            boolean isAdjFile,
            boolean keyFileSuccess,
            boolean isKeyFile) {

        return new FileCompletionRequest(
                request.requestId(),
                request.requestType(),
                request.cobDate(),
                request.freq(),
                baseFileSuccess,
                isBaseFile,
                adjFileSuccess,
                isAdjFile,
                keyFileSuccess,
                isKeyFile
        );
    }

    /**
     * Creates an error response when file processing fails
     */
    private FileCompletionRequest createErrorResponse(
            FileCreationDetails request,
            boolean isBaseFile,
            boolean isAdjFile,
            boolean isKeyFile) {

        return new FileCompletionRequest(
                request.requestId(),
                request.requestType(),
                request.cobDate(),
                request.freq(),
                false,
                isBaseFile,
                false,
                isAdjFile,
                false,
                isKeyFile
        );
    }

    /**
     * Process key file - create and upload to S3
     * Creates a zero-byte file if keysData is empty
     *
     * @param request File creation details
     * @return true if successful, false otherwise
     */
    private boolean processKeyFile(FileCreationDetails request) {
        try {
            log.info("Creating key file for request: {}", request.requestId());

            // Create file paths
            String localFilePath = baseDirectoryPath + "/logicRcrdID_" + request.requestId() + ".dat";
            String s3ObjectKey = buildS3ObjectKey(request.requestId(), "adjusted/keyvalues/keys.dat");

            log.info("Key file local path: {}", localFilePath);
            log.info("Key file S3 object key: {}", s3ObjectKey);

            // Create and write to file
            Path filePath = createLocalFile(localFilePath, request.keysData());

            // Upload file to S3
            return uploadFileToS3(request.requestId(), "key file", filePath, s3ObjectKey);

        } catch (Exception e) {
            log.error("Failed to create or upload key file for request {}: {}",
                    request.requestId(), e.getMessage(), e);
            return false;
        }
    }

    /**
     * Process all adjusted files - create and upload to S3
     * Creates zero-byte files if entity data is empty
     *
     * @param request File creation details
     * @return true if all files uploaded successfully, false if any fail
     */
    private boolean processAdjustedFiles(FileCreationDetails request) {
        if (request.adjustData() == null || request.adjustData().isEmpty()) {
            log.info("No adjust data to process for request: {}", request.requestId());
            return true;
        }

        List<CompletableFuture<Boolean>> futures = new ArrayList<>();

        // Process each entity file in parallel
        for (Map.Entry<String, List<String>> entry : request.adjustData().entrySet()) {
            String entityName = entry.getKey();
            List<String> data = entry.getValue();

            CompletableFuture<Boolean> future = CompletableFuture.supplyAsync(() ->
                    processAdjustedFile(request, entityName, data), fileProcessingExecutor);

            futures.add(future);
        }

        // Wait for all futures to complete
        try {
            CompletableFuture<Void> allDone = CompletableFuture.allOf(
                    futures.toArray(new CompletableFuture[0]));
            allDone.get(UPLOAD_TIMEOUT_MINUTES, TimeUnit.MINUTES);

            // Check if any failed
            return futures.stream().allMatch(f -> {
                try {
                    return f.get();
                } catch (Exception e) {
                    return false;
                }
            });

        } catch (Exception e) {
            log.error("Error waiting for adjusted file processing: {}", e.getMessage(), e);
            return false;
        }
    }

    /**
     * Process a single adjusted file
     */
    private boolean processAdjustedFile(FileCreationDetails request, String entityName, List<String> data) {
        try {
            log.info("Creating adjusted file for entity: {} (request ID: {})",
                    entityName, request.requestId());

            // Clean entity name
            String cleanEntityName = entityName.toLowerCase().replace("_vue", "");

            // Create file paths
            String localFilePath = baseDirectoryPath + "/" + cleanEntityName + "_adjusted_" + request.requestId() + ".dat";
            String s3ObjectKey = buildS3ObjectKey(request.requestId(), "adjusted/" + cleanEntityName + ".dat");

            log.info("Adjusted file local path for {}: {}", entityName, localFilePath);
            log.info("Adjusted file S3 object key for {}: {}", entityName, s3ObjectKey);

            // Create and write to file
            Path filePath = createLocalFile(localFilePath, data);

            // Upload file to S3
            return uploadFileToS3(request.requestId(), "adjusted file for entity " + entityName, filePath, s3ObjectKey);

        } catch (Exception e) {
            log.error("Failed to create or upload adjusted file for entity {} (request {}): {}",
                    entityName, request.requestId(), e.getMessage(), e);
            return false;
        }
    }

    /**
     * Process base file - create and upload to S3
     *
     * @param request File creation details
     * @return true if successful, false otherwise
     */
    private boolean processBaseFile(FileCreationDetails request) {
        try {
            log.info("Creating base file for request: {}", request.requestId());

            // Create file paths
            String localFilePath = baseDirectoryPath + "/" + request.requestId() + ".dat";
            String s3ObjectKey = buildS3ObjectKey(request.requestId(), "base/data.dat");

            log.info("Base file local path: {}", localFilePath);
            log.info("Base file S3 object key: {}", s3ObjectKey);

            // Generate sample data for base file
            List<String> sampleRecords = generateSampleBaseFileData(request);

            // Create and write to file
            Path filePath = createLocalFile(localFilePath, sampleRecords);

            // Upload file to S3
            return uploadFileToS3(request.requestId(), "base file", filePath, s3ObjectKey);

        } catch (Exception e) {
            log.error("Failed to create or upload base file for request {}: {}",
                    request.requestId(), e.getMessage(), e);
            return false;
        }
    }

    /**
     * Create a local file and write data to it
     *
     * @param filePath Path to create the file at
     * @param data Data to write (if null or empty, creates a zero-byte file)
     * @return Path object for the created file
     */
    private Path createLocalFile(String filePath, List<String> data) throws IOException {
        // Ensure directory exists
        Path directory = Paths.get(filePath).getParent();
        if (directory != null) {
            Files.createDirectories(directory);
        }

        // Create the file
        Path path = Paths.get(filePath);

        if (data == null || data.isEmpty()) {
            // Create empty (zero-byte) file if no data
            log.info("Creating zero-byte file as data is empty: {}", filePath);
            Files.write(path, new byte[0],
                    StandardOpenOption.CREATE,
                    StandardOpenOption.TRUNCATE_EXISTING);
        } else {
            // Normal case - write data to file
            String content = String.join(System.lineSeparator(), data);
            Files.write(path, content.getBytes(),
                    StandardOpenOption.CREATE,
                    StandardOpenOption.TRUNCATE_EXISTING);
        }

        log.info("File created: {}", filePath);
        return path;
    }

    /**
     * Upload a file to S3
     *
     * @param requestId Request ID for logging
     * @param fileDescription Description of file for logging
     * @param filePath Local file path
     * @param s3ObjectKey S3 object key
     * @return true if successful, false otherwise
     */
    private boolean uploadFileToS3(String requestId, String fileDescription, Path filePath, String s3ObjectKey) {
        // Simulate failure for specific request IDs
        if (requestId.equals("REQ1") || requestId.equals("REQ2") || requestId.equals("REQ8")) {
            log.error("Simulated failure for {} upload with request ID: {}", fileDescription, requestId);
            return false;
        }
        try {
            // Get file size
            long fileSize = Files.size(filePath);

            // Choose upload method based on file size
            if (fileSize < FILE_SIZE_THRESHOLD_BYTES) {
                fileUploadService.uploadSimpleFile(bucketName, s3ObjectKey, filePath, fileSize);
            } else {
                fileUploadService.uploadFileParallel(bucketName, s3ObjectKey, filePath);
            }

            log.info("{} uploaded successfully for request: {}",
                    capitalizeFirstLetter(fileDescription), requestId);
            return true;
        } catch (Exception e) {
            log.error("Failed to upload {} for request {}: {}",
                    fileDescription, requestId, e.getMessage(), e);
            return false;
        } finally {
            // Optionally clean up local file - uncomment if needed
            // try {
            //     Files.deleteIfExists(filePath);
            //     log.debug("Deleted local file: {}", filePath);
            // } catch (IOException e) {
            //     log.warn("Failed to delete local file {}: {}", filePath, e.getMessage());
            // }
        }
    }

    /**
     * Build the S3 object key for a file
     *
     * @param requestId Request ID
     * @param objectPath Path portion of the object key
     * @return Complete S3 object key
     */
    private String buildS3ObjectKey(String requestId, String objectPath) {
        String env = getExecutionEnvironment();
        return "data/abinitio/" + env + "/adjustment/" + requestId + "/" + objectPath;
    }

    /**
     * Generate sample data for base file (5 records)
     * In a real implementation, this would fetch data from a database
     *
     * @param request File creation details
     * @return List of sample data records
     */
    private List<String> generateSampleBaseFileData(FileCreationDetails request) {
        List<String> records = new ArrayList<>();

        // Get current timestamp for the records
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));

        // Add 5 sample records
        records.add("ID001," + request.requestId() + ",FIELD1_VALUE," + timestamp + ",ACTIVE");
        records.add("ID002," + request.requestId() + ",FIELD2_VALUE," + timestamp + ",INACTIVE");
        records.add("ID003," + request.requestId() + ",FIELD3_VALUE," + timestamp + ",PENDING");
        records.add("ID004," + request.requestId() + ",FIELD4_VALUE," + timestamp + ",ACTIVE");
        records.add("ID005," + request.requestId() + ",FIELD5_VALUE," + timestamp + ",COMPLETE");

        return records;
    }

    /**
     * Get execution environment based on active profile
     *
     * @return Execution environment string
     */
    private String getExecutionEnvironment() {
        if (activeProfile == null) {
            return "dev";
        }

        switch (activeProfile.toLowerCase()) {
            case "test":
            case "uat":
                return "uat";
            case "preprod":
                return "pre";
            case "prod":
                return "prd";
            default:
                return "dev";
        }
    }

    /**
     * Utility method to capitalize the first letter of a string
     */
    private String capitalizeFirstLetter(String input) {
        if (input == null || input.isEmpty()) {
            return input;
        }
        return input.substring(0, 1).toUpperCase() + input.substring(1);
    }
}-----------------

package com.example.s3upload.service;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;
import software.amazon.awssdk.core.async.AsyncRequestBody;
import software.amazon.awssdk.services.s3.S3AsyncClient;
import software.amazon.awssdk.services.s3.model.*;

import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.file.Path;
import java.nio.file.StandardOpenOption;
import java.time.Duration;
import java.time.Instant;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;
import java.util.concurrent.*;
import java.util.stream.Collectors;

/**
 * Service responsible for uploading files to S3 using both simple and multipart upload strategies.
 * Provides optimized parallel multipart uploads for large files.
 */
@Service
public class FileUploadService {
    private static final Logger log = LoggerFactory.getLogger(FileUploadService.class);

    // S3 multipart upload constraints
    private static final long MIN_PART_SIZE_BYTES = 5 * 1024 * 1024; // 5MB
    private static final int MAX_PARTS = 10000;
    private static final int DEFAULT_MAX_RETRIES = 3;
    private static final long MB = 1024 * 1024;

    // Timeouts (in minutes)
    private static final int INITIATE_UPLOAD_TIMEOUT = 2;
    private static final int COMPLETE_UPLOAD_TIMEOUT = 5;
    private static final int ABORT_UPLOAD_TIMEOUT = 2;
    private static final int THREAD_POOL_SHUTDOWN_TIMEOUT_SECONDS = 30;

    @Value("${app.upload.max-threads:10}")
    private int maxUploadThreads;

    @Value("${app.upload.part-size-mb:5}")
    private int partSizeMb;

    @Value("${app.upload.timeout-minutes:30}")
    private int uploadTimeoutMinutes;

    private final S3AsyncClient s3Client;
    private ExecutorService uploadThreadPool;

    /**
     * Constructor with dependency injection for the S3 client.
     *
     * @param s3Client Configured S3 async client for AWS or LocalStack
     */
    public FileUploadService(S3AsyncClient s3Client) {
        this.s3Client = s3Client;
    }

    /**
     * Initialize the service after construction.
     */
    @PostConstruct
    public void initialize() {
        uploadThreadPool = Executors.newFixedThreadPool(maxUploadThreads);
        log.info("Initialized upload thread pool with {} threads", maxUploadThreads);
    }

    /**
     * Simple upload for small files (not using multipart upload).
     *
     * @param bucketName Target S3 bucket
     * @param objectKey Object key in S3
     * @param filePath Path to local file
     * @param fileSize Size of the file
     * @throws IOException If upload fails
     */
    public void uploadSimpleFile(String bucketName, String objectKey, Path filePath, long fileSize) throws IOException {
        log.info("Starting simple upload for file: {} ({} bytes) to {}/{}",
                filePath.getFileName(), fileSize, bucketName, objectKey);

        try {
            // Create the PutObject request
            PutObjectRequest putObjectRequest = PutObjectRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .build();

            // Upload the file
            s3Client.putObject(putObjectRequest, AsyncRequestBody.fromFile(filePath))
                    .get(uploadTimeoutMinutes, TimeUnit.MINUTES);

            log.info("Simple upload completed successfully for {}", objectKey);
        } catch (Exception e) {
            String errorMsg = "Simple upload failed for " + objectKey + ": " + e.getMessage();
            log.error(errorMsg);
            throw new IOException(errorMsg, e);
        }
    }

    /**
     * Upload a file with parallel part uploads for better performance.
     * Automatically calculates optimal part size and uses multiple threads for uploading.
     *
     * @param bucketName Target S3 bucket
     * @param objectKey Object key in S3
     * @param filePath Path to local file
     * @throws Exception If upload fails
     */
    public void uploadFileParallel(String bucketName, String objectKey, Path filePath) throws Exception {
        Instant startTime = Instant.now();
        log.info("Starting parallel multipart upload for {}", objectKey);

        // Step 1: Calculate optimal part configuration
        ParallelUploadConfig config = calculateUploadConfig(filePath);
        logUploadConfiguration(config, objectKey);

        // Step 2: Initiate multipart upload
        String uploadId = initiateMultipartUpload(bucketName, objectKey);
        log.info("Multipart upload initiated with ID: {}", uploadId);

        try {
            // Step 3: Upload all parts in parallel
            List<CompletedPart> completedParts = uploadAllParts(
                    bucketName, objectKey, uploadId, filePath, config);

            // Step 4: Complete multipart upload
            completeMultipartUpload(bucketName, objectKey, uploadId, completedParts);

            // Log upload statistics
            logUploadStatistics(startTime, objectKey, config);
        } catch (Exception e) {
            // Abort upload on failure
            log.error("Multipart upload failed: {}", e.getMessage());
            abortMultipartUpload(bucketName, objectKey, uploadId);
            throw new IOException("Failed to upload file " + objectKey, e);
        }
    }

    /**
     * Uploads all parts of a file in parallel and returns the completed parts.
     */
    private List<CompletedPart> uploadAllParts(
            String bucketName, String objectKey, String uploadId,
            Path filePath, ParallelUploadConfig config) throws Exception {

        log.debug("Creating upload tasks for all {} parts...", config.numParts);
        List<CompletableFuture<CompletedPart>> uploadFutures = new ArrayList<>();

        // Create futures for each part upload
        for (int partNumber = 1; partNumber <= config.numParts; partNumber++) {
            PartUploadTask task = createPartUploadTask(
                    partNumber, config.partSize, config.fileSize);

            CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(
                    () -> uploadPartWithRetry(bucketName, objectKey, uploadId, filePath,
                            task, DEFAULT_MAX_RETRIES),
                    uploadThreadPool);

            uploadFutures.add(future);
        }

        log.info("All upload tasks created. Waiting for completion...");

        // Wait for all uploads to complete
        CompletableFuture<Void> allUploadsFuture = CompletableFuture.allOf(
                uploadFutures.toArray(new CompletableFuture[0]));

        allUploadsFuture.get(uploadTimeoutMinutes, TimeUnit.MINUTES);
        log.info("All part uploads completed successfully");

        // Collect and return all completed parts
        return uploadFutures.stream()
                .map(CompletableFuture::join)
                .collect(Collectors.toList());
    }

    /**
     * Creates a task description for uploading a specific part.
     */
    private PartUploadTask createPartUploadTask(int partNumber, long partSize, long fileSize) {
        final long position = (partNumber - 1) * partSize;
        final long size = Math.min(partSize, fileSize - position);

        return new PartUploadTask(partNumber, position, size);
    }

    /**
     * Uploads a single part with retries in case of failure.
     */
    private CompletedPart uploadPartWithRetry(
            String bucketName, String objectKey, String uploadId,
            Path filePath, PartUploadTask task, int maxRetries) {

        String threadName = Thread.currentThread().getName();
        log.debug("Thread {}: Starting upload of part {} (bytes {} to {})",
                threadName, task.partNumber, task.position, task.position + task.size - 1);

        int retryCount = 0;
        Exception lastException = null;

        while (retryCount <= maxRetries) {
            try {
                if (retryCount > 0) {
                    log.debug("Retry #{} for part {}", retryCount, task.partNumber);
                }

                CompletedPart part = uploadSinglePart(
                        bucketName, objectKey, uploadId, filePath, task);

                log.debug("Thread {}: Completed upload of part {}, ETag: {}",
                        threadName, task.partNumber, part.eTag());

                return part;

            } catch (Exception e) {
                lastException = e;
                retryCount++;

                if (retryCount > maxRetries) {
                    log.error("Part {} failed after {} retries: {}",
                            task.partNumber, retryCount, e.getMessage());
                    break;
                }

                // Calculate exponential backoff time
                long backoffMillis = (long) (Math.pow(2, retryCount) * 100);
                log.debug("Part {} upload failed, retrying in {} ms. Error: {}",
                        task.partNumber, backoffMillis, e.getMessage());

                try {
                    Thread.sleep(backoffMillis);
                } catch (InterruptedException ie) {
                    Thread.currentThread().interrupt();
                    throw new RuntimeException("Thread interrupted during backoff", ie);
                }
            }
        }

        // If we got here, we exhausted all retries
        throw new RuntimeException("Failed to upload part " + task.partNumber, lastException);
    }

    /**
     * Upload a single part of the file.
     */
    private CompletedPart uploadSinglePart(
            String bucketName, String objectKey, String uploadId,
            Path filePath, PartUploadTask task) throws IOException {

        // Read this part from the file
        ByteBuffer partData = readFileSegment(filePath, task.position, task.size);

        // Create and execute upload request
        UploadPartRequest uploadPartRequest = UploadPartRequest.builder()
                .bucket(bucketName)
                .key(objectKey)
                .uploadId(uploadId)
                .partNumber(task.partNumber)
                .contentLength(task.size)
                .build();

        try {
            UploadPartResponse response = s3Client.uploadPart(
                            uploadPartRequest, AsyncRequestBody.fromByteBuffer(partData))
                    .get(uploadTimeoutMinutes, TimeUnit.MINUTES);

            return CompletedPart.builder()
                    .partNumber(task.partNumber)
                    .eTag(response.eTag())
                    .build();

        } catch (Exception e) {
            throw new IOException("Failed to upload part " + task.partNumber + ": " + e.getMessage(), e);
        }
    }

    /**
     * Calculate optimal part size and count based on file size.
     * Returns a configuration object with upload parameters.
     */
    private ParallelUploadConfig calculateUploadConfig(Path filePath) throws IOException {
        long fileSize = filePath.toFile().length();

        // Start with configured or minimum part size
        long partSize = Math.max(partSizeMb * MB, MIN_PART_SIZE_BYTES);

        // If file is large enough to exceed max parts limit, increase part size
        if (fileSize > partSize * MAX_PARTS) {
            // Calculate minimum required part size to stay under part count limit
            partSize = (fileSize + MAX_PARTS - 1) / MAX_PARTS; // Ceiling division

            // Round up to nearest MB for cleaner numbers
            partSize = ((partSize + MB - 1) / MB) * MB; // Round up to nearest MB
        }

        // Calculate part count based on part size
        long partCount = (fileSize + partSize - 1) / partSize; // Ceiling division

        return new ParallelUploadConfig(fileSize, partSize, partCount);
    }

    /**
     * Initiate a multipart upload in S3.
     */
    private String initiateMultipartUpload(String bucketName, String objectKey) {
        try {
            CreateMultipartUploadRequest request = CreateMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .contentType("application/octet-stream")
                    .build();

            CreateMultipartUploadResponse response = s3Client.createMultipartUpload(request)
                    .get(INITIATE_UPLOAD_TIMEOUT, TimeUnit.MINUTES);

            return response.uploadId();
        } catch (Exception e) {
            throw new RuntimeException("Failed to initiate multipart upload: " + e.getMessage(), e);
        }
    }

    /**
     * Complete the multipart upload by combining all uploaded parts.
     */
    private void completeMultipartUpload(
            String bucketName, String objectKey, String uploadId, List<CompletedPart> parts) {

        try {
            // Sort parts by part number
            List<CompletedPart> sortedParts = parts.stream()
                    .sorted(Comparator.comparingInt(CompletedPart::partNumber))
                    .collect(Collectors.toList());

            CompleteMultipartUploadRequest completeRequest = CompleteMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .uploadId(uploadId)
                    .multipartUpload(CompletedMultipartUpload.builder()
                            .parts(sortedParts)
                            .build())
                    .build();

            CompleteMultipartUploadResponse response = s3Client.completeMultipartUpload(completeRequest)
                    .get(COMPLETE_UPLOAD_TIMEOUT, TimeUnit.MINUTES);

            log.info("Multipart upload completed successfully! Object: {}, ETag: {}",
                    response.key(), response.eTag());

        } catch (Exception e) {
            throw new RuntimeException("Failed to complete multipart upload: " + e.getMessage(), e);
        }
    }

    /**
     * Abort a multipart upload.
     */
    private void abortMultipartUpload(String bucketName, String objectKey, String uploadId) {
        try {
            AbortMultipartUploadRequest abortRequest = AbortMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .uploadId(uploadId)
                    .build();

            s3Client.abortMultipartUpload(abortRequest).get(ABORT_UPLOAD_TIMEOUT, TimeUnit.MINUTES);
            log.info("Multipart upload aborted successfully");
        } catch (Exception e) {
            // Just log error since this is already in an error path
            log.error("Failed to abort multipart upload: {}", e.getMessage());
        }
    }

    /**
     * Read a specific segment from the file.
     */
    private ByteBuffer readFileSegment(Path filePath, long position, long size) throws IOException {
        ByteBuffer buffer = ByteBuffer.allocate((int) size);

        try (FileChannel channel = FileChannel.open(filePath, StandardOpenOption.READ)) {
            channel.position(position);

            int bytesRead;
            int totalRead = 0;

            // Read until buffer is full or EOF
            while (totalRead < size && (bytesRead = channel.read(buffer)) != -1) {
                totalRead += bytesRead;
            }

            if (totalRead < size) {
                log.warn("Read {} bytes but expected {} bytes", totalRead, size);
            }

            buffer.flip();
            return buffer;
        }
    }

    /**
     * Log the upload configuration.
     */
    private void logUploadConfiguration(ParallelUploadConfig config, String objectKey) {
        log.info("Upload configuration for {}:", objectKey);
        log.info("  - File size: {} bytes ({} MB)",
                config.fileSize, config.fileSize / MB);
        log.info("  - Part size: {} bytes ({} MB)",
                config.partSize, config.partSize / MB);
        log.info("  - Number of parts: {}", config.numParts);
        log.info("  - Thread pool size: {}", maxUploadThreads);
    }

    /**
     * Log upload statistics at the end of an upload.
     */
    private void logUploadStatistics(Instant startTime, String objectKey, ParallelUploadConfig config) {
        Duration uploadTime = Duration.between(startTime, Instant.now());
        double throughputMBps = (double) config.fileSize / (uploadTime.toMillis() * 1000) * MB;

        log.info("Upload statistics for {}:", objectKey);
        log.info("  - Total time: {}.{} seconds",
                uploadTime.getSeconds(), uploadTime.getNano() / 1000000);
        log.info("  - Throughput: {:.2f} MB/s", throughputMBps);
    }

    /**
     * Clean up resources on service shutdown.
     */
    @PreDestroy
    public void cleanup() {
        log.info("Shutting down FileUploadService resources");

        if (uploadThreadPool != null && !uploadThreadPool.isShutdown()) {
            try {
                uploadThreadPool.shutdown();
                if (!uploadThreadPool.awaitTermination(THREAD_POOL_SHUTDOWN_TIMEOUT_SECONDS, TimeUnit.SECONDS)) {
                    log.warn("Upload thread pool did not terminate in allowed time - forcing shutdown");
                    uploadThreadPool.shutdownNow();
                }
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                log.warn("Upload thread pool shutdown interrupted", e);
                uploadThreadPool.shutdownNow();
            }
        }
    }

    /**
     * Configuration class for parallel uploads.
     */
    private static class ParallelUploadConfig {
        final long fileSize;
        final long partSize;
        final long numParts;

        ParallelUploadConfig(long fileSize, long partSize, long numParts) {
            this.fileSize = fileSize;
            this.partSize = partSize;
            this.numParts = numParts;
        }
    }

    /**
     * Task description for a single part upload.
     */
    private static class PartUploadTask {
        final int partNumber;
        final long position;
        final long size;

        PartUploadTask(int partNumber, long position, long size) {
            this.partNumber = partNumber;
            this.position = position;
            this.size = size;
        }
    }
}
===============
package com.example.s3upload.web;

import com.example.s3upload.model.FileCompletionRequest;
import com.example.s3upload.model.FileCreationDetails;
import com.example.s3upload.service.FileService;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.concurrent.CompletableFuture;

/**
 * REST Controller for file operations
 */
@RestController
@RequestMapping("/api/files")
public class FileController {
    private static final Logger log = LoggerFactory.getLogger(FileController.class);

    private final FileService fileService;

    public FileController(FileService fileService) {
        this.fileService = fileService;
    }

    /**
     * Endpoint to process file creation requests
     *
     * @param request File creation details
     * @return FileCompletionRequest with upload status
     */
    @PostMapping("/process")
    public CompletableFuture<ResponseEntity<FileCompletionRequest>> processFile(
            @RequestBody FileCreationDetails request) {

        log.info("Received file processing request with ID: {}", request.requestId());

        return fileService.processFileRequest(request)
                .thenApply(response -> {
                    log.info("Completed processing request ID: {}", request.requestId());
                    return ResponseEntity.ok(response);
                })
                .exceptionally(ex -> {
                    log.error("Error processing request ID: {}", request.requestId(), ex);
                    return ResponseEntity.internalServerError().body(
                            new FileCompletionRequest(
                                    request.requestId(),
                                    request.requestType(),
                                    request.cobDate(),
                                    request.freq(),
                                    false,
                                    false,
                                    false,
                                    !request.adjustData().isEmpty(),
                                    false,
                                    !request.keysData().isEmpty()
                            )
                    );
                });
    }
}
=======================
package com.example.s3upload;

import java.io.*;
import java.net.HttpURLConnection;
import java.net.URL;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.ThreadLocalRandom;

/**
 * Test program to generate a 500MB file with 5 entities (100MB each)
 * and call the upload endpoint
 */
public class LargeFileUploadTest {

    // Configuration
    private static final int ENTITY_COUNT = 5;
    private static final long ENTITY_SIZE_MB = 100;
    private static final String UPLOAD_URL = "http://localhost:8080/files/upload";
    private static final String OUTPUT_DIR = "D:\\data";
    private static final String UPLOAD_BUCKET = "demo"; // Optional bucket name

    // Entity names
    private static final String[] ENTITY_NAMES = {
            "customer-records",
            "transaction-history",
            "product-catalog",
            "inventory-tracking",
            "analytics-data"
    };

    public static void main(String[] args) {
        try {
            System.out.println("Starting Large File Upload Test");
            System.out.println("Generating " + ENTITY_COUNT + " entities with " + ENTITY_SIZE_MB + "MB each");

            // Create output directory if it doesn't exist
            Path outputDir = Paths.get(OUTPUT_DIR);
            Files.createDirectories(outputDir);

            // Generate entities and prepare payload
            List<Map<String, Object>> entityPayloads = generateEntities(outputDir);

            // Convert payload to JSON
            String jsonPayload = convertToJson(entityPayloads);

            // Save payload to file for inspection
            Path payloadFile = outputDir.resolve("payload.json");
            Files.writeString(payloadFile, jsonPayload);
            System.out.println("Payload saved to: " + payloadFile);

            // Call upload endpoint
            System.out.println("Calling upload endpoint: " + UPLOAD_URL);
            String response = callUploadEndpoint(jsonPayload);

            System.out.println("Upload response:");
            System.out.println(response);

            System.out.println("Test completed successfully");

        } catch (Exception e) {
            System.err.println("Error during test: " + e.getMessage());
            e.printStackTrace();
        }
    }

    /**
     * Generate entities with large data
     */
    private static List<Map<String, Object>> generateEntities(Path outputDir) throws IOException {
        List<Map<String, Object>> entityPayloads = new ArrayList<>();

        for (int i = 0; i < ENTITY_COUNT; i++) {
            String entityName = ENTITY_NAMES[i];
            System.out.println("Generating entity: " + entityName);

            // Generate data for this entity
            List<String> data = generateLargeEntityData(ENTITY_SIZE_MB);

            // Create entity payload
            Map<String, Object> entityPayload = new HashMap<>();
            entityPayload.put("entityName", entityName);
            entityPayload.put("data", data);

            entityPayloads.add(entityPayload);

            // Log progress
            System.out.println("Entity " + entityName + " generated with " +
                    data.size() + " lines (" + (i+1) + "/" + ENTITY_COUNT + ")");
        }

        return entityPayloads;
    }

    /**
     * Generate large data for an entity
     */
    private static List<String> generateLargeEntityData(long sizeMB) {
        List<String> data = new ArrayList<>();

        // Add header row
        data.add(generateHeader());

        // Track size
        long totalSize = data.get(0).length();
        long targetSize = sizeMB * 1024 * 1024; // Convert to bytes

        int rowCounter = 0;
        int progressInterval = 1000;

        // Generate rows until we reach target size
        while (totalSize < targetSize) {
            String row = generateDataRow();
            data.add(row);
            totalSize += row.length();

            rowCounter++;
            if (rowCounter % progressInterval == 0) {
                double percentComplete = (double) totalSize / targetSize * 100;
                System.out.printf("  Progress: %.2f%% (%d KB / %d MB)%n",
                        percentComplete, totalSize / 1024, sizeMB);
            }
        }

        System.out.println("  Generated " + rowCounter + " rows, total size: " +
                (totalSize / (1024 * 1024)) + "MB");

        return data;
    }

    /**
     * Generate a header row for CSV data
     */
    private static String generateHeader() {
        return "id,timestamp,customer_id,product_id,transaction_type,amount,status,region,category,subcategory," +
                "payment_method,shipping_method,discount_code,tax_amount,item_count,description";
    }

    /**
     * Generate a random data row
     */
    private static String generateDataRow() {
        StringBuilder row = new StringBuilder();

        // id
        row.append(UUID.randomUUID()).append(",");

        // timestamp
        LocalDateTime timestamp = LocalDateTime.now().minusDays(ThreadLocalRandom.current().nextInt(365));
        row.append(timestamp.format(DateTimeFormatter.ISO_LOCAL_DATE_TIME)).append(",");

        // customer_id
        row.append("CUST-").append(ThreadLocalRandom.current().nextInt(1, 100000)).append(",");

        // product_id
        row.append("PROD-").append(ThreadLocalRandom.current().nextInt(1, 50000)).append(",");

        // transaction_type
        String[] transactionTypes = {"PURCHASE", "REFUND", "EXCHANGE", "RETURN", "ADJUSTMENT"};
        row.append(transactionTypes[ThreadLocalRandom.current().nextInt(transactionTypes.length)]).append(",");

        // amount
        double amount = ThreadLocalRandom.current().nextDouble(1.0, 1000.0);
        row.append(String.format("%.2f", amount)).append(",");

        // status
        String[] statuses = {"COMPLETED", "PENDING", "CANCELLED", "FAILED", "IN_PROGRESS"};
        row.append(statuses[ThreadLocalRandom.current().nextInt(statuses.length)]).append(",");

        // region
        String[] regions = {"NORTH", "SOUTH", "EAST", "WEST", "CENTRAL", "NORTHEAST", "NORTHWEST", "SOUTHEAST", "SOUTHWEST"};
        row.append(regions[ThreadLocalRandom.current().nextInt(regions.length)]).append(",");

        // category
        String[] categories = {"ELECTRONICS", "CLOTHING", "FOOD", "BOOKS", "HOME", "BEAUTY", "SPORTS", "TOYS", "AUTOMOTIVE"};
        row.append(categories[ThreadLocalRandom.current().nextInt(categories.length)]).append(",");

        // subcategory
        String[] subcategories = {"PREMIUM", "STANDARD", "ECONOMY", "CLEARANCE", "NEW", "USED", "REFURBISHED"};
        row.append(subcategories[ThreadLocalRandom.current().nextInt(subcategories.length)]).append(",");

        // payment_method
        String[] paymentMethods = {"CREDIT", "DEBIT", "PAYPAL", "APPLE_PAY", "GOOGLE_PAY", "BANK_TRANSFER", "GIFT_CARD"};
        row.append(paymentMethods[ThreadLocalRandom.current().nextInt(paymentMethods.length)]).append(",");

        // shipping_method
        String[] shippingMethods = {"STANDARD", "EXPRESS", "NEXT_DAY", "TWO_DAY", "INTERNATIONAL", "PICKUP"};
        row.append(shippingMethods[ThreadLocalRandom.current().nextInt(shippingMethods.length)]).append(",");

        // discount_code
        boolean hasDiscount = ThreadLocalRandom.current().nextBoolean();
        if (hasDiscount) {
            row.append("DISC-").append(ThreadLocalRandom.current().nextInt(1, 100)).append(",");
        } else {
            row.append(",");
        }

        // tax_amount
        double taxAmount = amount * 0.08; // Assume 8% tax
        row.append(String.format("%.2f", taxAmount)).append(",");

        // item_count
        row.append(ThreadLocalRandom.current().nextInt(1, 11)).append(",");

        // description
        row.append("\"Transaction details with additional information for analysis purposes. ");
        row.append("This includes extended metadata about the transaction that can be used for ");
        row.append("various business intelligence reports and dashboards. The data can also be ");
        row.append("used for customer trend analysis and marketing campaign effectiveness measurement.\"");

        return row.toString();
    }

    /**
     * Convert entity payloads to JSON
     */
    private static String convertToJson(List<Map<String, Object>> entityPayloads) {
        StringBuilder json = new StringBuilder();
        json.append("[\n");

        for (int i = 0; i < entityPayloads.size(); i++) {
            Map<String, Object> entity = entityPayloads.get(i);
            json.append("  {\n");
            json.append("    \"entityName\": \"").append(entity.get("entityName")).append("\",\n");
            json.append("    \"data\": [\n");

            @SuppressWarnings("unchecked")
            List<String> data = (List<String>) entity.get("data");

            for (int j = 0; j < data.size(); j++) {
                json.append("      \"").append(escapeJson(data.get(j))).append("\"");
                if (j < data.size() - 1) {
                    json.append(",");
                }
                json.append("\n");
            }

            json.append("    ]\n");
            json.append("  }");
            if (i < entityPayloads.size() - 1) {
                json.append(",");
            }
            json.append("\n");
        }

        json.append("]\n");
        return json.toString();
    }

    /**
     * Escape special characters in JSON
     */
    private static String escapeJson(String input) {
        return input.replace("\\", "\\\\")
                .replace("\"", "\\\"")
                .replace("\n", "\\n")
                .replace("\r", "\\r")
                .replace("\t", "\\t");
    }

    /**
     * Call upload endpoint with JSON payload
     */
    private static String callUploadEndpoint(String jsonPayload) throws IOException {
        URL url = new URL(UPLOAD_URL + (UPLOAD_BUCKET != null ? "?bucket=" + UPLOAD_BUCKET : ""));
        HttpURLConnection connection = (HttpURLConnection) url.openConnection();
        connection.setRequestMethod("POST");
        connection.setRequestProperty("Content-Type", "application/json");
        connection.setRequestProperty("Accept", "application/json");
        connection.setDoOutput(true);

        // Set longer timeouts for large data
        connection.setConnectTimeout(30000); // 30 seconds
        connection.setReadTimeout(300000);   // 5 minutes

        // Send payload
        try (OutputStream os = connection.getOutputStream()) {
            byte[] input = jsonPayload.getBytes(StandardCharsets.UTF_8);
            os.write(input, 0, input.length);
        }

        // Read response
        StringBuilder response = new StringBuilder();
        try (BufferedReader br = new BufferedReader(
                new InputStreamReader(connection.getInputStream(), StandardCharsets.UTF_8))) {
            String responseLine;
            while ((responseLine = br.readLine()) != null) {
                response.append(responseLine.trim());
            }
        }

        return response.toString();
    }
}
==============
package com.example.fileuploader;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.http.*;
import org.springframework.web.client.RestTemplate;

import java.io.File;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;

/**
 * Application that runs 10 concurrent threads to submit file processing requests
 */
@SpringBootApplication
public class ConcurrentRequestsApplication implements CommandLineRunner {
    private static final Logger log = LoggerFactory.getLogger(ConcurrentRequestsApplication.class);

    private static final String API_URL = "http://localhost:8080/api/files/process";
    private static final int NUM_THREADS = 10;
    private static final int REQUEST_TIMEOUT_SECONDS = 60;
    private static final String RESULTS_DIR = "D://data/test-results";

    private final ObjectMapper objectMapper;
    private final RestTemplate restTemplate;
    private final ExecutorService executorService;
    private final CountDownLatch completionLatch;
    private final AtomicInteger successCount = new AtomicInteger(0);
    private final AtomicInteger failureCount = new AtomicInteger(0);

    public ConcurrentRequestsApplication() {
        this.objectMapper = new ObjectMapper()
                .enable(SerializationFeature.INDENT_OUTPUT);
        this.restTemplate = new RestTemplate();
        this.executorService = Executors.newFixedThreadPool(NUM_THREADS);
        this.completionLatch = new CountDownLatch(NUM_THREADS);
    }

    public static void main(String[] args) {
        SpringApplication.run(ConcurrentRequestsApplication.class, args);
    }

    @Override
    public void run(String... args) throws Exception {
        log.info("Starting concurrent request test with {} threads", NUM_THREADS);

        // Create results directory
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
        String resultsDirPath = RESULTS_DIR + "/" + timestamp;
        Files.createDirectories(Paths.get(resultsDirPath));

        // Generate payloads
        List<Map<String, Object>> payloads = generatePayloads();

        // Save payloads to disk for reference
        for (int i = 0; i < payloads.size(); i++) {
            objectMapper.writeValue(new File(resultsDirPath + "/request_" + (i + 1) + ".json"), payloads.get(i));
        }

        log.info("Generated {} payloads. Starting concurrent requests...", payloads.size());

        // Submit all requests concurrently
        List<Future<RequestResult>> futures = new ArrayList<>();
        for (int i = 0; i < payloads.size(); i++) {
            final int threadId = i + 1;
            final Map<String, Object> payload = payloads.get(i);

            // Submit task to executor
            Future<RequestResult> future = executorService.submit(() -> {
                try {
                    log.info("Thread {} starting request", threadId);
                    RequestResult result = sendRequest(threadId, payload);
                    if (result.isSuccess()) {
                        successCount.incrementAndGet();
                    } else {
                        failureCount.incrementAndGet();
                    }
                    return result;
                } catch (Exception e) {
                    log.error("Thread {} error: {}", threadId, e.getMessage(), e);
                    return new RequestResult(threadId, false, HttpStatus.INTERNAL_SERVER_ERROR.value(),
                            "Error: " + e.getMessage(), null);
                } finally {
                    completionLatch.countDown();
                }
            });

            futures.add(future);
        }

        // Wait for all requests to complete
        log.info("Waiting for all requests to complete...");
        boolean completed = completionLatch.await(REQUEST_TIMEOUT_SECONDS, TimeUnit.SECONDS);
        if (!completed) {
            log.warn("Not all requests completed within the timeout period of {} seconds", REQUEST_TIMEOUT_SECONDS);
        }

        // Collect and process results
        List<RequestResult> results = new ArrayList<>();
        for (Future<RequestResult> future : futures) {
            try {
                results.add(future.get(5, TimeUnit.SECONDS));
            } catch (Exception e) {
                log.error("Error retrieving result: {}", e.getMessage());
            }
        }

        // Save results
        objectMapper.writeValue(new File(resultsDirPath + "/results.json"), results);

        // Print summary
        log.info("Test completed. Total requests: {}", NUM_THREADS);
        log.info("Successful requests: {}", successCount.get());
        log.info("Failed requests: {}", failureCount.get());
        log.info("Results saved to: {}", resultsDirPath);

        // Print each result
        System.out.println("\n\n========== TEST RESULTS ==========");
        results.stream()
                .sorted(Comparator.comparingInt(RequestResult::getThreadId))
                .forEach(result -> {
                    System.out.println("\n----------------------------------");
                    System.out.println("Thread " + result.getThreadId() + ": " +
                            (result.isSuccess() ? "SUCCESS" : "FAILURE"));
                    System.out.println("Status: " + result.getStatusCode());

                    // Print formatted JSON response
                    System.out.println("\nRESPONSE JSON:");
                    try {
                        if (result.getResponseBody() != null) {
                            // Pretty print the JSON
                            Object json = objectMapper.readValue(result.getResponseBody(), Object.class);
                            System.out.println(objectMapper.writerWithDefaultPrettyPrinter()
                                    .writeValueAsString(json));

                            // Print structured summary based on FileCompletionRequest structure
                            if (result.getResponse() != null) {
                                System.out.println("\nFILE COMPLETION SUMMARY:");
                                String requestId = (String) result.getResponse().get("requestId");
                                System.out.println("Request ID:      " + requestId);

                                // Handle possible different field names in the response
                                Boolean adjSuccess = (Boolean) (
                                        result.getResponse().containsKey("adjFileStatus") ?
                                                result.getResponse().get("adjFileStatus") :
                                                result.getResponse().get("adjFileSuccess")
                                );

                                Boolean baseSuccess = (Boolean) (
                                        result.getResponse().containsKey("baseFileStatus") ?
                                                result.getResponse().get("baseFileStatus") :
                                                result.getResponse().get("baseFileSuccess")
                                );

                                Boolean keySuccess = (Boolean) (
                                        result.getResponse().containsKey("keyFileStatus") ?
                                                result.getResponse().get("keyFileStatus") :
                                                result.getResponse().get("keyFileSuccess")
                                );

                                System.out.println("Adjusted Files:  " + formatStatus(adjSuccess));
                                System.out.println("Base File:       " + formatStatus(baseSuccess));
                                System.out.println("Key File:        " + formatStatus(keySuccess));
                            }
                        } else {
                            System.out.println("No response body received");
                        }
                    } catch (Exception e) {
                        System.out.println("Error formatting response: " + e.getMessage());
                        System.out.println("Raw response: " + result.getResponseBody());
                    }
                });
        System.out.println("\n==================================");

        // Shutdown executor
        executorService.shutdown();
    }

    /**
     * Format a boolean status value for display
     */
    private String formatStatus(Boolean status) {
        if (status == null) {
            return "UNKNOWN";
        }
        return status ? "SUCCESS" : "FAILURE";
    }

    /**
     * Send a request to the API
     */
    private RequestResult sendRequest(int threadId, Map<String, Object> payload) {
        String threadName = "Thread-" + threadId;
        log.info("{} sending request", threadName);

        try {
            // Create headers
            HttpHeaders headers = new HttpHeaders();
            headers.setContentType(MediaType.APPLICATION_JSON);
            headers.add("X-Thread-ID", String.valueOf(threadId));

            // Create request entity
            String jsonPayload = objectMapper.writeValueAsString(payload);
            HttpEntity<String> requestEntity = new HttpEntity<>(jsonPayload, headers);

            // Record start time
            long startTime = System.currentTimeMillis();

            // Send request
            ResponseEntity<String> response = restTemplate.exchange(
                    API_URL, HttpMethod.POST, requestEntity, String.class);

            // Record end time
            long duration = System.currentTimeMillis() - startTime;

            // Parse response
            Map<String, Object> responseMap = null;
            String responseBody = response.getBody();

            if (responseBody != null && !responseBody.isEmpty()) {
                try {
                    responseMap = objectMapper.readValue(responseBody, Map.class);
                    log.debug("Response map: {}", responseMap);
                } catch (Exception e) {
                    log.error("Failed to parse response JSON: {}", e.getMessage());
                }
            }

            log.info("{} received response in {}ms: {}",
                    threadName, duration, response.getStatusCode());

            if (responseMap != null) {
                // Log key statuses from the response
                String requestId = (String) responseMap.get("requestId");

                // Handle possible different field names
                Boolean keyStatus = getStatusValue(responseMap, "keyFileStatus", "keyFileSuccess");
                Boolean adjStatus = getStatusValue(responseMap, "adjFileStatus", "adjFileSuccess");
                Boolean baseStatus = getStatusValue(responseMap, "baseFileStatus", "baseFileSuccess");

                log.info("{} result: requestId={}, key={}, adj={}, base={}",
                        threadName, requestId, keyStatus, adjStatus, baseStatus);
            }

            return new RequestResult(
                    threadId,
                    response.getStatusCode().is2xxSuccessful(),
                    response.getStatusCodeValue(),
                    responseBody,
                    responseMap
            );

        } catch (Exception e) {
            log.error("{} error: {}", threadName, e.getMessage(), e);
            return new RequestResult(
                    threadId,
                    false,
                    HttpStatus.INTERNAL_SERVER_ERROR.value(),
                    "Error: " + e.getMessage(),
                    null
            );
        }
    }

    /**
     * Get a status value from the response map, checking multiple possible field names
     */
    private Boolean getStatusValue(Map<String, Object> responseMap, String... fieldNames) {
        for (String fieldName : fieldNames) {
            if (responseMap.containsKey(fieldName)) {
                return (Boolean) responseMap.get(fieldName);
            }
        }
        return null;
    }

    /**
     * Generate 10 different payloads for testing
     */
    private List<Map<String, Object>> generatePayloads() {
        List<Map<String, Object>> payloads = new ArrayList<>();

        for (int i = 1; i <= NUM_THREADS; i++) {
            String requestId = "REQ" + i;
            Map<String, Object> payload = createBasePayload(requestId);

            // Add special adjust data for this thread
            Map<String, List<String>> adjustData = createAdjustDataForThread(i);
            payload.put("adjustData", adjustData);

            payloads.add(payload);
        }

        return payloads;
    }

    /**
     * Create the base payload structure (common for all threads)
     */
    private Map<String, Object> createBasePayload(String requestId) {
        Map<String, Object> payload = new HashMap<>();

        // Basic request details
        payload.put("requestId", requestId);
        payload.put("sliceNames", Arrays.asList("Slice1", "Slice2", "Slice3"));
        payload.put("distinctIds", Arrays.asList("ID001", "ID002", "ID003"));
        payload.put("idFilePath", "D://data/idfiles/sample_ids.txt");
        payload.put("parentTableName", "CustomerAccounts");
        payload.put("requestType", "DAILY_PROCESSING");
        payload.put("cobDate", "20250515");
        payload.put("freq", "DAILY");
        payload.put("tableList", Arrays.asList("Customers", "Accounts", "Transactions"));

        // Add keys data - always successful
        List<String> keysData = Arrays.asList(
                "CUS001=JOHN_DOE_KEY",
                "CUS002=JANE_SMITH_KEY",
                "CUS003=BOB_JOHNSON_KEY",
                "ACC001=SAVINGS_ACCOUNT_KEY",
                "ACC002=CHECKING_ACCOUNT_KEY",
                "ACC003=SAVINGS_ACCOUNT_KEY_2",
                "ACC004=CHECKING_ACCOUNT_KEY_2"
        );
        payload.put("keysData", keysData);

        return payload;
    }

    /**
     * Create adjust data specific to each thread
     * Some threads will have errors to test failure scenarios
     */
    private Map<String, List<String>> createAdjustDataForThread(int threadId) {
        Map<String, List<String>> adjustData = new HashMap<>();

        // Common data for all threads
        adjustData.put("customer_vue", Arrays.asList(
                "CUS001,John Doe,Active,New York,10001",
                "CUS002,Jane Smith,Active,Los Angeles,90001",
                "CUS003,Bob Johnson,Inactive,Chicago,60601"
        ));

        // Create thread-specific adjustment data
        String adjustmentDataKey = "adjustmentdata" + threadId;
        List<String> threadSpecificData = new ArrayList<>();

        // Add some base records
        threadSpecificData.add("ADJ" + threadId + "01,Customer" + (threadId * 3 - 2) +
                ",Account" + (threadId * 3 - 2) + "," + (400 + threadId * 100) + ".00,20250" + (510 + threadId));
        threadSpecificData.add("ADJ" + threadId + "02,Customer" + (threadId * 3 - 1) +
                ",Account" + (threadId * 3 - 1) + "," + (450 + threadId * 100) + ".50,20250" + (511 + threadId));

        // Add errors to specific threads
        if (threadId == 3 || threadId == 7) {
            // Add invalid data for threads 3 and 7
            threadSpecificData.add("INVALID_DATA_FORMAT");
        } else {
            // Normal data for other threads
            threadSpecificData.add("ADJ" + threadId + "03,Customer" + (threadId * 3) +
                    ",Account" + (threadId * 3) + "," + (500 + threadId * 100) + ".00,20250" + (512 + threadId));
        }

        adjustData.put(adjustmentDataKey, threadSpecificData);

        // Modify account_vue for some threads to create failures
        if (threadId == 4 || threadId == 8) {
            adjustData.put("account_vue", Arrays.asList(
                    "ACC001,CUS001,Savings,10000.50,20200101",
                    "ACC002,CUS001,Checking,INVALID_AMOUNT,20210315",
                    "ACC003,CUS002,Savings,25000.00,20190620"
            ));
        } else {
            adjustData.put("account_vue", Arrays.asList(
                    "ACC001,CUS001,Savings,10000.50,20200101",
                    "ACC002,CUS001,Checking,5000.75,20210315",
                    "ACC003,CUS002,Savings,25000.00,20190620",
                    "ACC004,CUS003,Checking,1500.25,20220110"
            ));
        }

        // Modify transaction_vue for some threads to create failures
        if (threadId == 5 || threadId == 9) {
            adjustData.put("transaction_vue", Arrays.asList(
                    "TRX001,ACC001,Deposit,500.00,20250510",
                    "TRX002,ACC002,Withdrawal,INVALID_AMOUNT,20250511",
                    "TRX003,ACC003,Deposit,1000.00,20250512"
            ));
        } else {
            adjustData.put("transaction_vue", Arrays.asList(
                    "TRX001,ACC001,Deposit,500.00,20250510",
                    "TRX002,ACC002,Withdrawal,150.50,20250511",
                    "TRX003,ACC003,Deposit,1000.00,20250512",
                    "TRX004,ACC002,Transfer,300.25,20250513",
                    "TRX005,ACC004,Withdrawal,200.00,20250514"
            ));
        }

        return adjustData;
    }

    /**
     * Class to capture request results
     */
    static class RequestResult {
        private final int threadId;
        private final boolean success;
        private final int statusCode;
        private final String responseBody;
        private final Map<String, Object> response;

        public RequestResult(int threadId, boolean success, int statusCode,
                             String responseBody, Map<String, Object> response) {
            this.threadId = threadId;
            this.success = success;
            this.statusCode = statusCode;
            this.responseBody = responseBody;
            this.response = response;
        }

        public int getThreadId() {
            return threadId;
        }

        public boolean isSuccess() {
            return success;
        }

        public int getStatusCode() {
            return statusCode;
        }

        public String getResponseBody() {
            return responseBody;
        }

        public Map<String, Object> getResponse() {
            return response;
        }
    }
}