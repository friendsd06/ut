"""
Simplified Databricks SQL Warehouse reader module for direct query execution.
"""

import logging
from dataclasses import dataclass
from typing import Optional
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.utils import AnalysisException

@dataclass(frozen=True)
class WarehouseConfig:
    """Configuration for Databricks SQL Warehouse connection."""
    host: str
    http_path: str
    driver: str = "com.simba.spark.jdbc.Driver"

    def __post_init__(self):
        """Validate configuration."""
        if not self.host:
            raise ValueError("Host cannot be empty")
        if not self.http_path:
            raise ValueError("HTTP path cannot be empty")
        if not self.http_path.startswith("/"):
            raise ValueError("HTTP path must start with '/'")

class DatabaseError(Exception):
    """Base exception for database-related errors."""
    pass

class ConnectionError(DatabaseError):
    """Raised when connection fails."""
    pass

class QueryError(DatabaseError):
    """Raised when query execution fails."""
    pass

class DatabricksSqlWarehouseReader:
    """
    Simple reader for executing queries on Databricks SQL Warehouse.
    Focused on direct query execution without partitioning.
    """

    def __init__(
        self,
        warehouse_config: WarehouseConfig,
        token_provider: callable,
        logger: Optional[logging.Logger] = None
    ):
        """
        Initialize the reader.

        Args:
            warehouse_config: Warehouse connection configuration
            token_provider: Function that returns authentication token
            logger: Optional custom logger
        """
        self.config = warehouse_config
        self.get_token = token_provider
        self.logger = logger or logging.getLogger(__name__)

    def execute_query(
        self,
        spark: SparkSession,
        query: str
    ) -> DataFrame:
        """
        Execute a SQL query.

        Args:
            spark: Active SparkSession
            query: SQL query to execute

        Returns:
            DataFrame with query results

        Raises:
            ConnectionError: If connection fails
            QueryError: If query execution fails
        """
        if not query or not query.strip():
            raise ValueError("Query cannot be empty")

        try:
            self.logger.info("Executing warehouse query")

            # Build JDBC URL
            jdbc_url = (
                f"jdbc:spark://{self.config.host}:443/default"
                f";transportMode=http;ssl=1"
                f";httpPath={self.config.http_path}"
            )

            # Configure query
            options = {
                "url": jdbc_url,
                "driver": self.config.driver,
                "dbtable": f"({query}) AS query_result",
                "user": "token",
                "password": self.get_token()
            }

            # Execute query
            df = spark.read.format("jdbc").options(**options).load()
            self.logger.info("Query executed successfully")

            return df

        except AnalysisException as e:
            error_msg = str(e)
            self.logger.error(f"Query failed: {error_msg}")
            raise QueryError(f"Query execution failed: {error_msg}")

        except Exception as e:
            error_msg = str(e)
            self.logger.error(f"Database error: {error_msg}")

            if "Connection refused" in error_msg:
                raise ConnectionError(f"Failed to connect: {error_msg}")
            raise DatabaseError(f"Unexpected error: {error_msg}")

# Example usage
def example_usage():
    """Example of how to use the warehouse reader."""
    # Configuration
    config = WarehouseConfig(
        host="your-host.databricks.com",
        http_path="/sql/1.0/warehouses/xyz"
    )

    # Create reader
    reader = DatabricksSqlWarehouseReader(
        warehouse_config=config,
        token_provider=lambda: "your-token"
    )

    # Execute query
    try:
        spark = SparkSession.builder.appName("WarehouseQuery").getOrCreate()

        query = """
        SELECT
            department,
            COUNT(*) as employee_count,
            AVG(salary) as avg_salary
        FROM employees
        WHERE hire_date >= '2024-01-01'
        GROUP BY department
        """

        df = reader.execute_query(spark, query)
        print(f"Query returned {df.count()} rows")

    except (ConnectionError, QueryError) as e:
        print(f"Error: {str(e)}")
    except Exception as e:
        print(f"Unexpected error: {str(e)}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    example_usage() g