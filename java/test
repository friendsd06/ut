package com.example.s3demo.aws;

import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;
import software.amazon.awssdk.auth.credentials.StaticCredentialsProvider;
import software.amazon.awssdk.core.async.AsyncRequestBody;
import software.amazon.awssdk.regions.Region;
import software.amazon.awssdk.services.s3.S3AsyncClient;
import software.amazon.awssdk.services.s3.S3Configuration;
import software.amazon.awssdk.services.s3.model.*;

import java.net.URI;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.file.*;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.*;

/**
 * Upload every .txt file in D:\data\txt\ to LocalStack S3.
 *  - Small files (<5 MiB)   ➜ single PutObject
 *  - Big   files (≥5 MiB)   ➜ manual multipart upload
 */
public class TxtUploader {

    /* ── CONFIG — Adjust paths / bucket / part size if needed ─────────── */
    private static final Path   SOURCE_DIR = Paths.get("D:\\data\\");
    private static final String BUCKET     = "demo";
    private static final long   PART_SIZE  = 5 * 1_024 * 1_024;  // 5 MiB
    /* ──────────────────────────────────────────────────────────────────── */

    private final S3AsyncClient s3;

    public TxtUploader() {
        this.s3 = S3AsyncClient.builder()
                .endpointOverride(URI.create("https://localhost.localstack.cloud:4566"))     // LocalStack
                .region(Region.US_EAST_1)
                .credentialsProvider(
                        StaticCredentialsProvider.create(
                                AwsBasicCredentials.create("test", "test")))
                // ★ Path-style needed for LocalStack
                .serviceConfiguration(
                        S3Configuration.builder()
                                .pathStyleAccessEnabled(true)
                                .build())
                .build();
    }

    /* ========= entry-point ========= */
    public static void main(String[] args) throws Exception {
        new TxtUploader().run();
    }

    /* ========= workflow ========= */
    private void run() throws Exception {
        ensureBucket();

        ExecutorService pool = Executors.newFixedThreadPool(8);
        try {
            List<CompletableFuture<Void>> all = new ArrayList<>();

            Files.list(SOURCE_DIR)
                    .filter(p -> p.toString().endsWith(".txt"))
                    .forEach(p -> all.add(
                            CompletableFuture.runAsync(() -> upload(p), pool)));

            CompletableFuture.allOf(all.toArray(new CompletableFuture[0])).join();
            System.out.println("✅  uploads complete");
        } finally {
            pool.shutdown();
            pool.awaitTermination(1, TimeUnit.MINUTES);
            s3.close();
        }
    }

    private void ensureBucket() {
        try {
            s3.createBucket(b -> b.bucket(BUCKET)).join();
            System.out.printf("Bucket %s created%n", BUCKET);
        } catch (BucketAlreadyOwnedByYouException | BucketAlreadyExistsException ignored) {
            System.out.printf("Bucket %s already exists%n", BUCKET);
        }
    }

    private void upload(Path file) {
        try {
            long size = Files.size(file);
            String key = file.getFileName().toString();

            if (size < PART_SIZE) {
                putObject(file, key).join();
            } else {
                multipart(file, key).join();
            }
        } catch (Exception e) {
            throw new CompletionException("Upload failed for " + file, e);
        }
    }

    /* ---------- small file: single PUT ---------- */
    private CompletableFuture<Void> putObject(Path file, String key) {
        return s3.putObject(
                        PutObjectRequest.builder()
                                .bucket(BUCKET)
                                .key(key)
                                .build(),
                        AsyncRequestBody.fromFile(file))
                .thenAccept(r ->
                        System.out.printf("PUT  %-20s etag=%s%n", key, r.eTag()));
    }

    /* ---------- big file: manual multipart ---------- */
    private CompletableFuture<Void> multipart(Path file, String key) throws Exception {

        String uploadId = s3.createMultipartUpload(
                b -> b.bucket(BUCKET).key(key)).join().uploadId();

        List<CompletedPart> completed = new CopyOnWriteArrayList<>();
        List<CompletableFuture<?>> partFutures = new ArrayList<>();

        try (FileChannel ch = FileChannel.open(file, StandardOpenOption.READ)) {
            long size = ch.size();
            long pos  = 0;
            int  part = 1;

            while (pos < size) {
                long bytes = Math.min(PART_SIZE, size - pos);
                long offset = pos;
                int  number = part++;

                partFutures.add(
                        s3.uploadPart(
                                        UploadPartRequest.builder()
                                                .bucket(BUCKET).key(key)
                                                .uploadId(uploadId)
                                                .partNumber(number)
                                                .contentLength(bytes)
                                                .build(),
                                        slicePublisher(file, offset, bytes))
                                .thenAccept(resp ->
                                        completed.add(CompletedPart.builder()
                                                .partNumber(number)
                                                .eTag(resp.eTag())
                                                .build()))
                );
                pos += bytes;
            }
        }

        return CompletableFuture.allOf(partFutures.toArray(new CompletableFuture[0]))
                .thenCompose(v -> {
                    completed.sort((a, b) -> a.partNumber() - b.partNumber());
                    return s3.completeMultipartUpload(b -> b
                            .bucket(BUCKET).key(key).uploadId(uploadId)
                            .multipartUpload(
                                    CompletedMultipartUpload.builder()
                                            .parts(completed)
                                            .build()));
                })
                .thenAccept(r ->
                        System.out.printf("MPU  %-20s etag=%s%n", key, r.eTag()))
                .exceptionallyCompose(ex ->
                        // abort MPU on any error
                        s3.abortMultipartUpload(b -> b.bucket(BUCKET)
                                        .key(key)
                                        .uploadId(uploadId))
                                .handle((v,ignored) -> { throw new CompletionException(ex); }));
    }

    /* helper: Reactive publisher for a slice of the file */
    private static AsyncRequestBody slicePublisher(Path file, long offset, long bytes) {
        return AsyncRequestBody.fromPublisher(sub -> {
            try (FileChannel fc = FileChannel.open(file, StandardOpenOption.READ)) {
                ByteBuffer buf = fc.map(FileChannel.MapMode.READ_ONLY, offset, bytes);
                sub.onNext(buf);
                sub.onComplete();
            } catch (Exception ex) {
                sub.onError(ex);
            }
        });
    }
}