from pyspark.sql import DataFrame
from pyspark.sql.functions import col, explode, flatten
from pyspark.sql.types import ArrayType
from typing import List, Dict, Optional
from dataclasses import dataclass
import yaml
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class SourceConfig:
    """Configuration for data source"""
    table_name: Optional[str] = None
    query: Optional[str] = None

@dataclass
class WriteConfig:
    """Configuration for data writing"""
    path: str
    format: str = 'parquet'
    mode: str = 'overwrite'
    partition_by: Optional[List[str]] = None
    options: Dict[str, str] = None

class ConfigReader:
    """Configuration reader with validation"""
    def __init__(self, config_input: str):
        self.config = yaml.safe_load(config_input)
        self._validate_config()

    def _validate_config(self):
        if not isinstance(self.config, dict):
            raise ValueError("Configuration must be a dictionary")

        required_fields = ['tables', 's3_settings']
        if not all(field in self.config for field in required_fields):
            raise ValueError(f"Missing required fields: {required_fields}")

    def get_table_config(self, table_name: str) -> Dict:
        return self.config['tables'].get(table_name, {})

    def get_s3_config(self) -> Dict:
        return self.config['s3_settings']

class DataReader:
    """Handles reading data from Databricks tables"""
    def read_data(self, source_config: SourceConfig) -> DataFrame:
        if source_config.query:
            logger.info(f"Reading data using SQL query")
            return spark.sql(source_config.query)
        elif source_config.table_name:
            logger.info(f"Reading data from table: {source_config.table_name}")
            return spark.table(source_config.table_name)
        else:
            raise ValueError("Either query or table_name must be provided")

class DataWriter:
    """Handles writing data to S3"""
    def __init__(self, s3_config: Dict):
        self.s3_config = s3_config

    def write_data(self, df: DataFrame, write_config: WriteConfig) -> None:
        base_path = self.s3_config['base_path']
        full_path = f"{base_path}/{write_config.path}"

        writer = df.write.mode(write_config.mode)

        if write_config.format == 'delta':
            writer = writer.option("mergeSchema", "true")
            writer = writer.option("optimizeWrite", "true")

        if write_config.partition_by:
            writer = writer.partitionBy(write_config.partition_by)

        if write_config.options:
            for key, value in write_config.options.items():
                writer = writer.option(key, value)

        logger.info(f"Writing data to: {full_path}")
        writer.format(write_config.format).save(full_path)

class DataFlattener:
    """Handles flattening of nested structures"""
    def flatten_data(self, df: DataFrame, columns: List[str]) -> DataFrame:
        if not columns:
            return df

        result_df = df
        for column in columns:
            if column not in result_df.columns:
                logger.warning(f"Column '{column}' not found in data, skipping")
                continue

            field = [f for f in result_df.schema.fields if f.name == column][0]
            if isinstance(field.dataType, ArrayType):
                if isinstance(field.dataType.elementType, ArrayType):
                    logger.info(f"Flattening nested array: {column}")
                    result_df = result_df.withColumn(column, flatten(col(column)))
                logger.info(f"Exploding array: {column}")
                result_df = result_df.withColumn(column, explode(col(column)))

        return result_df

class BatchProcessor:
    """Handles batch processing of multiple tables"""
    def __init__(self, processor: 'DataProcessor'):
        self.processor = processor

    def process_tables(self, table_names: List[str]) -> Dict[str, str]:
        """Process multiple tables"""
        results = {}

        for table in table_names:
            try:
                logger.info(f"Processing table: {table}")

                # Get table configuration
                table_config = self.processor.config_reader.get_table_config(table)
                if not table_config:
                    raise ValueError(f"No configuration found for table: {table}")

                # Create configurations
                source_config = SourceConfig(**table_config.get('source', {}))
                write_config = WriteConfig(
                    path=table_config['s3_path'],
                    format=table_config.get('format', 'parquet'),
                    mode=table_config.get('mode', 'overwrite'),
                    partition_by=table_config.get('partition_by'),
                    options=table_config.get('options', {})
                )

                # Process data
                df = self.processor.reader.read_data(source_config)

                if table_config.get('flatten_columns'):
                    df = self.processor.flattener.flatten_data(
                        df,
                        table_config['flatten_columns']
                    )

                self.processor.writer.write_data(df, write_config)

                results[table] = "Success"
                logger.info(f"Successfully processed table: {table}")

            except Exception as e:
                results[table] = f"Failed: {str(e)}"
                logger.error(f"Error processing table {table}: {str(e)}")

        return results

class DataProcessor:
    """Main processor class"""
    def __init__(self, config_reader: ConfigReader):
        self.config_reader = config_reader
        self.reader = DataReader()
        self.writer = DataWriter(config_reader.get_s3_config())
        self.flattener = DataFlattener()

# Notebook usage example
# COMMAND ----------
# Create test table
spark.sql("""
CREATE TABLE IF NOT EXISTS customer_orders (
    order_id STRING,
    customer_id STRING,
    order_date DATE,
    items ARRAY<STRUCT<
        item_id: STRING,
        product_name: STRING,
        quantity: INT,
        price: DOUBLE
    >>,
    shipping_address STRUCT<
        street: STRING,
        city: STRING,
        state: STRING,
        zip: STRING
    >
) USING DELTA
""")

# COMMAND ----------
# Insert test data
spark.sql("""
INSERT INTO customer_orders
SELECT
    'ORD001' as order_id,
    'CUST001' as customer_id,
    current_date() as order_date,
    array(
        named_struct(
            'item_id', 'ITEM001',
            'product_name', 'Laptop',
            'quantity', 1,
            'price', 999.99
        ),
        named_struct(
            'item_id', 'ITEM002',
            'product_name', 'Mouse',
            'quantity', 2,
            'price', 29.99
        )
    ) as items,
    named_struct(
        'street', '123 Main St',
        'city', 'Boston',
        'state', 'MA',
        'zip', '02108'
    ) as shipping_address
""")

# COMMAND ----------
# Define configuration
config = """
tables:
  customer_orders_flat:
    source:
      table_name: customer_orders
    flatten_columns:
      - items
    s3_path: processed/customer_orders
    format: delta
    partition_by:
      - order_date
    options:
      mergeSchema: 'true'

s3_settings:
  base_path: '/tmp/test'  # For testing, using local path
"""

# Initialize processor
config_reader = ConfigReader(config)
processor = DataProcessor(config_reader)
batch_processor = BatchProcessor(processor)

# Process tables
results = batch_processor.process_tables(['customer_orders_flat'])

# Print results
for table, status in results.items():
    print(f"Table: {table} - Status: {status}")

# Verify results
result_df = spark.sql("SELECT * FROM delta.`/tmp/test/processed/customer_orders`")
print("\nFlattened Data Schema:")
result_df.printSchema()
print("\nFlattened Data Sample:")
result_df.show(2, False)