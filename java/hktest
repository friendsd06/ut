package com.example.s3upload.config;

import org.apache.tomcat.jdbc.pool.DataSource;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Profile;

import jakarta.annotation.PostConstruct;

/**
 * Configuration class for Databricks SQL Warehouse connections using token-based authentication
 */
@Configuration
public class DatabricksConfig {

    private static final Logger log = LoggerFactory.getLogger(DatabricksConfig.class);

    @Value("${databricks.sql.hostname}")
    private String hostname;

    @Value("${databricks.sql.http-path}")
    private String httpPath;

    @Value("${databricks.sql.access-token}")
    private String accessToken;

    @Value("${databricks.sql.catalog:hive_metastore}")
    private String catalog;

    @Value("${databricks.sql.schema:default}")
    private String schema;

    @Value("${databricks.sql.connection-pool.initial-size:5}")
    private int initialPoolSize;

    @Value("${databricks.sql.connection-pool.max-active:20}")
    private int maxActiveConnections;

    @Value("${databricks.sql.connection-pool.max-idle:10}")
    private int maxIdleConnections;

    @Value("${databricks.sql.connection-pool.min-idle:5}")
    private int minIdleConnections;

    @Value("${databricks.sql.connection-pool.max-wait:30000}")
    private int maxWaitMillis;

    @Value("${spring.profiles.active:dev}")
    private String activeProfile;

    @PostConstruct
    public void logConfiguration() {
        log.info("Initializing Databricks SQL configuration for environment: {}", activeProfile);
        log.info("Databricks Hostname: {}", hostname);
        log.info("Databricks HTTP Path: {}", httpPath);
        log.info("Databricks Catalog: {}", catalog);
        log.info("Databricks Schema: {}", schema);
        log.info("Connection Pool Configuration - Initial: {}, Max: {}, Min Idle: {}",
                initialPoolSize, maxActiveConnections, minIdleConnections);
    }

    /**
     * Creates and configures the Databricks SQL DataSource bean with token-based authentication
     * @return Configured DataSource for Databricks SQL connections
     */
    @Bean(name = "databricksDataSource")
    public DataSource databricksDataSource() {
        DataSource dataSource = new DataSource();

        // Basic connection properties
        dataSource.setDriverClassName("com.databricks.client.jdbc.Driver");

        // Build the JDBC URL and properties for token-based auth
        String jdbcUrl = String.format("jdbc:databricks://%s:443/default;httpPath=%s",
                hostname, httpPath);
        dataSource.setUrl(jdbcUrl);

        // NO username/password - using token-based auth instead

        // Databricks SQL specific properties for token-based auth
        dataSource.addConnectionProperty("AuthMech", "3"); // 3 = Username/Password auth mechanism
        dataSource.addConnectionProperty("httpPath", httpPath);
        dataSource.addConnectionProperty("SSL", "1"); // 1 = Enable SSL
        dataSource.addConnectionProperty("ConnCatalog", catalog);
        dataSource.addConnectionProperty("ConnSchema", schema);
        dataSource.addConnectionProperty("RowsFetchedPerBlock", "10000"); // Performance tuning
        dataSource.addConnectionProperty("EnableArrow", "1"); // Enable Arrow for better performance

        // Token-based authentication
        dataSource.addConnectionProperty("Password", ""); // Empty password
        dataSource.addConnectionProperty("PWD", ""); // Empty password alternative
        dataSource.addConnectionProperty("personalAccessToken", accessToken); // Use token

        // Connection pool settings
        dataSource.setInitialSize(initialPoolSize);
        dataSource.setMaxActive(maxActiveConnections);
        dataSource.setMaxIdle(maxIdleConnections);
        dataSource.setMinIdle(minIdleConnections);
        dataSource.setMaxWait(maxWaitMillis);

        // Connection validation
        dataSource.setValidationQuery("SELECT 1");
        dataSource.setTestOnBorrow(true);
        dataSource.setTestWhileIdle(true);
        dataSource.setValidationInterval(30000);

        // Avoid connection leaks
        dataSource.setRemoveAbandoned(true);
        dataSource.setRemoveAbandonedTimeout(60);
        dataSource.setLogAbandoned(true);

        // Additional options for performance
        dataSource.setTimeBetweenEvictionRunsMillis(30000);
        dataSource.setJdbcInterceptors(
                "org.apache.tomcat.jdbc.pool.interceptor.ConnectionState;"
                + "org.apache.tomcat.jdbc.pool.interceptor.StatementFinalizer");

        log.info("Databricks SQL DataSource configured successfully with token authentication");
        return dataSource;
    }

    /**
     * Creates a test-specific DataSource with different connection limits
     * Only active when the "test" profile is active
     */
    @Bean(name = "databricksTestDataSource")
    @Profile("test")
    public DataSource testDatabricksDataSource() {
        DataSource dataSource = databricksDataSource();

        // Use smaller connection pool for tests
        dataSource.setInitialSize(1);
        dataSource.setMaxActive(3);
        dataSource.setMaxIdle(2);
        dataSource.setMinIdle(1);

        log.info("Test-specific Databricks DataSource configured with reduced connection pool");
        return dataSource;
    }
}
====================================

package com.example.s3upload.web;

import com.example.s3upload.model.FileCompletionRequest;
import com.example.s3upload.model.FileCreationDetails;
import com.example.s3upload.service.FileService;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.concurrent.CompletableFuture;

/**
 * REST Controller with distinct endpoints for different file operations
 */
@RestController
@RequestMapping("/api/files")
public class FileController {

    private static final Logger log = LoggerFactory.getLogger(FileController.class);

    private final FileService fileService;

    public FileController(FileService fileService) {
        this.fileService = fileService;
    }

    /**
     * Endpoint to process complete file creation requests (key files and adjusted files)
     * This is a synchronous endpoint
     *
     * @param request File creation details
     * @return FileCompletionRequest with upload status
     */
    @PostMapping("/process")
    public ResponseEntity<FileCompletionRequest> processFile(@RequestBody FileCreationDetails request) {
        log.info("Received synchronous file processing request with ID: {}", request.requestId());

        try {
            FileCompletionRequest response = fileService.processFileRequestSync(request);
            log.info("Completed processing request ID: {}", request.requestId());
            return ResponseEntity.ok(response);
        } catch (Exception ex) {
            log.error("Error processing request ID: {}", request.requestId(), ex);
            return ResponseEntity.internalServerError().body(
                    new FileCompletionRequest(
                            request.requestId(),
                            request.requestType(),
                            request.cobDate(),
                            request.freq(),
                            false,
                            false,
                            false,
                            request.adjustData() != null && !request.adjustData().isEmpty(),
                            false,
                            request.keysData() != null && !request.keysData().isEmpty()
                    )
            );
        }
    }

    /**
     *
     * @param request File creation details containing keys
     * @return FileCompletionRequest with key persistence status
     */
    @PostMapping("/persist-keys")
    public ResponseEntity<FileCompletionRequest> persistKeys(@RequestBody FileCreationDetails request) {
        log.info("Received key persistence request with ID: {}", request.requestId());

        try {
            // Validate that keys exist
            if (request.keysData() == null || request.keysData().isEmpty()) {
                log.warn("No keys provided for request ID: {}", request.requestId());
                return ResponseEntity.badRequest().body(
                        new FileCompletionRequest(
                                request.requestId(),
                                request.requestType(),
                                request.cobDate(),
                                request.freq(),
                                false, false, // base file
                                false, false, // adjusted files
                                false, true   // key file (failure but was requested)
                        )
                );
            }

            // Directly use the reusable persistKeys method
            boolean keysSuccess = fileService.processKeyFile(request);
            log.info("Completed key persistence for request ID: {}, success: {}",
                    request.requestId(), keysSuccess);

            // Create response
            FileCompletionRequest response = new FileCompletionRequest(
                    request.requestId(),
                    request.requestType(),
                    request.cobDate(),
                    request.freq(),
                    false, false, // base file
                    false, false, // adjusted files
                    keysSuccess, true // key file
            );

            return ResponseEntity.ok(response);

        } catch (Exception ex) {
            log.error("Error persisting keys for request ID: {}", request.requestId(), ex);
            return ResponseEntity.internalServerError().body(
                    new FileCompletionRequest(
                            request.requestId(),
                            request.requestType(),
                            request.cobDate(),
                            request.freq(),
                            false, false, // base file
                            false, false, // adjusted files
                            false, true   // key file (requested but failed)
                    )
            );
        }
    }

    /**
     * Endpoint to create base file from databricks data
     * This is an asynchronous endpoint
     *
     * @param request File creation details with parameters for data retrieval
     * @return CompletableFuture with FileCompletionRequest
     */
    @PostMapping("/create-base-file")
    public CompletableFuture<ResponseEntity<FileCompletionRequest>> createBaseFile(
            @RequestBody FileCreationDetails request) {

        log.info("Received base file creation request with ID: {}", request.requestId());

        return fileService.processBaseFileAsync(request)
                .thenApply(response -> {
                    log.info("Completed base file creation for request ID: {}", request.requestId());
                    return ResponseEntity.ok(response);
                })
                .exceptionally(ex -> {
                    log.error("Error creating base file for request ID: {}", request.requestId(), ex);
                    return ResponseEntity.internalServerError().body(
                            new FileCompletionRequest(
                                    request.requestId(),
                                    request.requestType(),
                                    request.cobDate(),
                                    request.freq(),
                                    false,
                                    true,
                                    false,
                                    false,
                                    false,
                                    false
                            )
                    );
                });
    }

}
=======================================

package com.example.s3upload.service;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;
import software.amazon.awssdk.core.async.AsyncRequestBody;
import software.amazon.awssdk.services.s3.S3AsyncClient;
import software.amazon.awssdk.services.s3.model.*;

import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.file.Path;
import java.nio.file.StandardOpenOption;
import java.time.Duration;
import java.time.Instant;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;
import java.util.concurrent.*;
import java.util.stream.Collectors;

/**
 * Service responsible for uploading files to S3 using both simple and multipart upload strategies.
 * Provides optimized parallel multipart uploads for large files.
 */
@Service
public class FileUploadService {
    private static final Logger log = LoggerFactory.getLogger(FileUploadService.class);

    // S3 multipart upload constraints
    private static final long MIN_PART_SIZE_BYTES = 5 * 1024 * 1024; // 5MB
    private static final int MAX_PARTS = 10000;
    private static final int DEFAULT_MAX_RETRIES = 3;
    private static final long MB = 1024 * 1024;

    // Timeouts (in minutes)
    private static final int INITIATE_UPLOAD_TIMEOUT = 2;
    private static final int COMPLETE_UPLOAD_TIMEOUT = 5;
    private static final int ABORT_UPLOAD_TIMEOUT = 2;
    private static final int THREAD_POOL_SHUTDOWN_TIMEOUT_SECONDS = 30;

    @Value("${app.upload.max-threads:10}")
    private int maxUploadThreads;

    @Value("${app.upload.part-size-mb:5}")
    private int partSizeMb;

    @Value("${app.upload.timeout-minutes:30}")
    private int uploadTimeoutMinutes;

    private final S3AsyncClient s3Client;
    private ExecutorService uploadThreadPool;

    /**
     * Constructor with dependency injection for the S3 client.
     *
     * @param s3Client Configured S3 async client for AWS or LocalStack
     */
    public FileUploadService(S3AsyncClient s3Client) {
        this.s3Client = s3Client;
    }

    /**
     * Initialize the service after construction.
     */
    @PostConstruct
    public void initialize() {
        uploadThreadPool = Executors.newFixedThreadPool(maxUploadThreads);
        log.info("Initialized upload thread pool with {} threads", maxUploadThreads);
    }

    /**
     * Simple upload for small files (not using multipart upload).
     *
     * @param bucketName Target S3 bucket
     * @param objectKey Object key in S3
     * @param filePath Path to local file
     * @param fileSize Size of the file
     * @throws IOException If upload fails
     */
    public void uploadSimpleFile(String bucketName, String objectKey, Path filePath, long fileSize) throws IOException {
        log.info("Starting simple upload for file: {} ({} bytes) to {}/{}",
                filePath.getFileName(), fileSize, bucketName, objectKey);

        try {
            // Create the PutObject request
            PutObjectRequest putObjectRequest = PutObjectRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .build();

            // Upload the file
            s3Client.putObject(putObjectRequest, AsyncRequestBody.fromFile(filePath))
                    .get(uploadTimeoutMinutes, TimeUnit.MINUTES);

            log.info("Simple upload completed successfully for {}", objectKey);
        } catch (Exception e) {
            String errorMsg = "Simple upload failed for " + objectKey + ": " + e.getMessage();
            log.error(errorMsg);
            throw new IOException(errorMsg, e);
        }
    }

    /**
     * Upload a file with parallel part uploads for better performance.
     * Automatically calculates optimal part size and uses multiple threads for uploading.
     *
     * @param bucketName Target S3 bucket
     * @param objectKey Object key in S3
     * @param filePath Path to local file
     * @throws Exception If upload fails
     */
    public void uploadFileParallel(String bucketName, String objectKey, Path filePath) throws Exception {
        Instant startTime = Instant.now();
        log.info("Starting parallel multipart upload for {}", objectKey);

        // Step 1: Calculate optimal part configuration
        ParallelUploadConfig config = calculateUploadConfig(filePath);
        logUploadConfiguration(config, objectKey);

        // Step 2: Initiate multipart upload
        String uploadId = initiateMultipartUpload(bucketName, objectKey);
        log.info("Multipart upload initiated with ID: {}", uploadId);

        try {
            // Step 3: Upload all parts in parallel
            List<CompletedPart> completedParts = uploadAllParts(
                    bucketName, objectKey, uploadId, filePath, config);

            // Step 4: Complete multipart upload
            completeMultipartUpload(bucketName, objectKey, uploadId, completedParts);

            // Log upload statistics
            logUploadStatistics(startTime, objectKey, config);
        } catch (Exception e) {
            // Abort upload on failure
            log.error("Multipart upload failed: {}", e.getMessage());
            abortMultipartUpload(bucketName, objectKey, uploadId);
            throw new IOException("Failed to upload file " + objectKey, e);
        }
    }

    /**
     * Uploads all parts of a file in parallel and returns the completed parts.
     */
    private List<CompletedPart> uploadAllParts(
            String bucketName, String objectKey, String uploadId,
            Path filePath, ParallelUploadConfig config) throws Exception {

        log.debug("Creating upload tasks for all {} parts...", config.numParts);
        List<CompletableFuture<CompletedPart>> uploadFutures = new ArrayList<>();

        // Create futures for each part upload
        for (int partNumber = 1; partNumber <= config.numParts; partNumber++) {
            PartUploadTask task = createPartUploadTask(
                    partNumber, config.partSize, config.fileSize);

            CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(
                    () -> uploadPartWithRetry(bucketName, objectKey, uploadId, filePath,
                            task, DEFAULT_MAX_RETRIES),
                    uploadThreadPool);

            uploadFutures.add(future);
        }

        log.info("All upload tasks created. Waiting for completion...");

        // Wait for all uploads to complete
        CompletableFuture<Void> allUploadsFuture = CompletableFuture.allOf(
                uploadFutures.toArray(new CompletableFuture[0]));

        allUploadsFuture.get(uploadTimeoutMinutes, TimeUnit.MINUTES);
        log.info("All part uploads completed successfully");

        // Collect and return all completed parts
        return uploadFutures.stream()
                .map(CompletableFuture::join)
                .collect(Collectors.toList());
    }

    /**
     * Creates a task description for uploading a specific part.
     */
    private PartUploadTask createPartUploadTask(int partNumber, long partSize, long fileSize) {
        final long position = (partNumber - 1) * partSize;
        final long size = Math.min(partSize, fileSize - position);

        return new PartUploadTask(partNumber, position, size);
    }

    /**
     * Uploads a single part with retries in case of failure.
     */
    private CompletedPart uploadPartWithRetry(
            String bucketName, String objectKey, String uploadId,
            Path filePath, PartUploadTask task, int maxRetries) {

        String threadName = Thread.currentThread().getName();
        log.debug("Thread {}: Starting upload of part {} (bytes {} to {})",
                threadName, task.partNumber, task.position, task.position + task.size - 1);

        int retryCount = 0;
        Exception lastException = null;

        while (retryCount <= maxRetries) {
            try {
                if (retryCount > 0) {
                    log.debug("Retry #{} for part {}", retryCount, task.partNumber);
                }

                CompletedPart part = uploadSinglePart(
                        bucketName, objectKey, uploadId, filePath, task);

                log.debug("Thread {}: Completed upload of part {}, ETag: {}",
                        threadName, task.partNumber, part.eTag());

                return part;

            } catch (Exception e) {
                lastException = e;
                retryCount++;

                if (retryCount > maxRetries) {
                    log.error("Part {} failed after {} retries: {}",
                            task.partNumber, retryCount, e.getMessage());
                    break;
                }

                // Calculate exponential backoff time
                long backoffMillis = (long) (Math.pow(2, retryCount) * 100);
                log.debug("Part {} upload failed, retrying in {} ms. Error: {}",
                        task.partNumber, backoffMillis, e.getMessage());

                try {
                    Thread.sleep(backoffMillis);
                } catch (InterruptedException ie) {
                    Thread.currentThread().interrupt();
                    throw new RuntimeException("Thread interrupted during backoff", ie);
                }
            }
        }

        // If we got here, we exhausted all retries
        throw new RuntimeException("Failed to upload part " + task.partNumber, lastException);
    }

    /**
     * Upload a single part of the file.
     */
    private CompletedPart uploadSinglePart(
            String bucketName, String objectKey, String uploadId,
            Path filePath, PartUploadTask task) throws IOException {

        // Read this part from the file
        ByteBuffer partData = readFileSegment(filePath, task.position, task.size);

        // Create and execute upload request
        UploadPartRequest uploadPartRequest = UploadPartRequest.builder()
                .bucket(bucketName)
                .key(objectKey)
                .uploadId(uploadId)
                .partNumber(task.partNumber)
                .contentLength(task.size)
                .build();

        try {
            UploadPartResponse response = s3Client.uploadPart(
                            uploadPartRequest, AsyncRequestBody.fromByteBuffer(partData))
                    .get(uploadTimeoutMinutes, TimeUnit.MINUTES);

            return CompletedPart.builder()
                    .partNumber(task.partNumber)
                    .eTag(response.eTag())
                    .build();

        } catch (Exception e) {
            throw new IOException("Failed to upload part " + task.partNumber + ": " + e.getMessage(), e);
        }
    }

    /**
     * Calculate optimal part size and count based on file size.
     * Returns a configuration object with upload parameters.
     */
    private ParallelUploadConfig calculateUploadConfig(Path filePath) throws IOException {
        long fileSize = filePath.toFile().length();

        // Start with configured or minimum part size
        long partSize = Math.max(partSizeMb * MB, MIN_PART_SIZE_BYTES);

        // If file is large enough to exceed max parts limit, increase part size
        if (fileSize > partSize * MAX_PARTS) {
            // Calculate minimum required part size to stay under part count limit
            partSize = (fileSize + MAX_PARTS - 1) / MAX_PARTS; // Ceiling division

            // Round up to nearest MB for cleaner numbers
            partSize = ((partSize + MB - 1) / MB) * MB; // Round up to nearest MB
        }

        // Calculate part count based on part size
        long partCount = (fileSize + partSize - 1) / partSize; // Ceiling division

        return new ParallelUploadConfig(fileSize, partSize, partCount);
    }

    /**
     * Initiate a multipart upload in S3.
     */
    private String initiateMultipartUpload(String bucketName, String objectKey) {
        try {
            CreateMultipartUploadRequest request = CreateMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .contentType("application/octet-stream")
                    .build();

            CreateMultipartUploadResponse response = s3Client.createMultipartUpload(request)
                    .get(INITIATE_UPLOAD_TIMEOUT, TimeUnit.MINUTES);

            return response.uploadId();
        } catch (Exception e) {
            throw new RuntimeException("Failed to initiate multipart upload: " + e.getMessage(), e);
        }
    }

    /**
     * Complete the multipart upload by combining all uploaded parts.
     */
    private void completeMultipartUpload(
            String bucketName, String objectKey, String uploadId, List<CompletedPart> parts) {

        try {
            // Sort parts by part number
            List<CompletedPart> sortedParts = parts.stream()
                    .sorted(Comparator.comparingInt(CompletedPart::partNumber))
                    .collect(Collectors.toList());

            CompleteMultipartUploadRequest completeRequest = CompleteMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .uploadId(uploadId)
                    .multipartUpload(CompletedMultipartUpload.builder()
                            .parts(sortedParts)
                            .build())
                    .build();

            CompleteMultipartUploadResponse response = s3Client.completeMultipartUpload(completeRequest)
                    .get(COMPLETE_UPLOAD_TIMEOUT, TimeUnit.MINUTES);

            log.info("Multipart upload completed successfully! Object: {}, ETag: {}",
                    response.key(), response.eTag());

        } catch (Exception e) {
            throw new RuntimeException("Failed to complete multipart upload: " + e.getMessage(), e);
        }
    }

    /**
     * Abort a multipart upload.
     */
    private void abortMultipartUpload(String bucketName, String objectKey, String uploadId) {
        try {
            AbortMultipartUploadRequest abortRequest = AbortMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .uploadId(uploadId)
                    .build();

            s3Client.abortMultipartUpload(abortRequest).get(ABORT_UPLOAD_TIMEOUT, TimeUnit.MINUTES);
            log.info("Multipart upload aborted successfully");
        } catch (Exception e) {
            // Just log error since this is already in an error path
            log.error("Failed to abort multipart upload: {}", e.getMessage());
        }
    }

    /**
     * Read a specific segment from the file.
     */
    private ByteBuffer readFileSegment(Path filePath, long position, long size) throws IOException {
        ByteBuffer buffer = ByteBuffer.allocate((int) size);

        try (FileChannel channel = FileChannel.open(filePath, StandardOpenOption.READ)) {
            channel.position(position);

            int bytesRead;
            int totalRead = 0;

            // Read until buffer is full or EOF
            while (totalRead < size && (bytesRead = channel.read(buffer)) != -1) {
                totalRead += bytesRead;
            }

            if (totalRead < size) {
                log.warn("Read {} bytes but expected {} bytes", totalRead, size);
            }

            buffer.flip();
            return buffer;
        }
    }

    /**
     * Log the upload configuration.
     */
    private void logUploadConfiguration(ParallelUploadConfig config, String objectKey) {
        log.info("Upload configuration for {}:", objectKey);
        log.info("  - File size: {} bytes ({} MB)",
                config.fileSize, config.fileSize / MB);
        log.info("  - Part size: {} bytes ({} MB)",
                config.partSize, config.partSize / MB);
        log.info("  - Number of parts: {}", config.numParts);
        log.info("  - Thread pool size: {}", maxUploadThreads);
    }

    /**
     * Log upload statistics at the end of an upload.
     */
    private void logUploadStatistics(Instant startTime, String objectKey, ParallelUploadConfig config) {
        Duration uploadTime = Duration.between(startTime, Instant.now());
        double throughputMBps = (double) config.fileSize / (uploadTime.toMillis() * 1000) * MB;

        log.info("Upload statistics for {}:", objectKey);
        log.info("  - Total time: {}.{} seconds",
                uploadTime.getSeconds(), uploadTime.getNano() / 1000000);
        log.info("  - Throughput: {:.2f} MB/s", throughputMBps);
    }

    /**
     * Clean up resources on service shutdown.
     */
    @PreDestroy
    public void cleanup() {
        log.info("Shutting down FileUploadService resources");

        if (uploadThreadPool != null && !uploadThreadPool.isShutdown()) {
            try {
                uploadThreadPool.shutdown();
                if (!uploadThreadPool.awaitTermination(THREAD_POOL_SHUTDOWN_TIMEOUT_SECONDS, TimeUnit.SECONDS)) {
                    log.warn("Upload thread pool did not terminate in allowed time - forcing shutdown");
                    uploadThreadPool.shutdownNow();
                }
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                log.warn("Upload thread pool shutdown interrupted", e);
                uploadThreadPool.shutdownNow();
            }
        }
    }

    /**
     * Configuration class for parallel uploads.
     */
    private static class ParallelUploadConfig {
        final long fileSize;
        final long partSize;
        final long numParts;

        ParallelUploadConfig(long fileSize, long partSize, long numParts) {
            this.fileSize = fileSize;
            this.partSize = partSize;
            this.numParts = numParts;
        }
    }

    /**
     * Task description for a single part upload.
     */
    private static class PartUploadTask {
        final int partNumber;
        final long position;
        final long size;

        PartUploadTask(int partNumber, long position, long size) {
            this.partNumber = partNumber;
            this.position = position;
            this.size = size;
        }
    }
}
================================================
package com.example.s3upload.service;

import com.example.s3upload.model.FileCompletionRequest;
import com.example.s3upload.model.FileCreationDetails;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;
import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.*;

/**
 * Service responsible for processing file creation requests and managing
 * uploads to S3 storage
 */
@Service
public class FileService {

    private static final Logger log = LoggerFactory.getLogger(FileService.class);
    private static final long FILE_SIZE_THRESHOLD_MB = 10;
    private static final long FILE_SIZE_THRESHOLD_BYTES = FILE_SIZE_THRESHOLD_MB * 1024 * 1024;
    private static final int UPLOAD_TIMEOUT_MINUTES = 30;

    @Value("${app.upload.directory:D://data}")
    private String baseDirectoryPath;

    @Value("${app.s3.bucket-name}")
    private String bucketName;

    @Value("${spring.profiles.active:dev}")
    private String activeProfile;

    private final FileUploadService fileUploadService;
    private final DatabricksSqlService databricksService;
    private final ExecutorService fileProcessingExecutor;

    /**
     * Constructor with dependency injection
     */
    public FileService(FileUploadService fileUploadService, DatabricksSqlService databricksService) {
        this.fileUploadService = fileUploadService;
        this.databricksService = databricksService;
        this.fileProcessingExecutor = Executors.newFixedThreadPool(
                Runtime.getRuntime().availableProcessors());
    }

    /**
     * Initialize the service after construction
     */
    @PostConstruct
    public void initialize() {
        try {
            createDirectoryStructure();
            log.info("Directory structure created successfully at {}", baseDirectoryPath);
        } catch (IOException e) {
            log.error("Failed to create directory structure: {}", e.getMessage(), e);
        }
    }

    /**
     * Clean up resources when the service is being destroyed
     */
    @PreDestroy
    public void cleanup() {
        fileProcessingExecutor.shutdown();
        try {
            if (!fileProcessingExecutor.awaitTermination(1, TimeUnit.MINUTES)) {
                fileProcessingExecutor.shutdownNow();
            }
        } catch (InterruptedException e) {
            fileProcessingExecutor.shutdownNow();
            Thread.currentThread().interrupt();
        }
    }

    /**
     * Creates the required directory structure for file operations
     */
    private void createDirectoryStructure() throws IOException {
        // Create base directory
        Files.createDirectories(Paths.get(baseDirectoryPath));

        // Get execution environment
        String env = getExecutionEnvironment();

        // Create all required directories
        String basePath = baseDirectoryPath + "/abinitio/" + env + "/adjustment";
        Files.createDirectories(Paths.get(basePath));
        Files.createDirectories(Paths.get(basePath + "/base"));
        Files.createDirectories(Paths.get(basePath + "/adjusted"));
        Files.createDirectories(Paths.get(basePath + "/adjusted/keyvalues"));
    }

    /**
     * Creates a successful completion response
     */
    private FileCompletionRequest createCompletionResponse(
            FileCreationDetails request,
            boolean baseFileSuccess,
            boolean isBaseFile,
            boolean adjFileSuccess,
            boolean isAdjFile,
            boolean keyFileSuccess,
            boolean isKeyFile) {

        return new FileCompletionRequest(
                request.requestId(),
                request.requestType(),
                request.cobDate(),
                request.freq(),
                baseFileSuccess,
                isBaseFile,
                adjFileSuccess,
                isAdjFile,
                keyFileSuccess,
                isKeyFile
        );
    }

    /**
     * Creates an error response when file processing fails
     */
    private FileCompletionRequest createErrorResponse(
            FileCreationDetails request,
            boolean isBaseFile,
            boolean isAdjFile,
            boolean isKeyFile) {

        return new FileCompletionRequest(
                request.requestId(),
                request.requestType(),
                request.cobDate(),
                request.freq(),
                false,
                isBaseFile,
                false,
                isAdjFile,
                false,
                isKeyFile
        );
    }

    /**
     * API ENDPOINT 1: Process complete file request (synchronous)
     * Handles both key files and adjusted files (no base file processing)
     *
     * @param request File creation details
     * @return FileCompletionRequest with upload status
     */
    public FileCompletionRequest processFileRequestSync(FileCreationDetails request) {
        log.info("Processing synchronous file request with ID: {}", request.requestId());

        // Declare variables outside the try block so they're accessible in the catch block
        boolean isKeyFile = false;
        boolean isAdjFile = false;

        try {
            // Determine which files need to be created
            isKeyFile = request.keysData() != null && !request.keysData().isEmpty();
            isAdjFile = request.adjustData() != null && !request.adjustData().isEmpty();

            // Results tracking
            boolean keyFileSuccess = !isKeyFile; // true if not required
            boolean adjFileSuccess = !isAdjFile; // true if not required

            // Process key file if required
            if (isKeyFile) {
                keyFileSuccess = processKeyFile(request);
                log.info("Key file processing result for request {}: {}",
                        request.requestId(), keyFileSuccess ? "SUCCESS" : "FAILED");
            }

            // Process adjusted files if required
            if (isAdjFile) {
                adjFileSuccess = processAdjustedFiles(request);
                log.info("Adjusted files processing result for request {}: {}",
                        request.requestId(), adjFileSuccess ? "SUCCESS" : "FAILED");
            }

            // Create and return response
            return createCompletionResponse(
                    request,
                    false,  // baseFileSuccess (always false, not processing)
                    false,  // isBaseFile (always false, not processing)
                    adjFileSuccess,
                    isAdjFile,
                    keyFileSuccess,
                    isKeyFile
            );

        } catch (Exception e) {
            log.error("Error processing file request {}: {}", request.requestId(), e.getMessage(), e);
            return createErrorResponse(request, false, isAdjFile, isKeyFile);
        }
    }


    /**
     * API ENDPOINT 3: Create base file only (asynchronous)
     * Fetches data from database and creates the base file
     *
     * @param request File creation details
     * @return CompletableFuture with the completion response
     */
    public CompletableFuture<FileCompletionRequest> processBaseFileAsync(FileCreationDetails request) {
        log.info("Processing base file asynchronously for request ID: {}", request.requestId());

        return CompletableFuture.supplyAsync(() -> {
            try {
                // Fetch data from database in the same format as adjusted data
                Map<String, List<String>> baseData = fetchDataForBaseFile(request);
                log.info("Fetched base data with {} entities for request {}",
                        baseData.size(), request.requestId());

                // Process base file with the fetched data - reusing the adjusted files pattern
                boolean baseFileSuccess = processBaseFiles(request, baseData);
                log.info("Base file processing result for request {}: {}",
                        request.requestId(), baseFileSuccess ? "SUCCESS" : "FAILED");

                // Create and return response
                return createCompletionResponse(
                        request,
                        baseFileSuccess, true, // base file
                        false, false,          // adjusted files
                        false, false           // key file
                );

            } catch (Exception e) {
                log.error("Error creating base file for request {}: {}",
                        request.requestId(), e.getMessage(), e);
                return createErrorResponse(request, true, false, false);
            }
        }, fileProcessingExecutor);
    }


    /**
     * Process key file - create and upload to S3
     * Creates a zero-byte file if keysData is empty
     *
     * @param request File creation details
     * @return true if successful, false otherwise
     */
    public boolean processKeyFile(FileCreationDetails request) {
        try {
            log.info("Creating key file for request: {}", request.requestId());

            // Create file paths
            String localFilePath = baseDirectoryPath + "/logicRcrdID_" + request.requestId() + ".dat";
            String s3ObjectKey = buildS3ObjectKey(request.requestId(), "adjusted/keyvalues/keys.dat");

            log.info("Key file local path: {}", localFilePath);
            log.info("Key file S3 object key: {}", s3ObjectKey);

            // Create and write to file
            Path filePath = createLocalFile(localFilePath, request.keysData());

            // Upload file to S3
            return uploadFileToS3(request.requestId(), "key file", filePath, s3ObjectKey);

        } catch (Exception e) {
            log.error("Failed to create or upload key file for request {}: {}",
                    request.requestId(), e.getMessage(), e);
            return false;
        }
    }

    /**
     * Process all adjusted files - create and upload to S3
     * Creates zero-byte files if entity data is empty
     *
     * @param request File creation details
     * @return true if all files uploaded successfully, false if any fail
     */
    private boolean processAdjustedFiles(FileCreationDetails request) {
        if (request.adjustData() == null || request.adjustData().isEmpty()) {
            log.info("No adjust data to process for request: {}", request.requestId());
            return true;
        }
        return processEntityFiles(request, request.adjustData(), "adjusted", "adjusted file");
    }

    /**
     * Process base files - create and upload to S3
     * Reuses the same pattern as adjusted files
     *
     * @param request File creation details
     * @param baseData Map of base data entities
     * @return true if all files uploaded successfully, false if any fail
     */
    private boolean processBaseFiles(FileCreationDetails request, Map<String, List<String>> baseData) {
        if (baseData == null || baseData.isEmpty()) {
            log.info("No base data to process for request: {}", request.requestId());
            return false; // Return false for empty base data since it's required
        }
        return processEntityFiles(request, baseData, "base", "base file");
    }

    /**
     * Generic method to process entity files (adjusted or base)
     * Used by both adjusted and base file processing
     *
     * @param request File creation details
     * @param entityData Map of entity data
     * @param pathSegment Path segment for S3 storage ("adjusted" or "base")
     * @param fileDescription Description for logging ("adjusted file" or "base file")
     * @return true if all files uploaded successfully, false if any fail
     */
    private boolean processEntityFiles(FileCreationDetails request,
                                       Map<String, List<String>> entityData,
                                       String pathSegment,
                                       String fileDescription) {

        List<CompletableFuture<Boolean>> futures = new ArrayList<>();

        // Process each entity file in parallel
        for (Map.Entry<String, List<String>> entry : entityData.entrySet()) {
            String entityName = entry.getKey();
            List<String> data = entry.getValue();

            CompletableFuture<Boolean> future = CompletableFuture.supplyAsync(() ->
                            processEntityFile(request, entityName, data, pathSegment, fileDescription),
                    fileProcessingExecutor);

            futures.add(future);
        }

        // Wait for all futures to complete
        try {
            CompletableFuture<Void> allDone = CompletableFuture.allOf(
                    futures.toArray(new CompletableFuture[0]));
            allDone.get(UPLOAD_TIMEOUT_MINUTES, TimeUnit.MINUTES);

            // Check if any failed
            return futures.stream().allMatch(f -> {
                try {
                    return f.get();
                } catch (Exception e) {
                    return false;
                }
            });

        } catch (Exception e) {
            log.error("Error waiting for {} processing: {}", fileDescription, e.getMessage(), e);
            return false;
        }
    }

    /**
     * Process a single entity file
     * Used by both adjusted and base file processing
     */
    private boolean processEntityFile(FileCreationDetails request, String entityName,
                                      List<String> data, String pathSegment, String fileDescription) {
        try {
            log.info("Creating {} for entity: {} (request ID: {})",
                    fileDescription, entityName, request.requestId());

            // Clean entity name
            String cleanEntityName = entityName.toLowerCase().replace("_vue", "");

            // Create file paths
            String localFilePath = baseDirectoryPath + "/" + cleanEntityName + "_" +
                    pathSegment + "_" + request.requestId() + ".dat";
            String s3ObjectKey = buildS3ObjectKey(request.requestId(),
                    pathSegment + "/" + cleanEntityName + ".dat");

            log.info("{} local path for {}: {}", fileDescription, entityName, localFilePath);
            log.info("{} S3 object key for {}: {}", fileDescription, entityName, s3ObjectKey);

            // Create and write to file
            Path filePath = createLocalFile(localFilePath, data);

            // Upload file to S3
            return uploadFileToS3(request.requestId(),
                    fileDescription + " for entity " + entityName,
                    filePath, s3ObjectKey);

        } catch (Exception e) {
            log.error("Failed to create or upload {} for entity {} (request {}): {}",
                    fileDescription, entityName, request.requestId(), e.getMessage(), e);
            return false;
        }
    }

    /**
     * Fetch data from database for base file
     * Formats the data in the same structure as adjusted data (Map<String, List<String>>)
     * Returns hardcoded data for 5 entities: customer_base, account_base, transaction_base,
     * balance_base, and position_base
     *
     * @param request File creation details
     * @return Map of entity names to lists of data records
     */
    private Map<String, List<String>> fetchDataForBaseFile(FileCreationDetails request) {
        log.info("Fetching data for base file, request ID: {}", request.requestId());

        // This implementation uses hardcoded data for 5 base entities
        Map<String, List<String>> result = new HashMap<>();
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
        String requestId = request.requestId();

        // 1. Generate customer data
        List<String> customerData = new ArrayList<>();
        customerData.add("CUST001," + requestId + ",John Doe,ACTIVE," + timestamp + ",NORTH");
        customerData.add("CUST002," + requestId + ",Jane Smith,ACTIVE," + timestamp + ",SOUTH");
        customerData.add("CUST003," + requestId + ",Bob Johnson,INACTIVE," + timestamp + ",EAST");
        customerData.add("CUST004," + requestId + ",Alice Brown,ACTIVE," + timestamp + ",WEST");
        customerData.add("CUST005," + requestId + ",Charlie Davis,PENDING," + timestamp + ",NORTH");
        result.put("customer_base", customerData);

        // 2. Generate account data
        List<String> accountData = new ArrayList<>();
        accountData.add("ACC001,CUST001,SAVINGS,10000.50," + timestamp);
        accountData.add("ACC002,CUST001,CHECKING,5000.75," + timestamp);
        accountData.add("ACC003,CUST002,SAVINGS,25000.00," + timestamp);
        accountData.add("ACC004,CUST003,CHECKING,1500.25," + timestamp);
        accountData.add("ACC005,CUST004,SAVINGS,30000.00," + timestamp);
        result.put("account_base", accountData);

        // 3. Generate transaction data
        List<String> transactionData = new ArrayList<>();
        transactionData.add("TRX001,ACC001,DEPOSIT,500.00," + timestamp);
        transactionData.add("TRX002,ACC002,WITHDRAWAL,150.50," + timestamp);
        transactionData.add("TRX003,ACC003,DEPOSIT,1000.00," + timestamp);
        transactionData.add("TRX004,ACC002,TRANSFER,300.25," + timestamp);
        transactionData.add("TRX005,ACC004,WITHDRAWAL,200.00," + timestamp);
        result.put("transaction_base", transactionData);

        // 4. Generate balance data
        List<String> balanceData = new ArrayList<>();
        balanceData.add("BAL001,ACC001,10500.50,10500.50," + timestamp);
        balanceData.add("BAL002,ACC002,4850.25,4850.25," + timestamp);
        balanceData.add("BAL003,ACC003,26000.00,26000.00," + timestamp);
        balanceData.add("BAL004,ACC004,1300.25,1300.25," + timestamp);
        balanceData.add("BAL005,ACC005,30000.00,30000.00," + timestamp);
        result.put("balance_base", balanceData);

        // 5. Generate position data
        List<String> positionData = new ArrayList<>();
        positionData.add("POS001,ACC001,AAPL,10,1500.00," + timestamp);
        positionData.add("POS002,ACC001,MSFT,15,2500.00," + timestamp);
        positionData.add("POS003,ACC003,GOOGL,5,7500.00," + timestamp);
        positionData.add("POS004,ACC004,AMZN,2,3000.00," + timestamp);
        positionData.add("POS005,ACC005,TSLA,20,10000.00," + timestamp);
        result.put("position_base", positionData);

        log.info("Generated data with {} entities for base files, request ID: {}",
                result.size(), requestId);

        return result;
    }

    /**
     * Create a sample base data structure when database is not available
     */
    private Map<String, List<String>> createSampleBaseData(FileCreationDetails request) {
        Map<String, List<String>> result = new HashMap<>();
        result.put("base_data", generateSampleBaseFileData(request));
        return result;
    }


    /**
     * Create a local file and write data to it
     *
     * @param filePath Path to create the file at
     * @param data Data to write (if null or empty, creates a zero-byte file)
     * @return Path object for the created file
     */
    private Path createLocalFile(String filePath, List<String> data) throws IOException {
        // Ensure directory exists
        Path directory = Paths.get(filePath).getParent();
        if (directory != null) {
            Files.createDirectories(directory);
        }

        // Create the file
        Path path = Paths.get(filePath);

        if (data == null || data.isEmpty()) {
            // Create empty (zero-byte) file if no data
            log.info("Creating zero-byte file as data is empty: {}", filePath);
            Files.write(path, new byte[0],
                    StandardOpenOption.CREATE,
                    StandardOpenOption.TRUNCATE_EXISTING);
        } else {
            // Normal case - write data to file
            String content = String.join(System.lineSeparator(), data);
            Files.write(path, content.getBytes(),
                    StandardOpenOption.CREATE,
                    StandardOpenOption.TRUNCATE_EXISTING);
        }

        log.info("File created: {}", filePath);
        return path;
    }

    /**
     * Upload a file to S3
     *
     * @param requestId Request ID for logging
     * @param fileDescription Description of file for logging
     * @param filePath Local file path
     * @param s3ObjectKey S3 object key
     * @return true if successful, false otherwise
     */
    private boolean uploadFileToS3(String requestId, String fileDescription, Path filePath, String s3ObjectKey) {
        // Simulate failure for specific request IDs
        if (requestId.equals("REQ1") || requestId.equals("REQ2") || requestId.equals("REQ8")) {
            log.error("Simulated failure for {} upload with request ID: {}", fileDescription, requestId);
            return false;
        }
        try {
            // Get file size
            long fileSize = Files.size(filePath);

            // Choose upload method based on file size
            if (fileSize < FILE_SIZE_THRESHOLD_BYTES) {
                fileUploadService.uploadSimpleFile(bucketName, s3ObjectKey, filePath, fileSize);
            } else {
                fileUploadService.uploadFileParallel(bucketName, s3ObjectKey, filePath);
            }

            log.info("{} uploaded successfully for request: {}",
                    capitalizeFirstLetter(fileDescription), requestId);
            return true;
        } catch (Exception e) {
            log.error("Failed to upload {} for request {}: {}",
                    fileDescription, requestId, e.getMessage(), e);
            return false;
        } finally {
            // Optionally clean up local file - uncomment if needed
            // try {
            //     Files.deleteIfExists(filePath);
            //     log.debug("Deleted local file: {}", filePath);
            // } catch (IOException e) {
            //     log.warn("Failed to delete local file {}: {}", filePath, e.getMessage());
            // }
        }
    }

    /**
     * Build the S3 object key for a file
     *
     * @param requestId Request ID
     * @param objectPath Path portion of the object key
     * @return Complete S3 object key
     */
    private String buildS3ObjectKey(String requestId, String objectPath) {
        String env = getExecutionEnvironment();
        return "data/abinitio/" + env + "/adjustment/" + requestId + "/" + objectPath;
    }

    /**
     * Generate sample data for base file (5 records)
     * In a real implementation, this would fetch data from a database
     *
     * @param request File creation details
     * @return List of sample data records
     */
    private List<String> generateSampleBaseFileData(FileCreationDetails request) {
        List<String> records = new ArrayList<>();

        // Get current timestamp for the records
        String timestamp = LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));

        // Add 5 sample records
        records.add("ID001," + request.requestId() + ",FIELD1_VALUE," + timestamp + ",ACTIVE");
        records.add("ID002," + request.requestId() + ",FIELD2_VALUE," + timestamp + ",INACTIVE");
        records.add("ID003," + request.requestId() + ",FIELD3_VALUE," + timestamp + ",PENDING");
        records.add("ID004," + request.requestId() + ",FIELD4_VALUE," + timestamp + ",ACTIVE");
        records.add("ID005," + request.requestId() + ",FIELD5_VALUE," + timestamp + ",COMPLETE");

        return records;
    }

    /**
     * Get execution environment based on active profile
     *
     * @return Execution environment string
     */
    private String getExecutionEnvironment() {
        if (activeProfile == null) {
            return "dev";
        }

        switch (activeProfile.toLowerCase()) {
            case "test":
            case "uat":
                return "uat";
            case "preprod":
                return "pre";
            case "prod":
                return "prd";
            default:
                return "dev";
        }
    }

    /**
     * Sanitize table name to prevent SQL injection
     */
    private String sanitizeTableName(String tableName) {
        return tableName.replaceAll("[^a-zA-Z0-9_.]", "");
    }

    /**
     * Utility method to capitalize the first letter of a string
     */
    private String capitalizeFirstLetter(String input) {
        if (input == null || input.isEmpty()) {
            return input;
        }
        return input.substring(0, 1).toUpperCase() + input.substring(1);
    }
}
====================

package com.example.s3upload.config;

import org.apache.ibatis.session.SqlSessionFactory;
import org.mybatis.spring.SqlSessionFactoryBean;
import org.mybatis.spring.SqlSessionTemplate;
import org.mybatis.spring.annotation.MapperScan;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.core.io.support.PathMatchingResourcePatternResolver;
import org.springframework.jdbc.datasource.DataSourceTransactionManager;
import org.springframework.jdbc.datasource.SimpleDriverDataSource;
import org.springframework.transaction.PlatformTransactionManager;
import org.springframework.transaction.annotation.EnableTransactionManagement;

import javax.sql.DataSource;
import jakarta.annotation.PostConstruct;
import java.util.Properties;

/**
 * Configuration class for Databricks SQL Warehouse connections using token-based authentication
 * Configured for MyBatis integration without connection pooling
 */
@Configuration
@EnableTransactionManagement
@MapperScan(basePackages = "com.example.s3upload.mapper")
public class DatabricksConfig {

    private static final Logger log = LoggerFactory.getLogger(DatabricksConfig.class);

    @Value("${databricks.sql.hostname}")
    private String hostname;

    @Value("${databricks.sql.http-path}")
    private String httpPath;

    @Value("${databricks.sql.access-token}")
    private String accessToken;

    @Value("${databricks.sql.catalog:hive_metastore}")
    private String catalog;

    @Value("${databricks.sql.schema:default}")
    private String schema;

    @Value("${mybatis.mapper-locations:classpath:mapper/**/*.xml}")
    private String mapperLocations;

    @Value("${mybatis.type-aliases-package:com.example.s3upload.model}")
    private String typeAliasesPackage;

    @Value("${spring.profiles.active:dev}")
    private String activeProfile;

    @PostConstruct
    public void logConfiguration() {
        log.info("Initializing Databricks SQL configuration with MyBatis for environment: {}", activeProfile);
        log.info("Databricks Hostname: {}", hostname);
        log.info("Databricks HTTP Path: {}", httpPath);
        log.info("Databricks Catalog: {}", catalog);
        log.info("Databricks Schema: {}", schema);
        log.info("MyBatis Mapper Locations: {}", mapperLocations);
        log.info("MyBatis Type Aliases Package: {}", typeAliasesPackage);
    }

    /**
     * Creates and configures a simple Databricks SQL DataSource
     * with token-based authentication (no connection pooling)
     * @return Configured DataSource for Databricks SQL connections
     */
    @Bean
    @Primary
    public DataSource dataSource() {
        SimpleDriverDataSource dataSource = new SimpleDriverDataSource();

        try {
            // Set JDBC driver
            dataSource.setDriverClass(
                (Class<? extends java.sql.Driver>) Class.forName("com.databricks.client.jdbc.Driver")
            );

            // Build the JDBC URL
            String jdbcUrl = String.format("jdbc:databricks://%s:443/default", hostname);
            dataSource.setUrl(jdbcUrl);

            // Set connection properties for token auth
            Properties props = new Properties();
            props.setProperty("httpPath", httpPath);
            props.setProperty("AuthMech", "3"); // 3 = Username/Password auth mechanism
            props.setProperty("SSL", "1"); // 1 = Enable SSL
            props.setProperty("ConnCatalog", catalog);
            props.setProperty("ConnSchema", schema);
            props.setProperty("personalAccessToken", accessToken); // Use token
            props.setProperty("RowsFetchedPerBlock", "10000"); // Performance tuning
            props.setProperty("EnableArrow", "1"); // Enable Arrow for better performance

            dataSource.setConnectionProperties(props);

            log.info("Databricks SQL DataSource configured successfully with token authentication (no connection pooling)");

        } catch (ClassNotFoundException e) {
            log.error("Failed to load Databricks JDBC driver: {}", e.getMessage(), e);
            throw new RuntimeException("Failed to initialize Databricks DataSource", e);
        }

        return dataSource;
    }

    /**
     * Creates and configures the MyBatis SqlSessionFactory
     * @param dataSource The configured Databricks SQL DataSource
     * @return SqlSessionFactory for MyBatis operations
     * @throws Exception If an error occurs during setup
     */
    @Bean
    public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception {
        SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean();
        sqlSessionFactoryBean.setDataSource(dataSource);

        // Configure MyBatis
        PathMatchingResourcePatternResolver resolver = new PathMatchingResourcePatternResolver();
        sqlSessionFactoryBean.setMapperLocations(resolver.getResources(mapperLocations));
        sqlSessionFactoryBean.setTypeAliasesPackage(typeAliasesPackage);

        // Optional MyBatis configuration
        org.apache.ibatis.session.Configuration configuration = new org.apache.ibatis.session.Configuration();
        configuration.setMapUnderscoreToCamelCase(true);
        configuration.setCacheEnabled(false);

        // Add catalog and schema as variables for use in mapper files
        Properties variables = new Properties();
        variables.setProperty("catalog", catalog);
        variables.setProperty("schema", schema);
        configuration.setVariables(variables);

        sqlSessionFactoryBean.setConfiguration(configuration);

        return sqlSessionFactoryBean.getObject();
    }

    /**
     * Creates the SqlSessionTemplate for MyBatis
     * @param sqlSessionFactory The configured SqlSessionFactory
     * @return SqlSessionTemplate for MyBatis operations
     */
    @Bean
    public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) {
        return new SqlSessionTemplate(sqlSessionFactory);
    }

    /**
     * Configures the transaction manager for the Databricks DataSource
     * @param dataSource The configured Databricks SQL DataSource
     * @return PlatformTransactionManager for transaction management
     */
    @Bean
    public PlatformTransactionManager transactionManager(DataSource dataSource) {
        return new DataSourceTransactionManager(dataSource);
    }
}


======================

# Databricks SQL Warehouse Configuration with Token-based Authentication
databricks:
  sql:
    hostname: your-workspace.cloud.databricks.com
    http-path: /sql/1.0/warehouses/your-warehouse-id
    access-token: ${DATABRICKS_TOKEN}  # Using environment variable for security
    catalog: hive_metastore
    schema: default

# MyBatis Configuration
mybatis:
  mapper-locations: classpath:mapper/**/*.xml
  type-aliases-package: com.example.s3upload.model
  configuration:
    map-underscore-to-camel-case: true
    cache-enabled: false