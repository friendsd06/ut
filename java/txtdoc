S3 Multipart Upload: End-to-End Flow Explanation
The diagram shows the complete process of uploading a large file to Amazon S3 using multipart upload. Here's a step-by-step explanation:
Step 1: Initiate Multipart Upload

The client application sends a request to Amazon S3 to initiate a multipart upload
This creates an upload "session" for the file
No actual file data is transferred in this step

Step 2: Receive Upload ID

S3 responds with a unique upload ID
This ID will be used to identify all parts of this specific multipart upload
The client stores this ID for use in subsequent steps

Step 3: Split File into Parts

The client application divides the large file (e.g., 5GB) into smaller chunks
Each part typically ranges from 5MB to 5GB (except the last part which can be smaller)
The optimal part size depends on network conditions and file size
Parts are numbered sequentially (1, 2, 3, ..., N)

Step 4: Upload Parts in Parallel

The client uploads all parts to S3 simultaneously
Each upload request includes:

The upload ID from Step 2
A part number (1, 2, 3, etc.)
The actual part data


Parallel uploads significantly improve throughput
Failed parts can be retried independently without restarting the entire upload

Step 5: Store ETag for Each Part

For each successfully uploaded part, S3 returns an ETag (entity tag)
The client must store each part number and its corresponding ETag
These ETags are essentially checksums that verify data integrity

Step 6: Complete Multipart Upload

After all parts are uploaded successfully, the client sends a "Complete" request
This request includes the upload ID and a list of all part numbers with their ETags
S3 validates all parts, combines them in the correct order, and creates the final object
S3 returns a final ETag for the complete file

Key Advantages of Multipart Upload:

Better throughput with parallel uploads
Ability to pause and resume uploads
Quick recovery from network issues (only failed parts need to be retried)
Support for files larger than 5GB (the single operation upload limit)

Would you like me to explain any specific aspect of this process in more detail?


Deep Dive: How Multipart Upload Parts Are Created
I'll explain in depth how file parts are created during S3 multipart uploads, using sample data to illustrate the process.
Part Creation Process in DetailS3 Multipart Upload Part Creation ProcessImageÂ How Parts Are Created: Technical Details
1. Mathematical Approach to Part Sizing
When breaking down a file for multipart upload, there are specific constraints that must be considered:

Minimum part size: 5MB (except the last part)
Maximum part size: 5GB
Maximum number of parts: 10,000
Maximum total file size: 5TB

For our example with a 100MB file, let's walk through the decision-making process:
javascript// Sample code for calculating part size
function calculateOptimalPartSize(fileSize) {
  const MIN_PART_SIZE = 5 * 1024 * 1024; // 5MB
  const MAX_PARTS = 10000;

  // Start with minimum part size
  let partSize = MIN_PART_SIZE;

  // If this would create too many parts, increase part size
  if (Math.ceil(fileSize / partSize) > MAX_PARTS) {
    partSize = Math.ceil(fileSize / MAX_PARTS);
  }

  // Round to convenient size (optional)
  return Math.ceil(partSize / (1024 * 1024)) * 1024 * 1024;
}

// For our 100MB file
const fileSize = 100 * 1024 * 1024; // 100MB in bytes
const partSize = calculateOptimalPartSize(fileSize); // Returns ~5MB
const numParts = Math.ceil(fileSize / partSize); // 20 parts
For our diagram example, we chose 25MB parts for simplicity, resulting in 4 equal parts.
2. Buffer-Level Part Creation
At the programming level, a multipart upload involves working with file buffers or streams. Here's how the actual byte-level splitting typically works:
javascript// Pseudocode for reading and uploading parts
async function uploadInParts(filePath, partSize) {
  // Step 1: Initialize multipart upload
  const uploadId = await initiateMultipartUpload(bucketName, objectKey);

  // Step 2: Prepare for reading file in chunks
  const fileSize = getFileSize(filePath);
  const numParts = Math.ceil(fileSize / partSize);
  const uploadPromises = [];
  const partETags = [];

  // Step 3: Read and upload each part
  for (let i = 0; i < numParts; i++) {
    // Calculate byte range for this part
    const start = i * partSize;
    const end = Math.min(start + partSize - 1, fileSize - 1);

    // Read this chunk from file
    const fileChunk = await readFileChunk(filePath, start, end);

    // Upload part to S3
    const partNumber = i + 1; // Part numbers start at 1, not 0
    const partResponse = await uploadPart(bucketName, objectKey, uploadId, partNumber, fileChunk);

    // Store the ETag for this part
    partETags.push({
      PartNumber: partNumber,
      ETag: partResponse.ETag
    });
  }

  // Step 4: Complete multipart upload
  await completeMultipartUpload(bucketName, objectKey, uploadId, partETags);
}
3. Real-World Example with Sample Data
Let's examine how a real 100MB image file might be broken down:
Original File (100MB JPG image):

First few bytes: FF D8 FF E0 00 10 4A 46 49 46 00 01... (JPEG header)
Middle section: Contains actual image data
End bytes: ... FF D9 (JPEG end marker)

Part 1 (0-25MB):

Contains: Full JPEG header + first quarter of image data
Starts with: FF D8 FF E0 00 10 4A 46...
Content type in upload request: application/octet-stream
HTTP headers include:
Content-Length: 26214400
x-amz-part-number: 1
x-amz-upload-id: [upload-id-from-initiation]


Part 2 (25MB-50MB):

Contains: Just raw binary image data from the middle of the file
No file headers/footers - just the next 25MB chunk of bytes
Note: This part might not be meaningful on its own (can't be viewed as an image)

Parts 3-4:

Similar to Part 2, just raw byte chunks
Part 4 will contain the JPEG footer (FF D9) at the end

4. Technical Implementation Considerations

Buffered vs. Streaming Reading:

For smaller files: Read entire part into memory before uploading
For very large files: Stream directly from disk to S3 to conserve memory


Content-MD5 Headers:
javascript// Calculate MD5 of part data for integrity verification
const md5Hash = calculateMD5(partData);
const md5Base64 = Buffer.from(md5Hash, 'hex').toString('base64');

// Add to upload request
headers['Content-MD5'] = md5Base64;

Checksumming Parts:

S3 computes an MD5 checksum for each uploaded part
Returns ETag in the response (usually the MD5 hash in quotes)
Example ETag: "a54357aff0632cce46d942af68356b38"


Concurrency Management:
javascript// Using a concurrency limiter
const concurrencyLimit = 4; // Upload 4 parts at a time
const partUploadTasks = [];

for (let i = 0; i < numParts; i++) {
  partUploadTasks.push(() => uploadSinglePart(i));
}

// Execute with limited concurrency
const results = await runWithConcurrencyLimit(partUploadTasks, concurrencyLimit);


5. Error Handling and Retries
A key advantage of multipart uploads is robust error handling:
javascriptasync function uploadPartWithRetry(partNumber, partData, maxRetries = 3) {
  let attempt = 0;

  while (attempt < maxRetries) {
    try {
      return await uploadPart(bucketName, key, uploadId, partNumber, partData);
    } catch (error) {
      attempt++;

      if (attempt >= maxRetries) {
        throw new Error(`Failed to upload part ${partNumber} after ${maxRetries} attempts`);
      }

      // Exponential backoff
      const delay = 1000 * Math.pow(2, attempt);
      await sleep(delay);
    }
  }
}
6. Working with SDK Examples
Using AWS SDK for JavaScript v3:
javascriptimport { S3Client, CreateMultipartUploadCommand, UploadPartCommand,
         CompleteMultipartUploadCommand } from "@aws-sdk/client-s3";
import { createReadStream, statSync } from "fs";

const s3Client = new S3Client({ region: "us-west-2" });

async function uploadLargeFile(filePath, bucketName, key) {
  // Get file size
  const fileSize = statSync(filePath).size;
  const partSize = 25 * 1024 * 1024; // 25MB per part

  // Initiate upload
  const createResponse = await s3Client.send(new CreateMultipartUploadCommand({
    Bucket: bucketName,
    Key: key
  }));

  const uploadId = createResponse.UploadId;
  const parts = [];

  // Calculate number of parts
  const numParts = Math.ceil(fileSize / partSize);

  for (let i = 0; i < numParts; i++) {
    const start = i * partSize;
    const end = Math.min((i + 1) * partSize, fileSize) - 1;

    // Create readable stream for this part only
    const fileStream = createReadStream(filePath, {
      start: start,
      end: end
    });

    // Upload the part
    const uploadResponse = await s3Client.send(new UploadPartCommand({
      Bucket: bucketName,
      Key: key,
      UploadId: uploadId,
      PartNumber: i + 1,
      Body: fileStream,
      ContentLength: end - start + 1
    }));

    // Save ETag for completing the upload
    parts.push({
      PartNumber: i + 1,
      ETag: uploadResponse.ETag
    });
  }

  // Complete multipart upload
  await s3Client.send(new CompleteMultipartUploadCommand({
    Bucket: bucketName,
    Key: key,
    UploadId: uploadId,
    MultipartUpload: {
      Parts: parts
    }
  }));
}
Practical Tips for Optimal Part Sizing

File size considerations:

Small files (<100MB): 5-10MB parts
Medium files (100MB-1GB): 10-25MB parts
Large files (1GB-5TB): 100MB-1GB parts


Network conditions:

Fast, stable connections: Larger parts (50MB+)
Unstable connections: Smaller parts (10-25MB)


Memory constraints:

Limited memory environments: Stream parts directly without buffering entire parts



Would you like me to explain any specific aspect of this process in more detail?
=========================

S3 Multipart Upload in Java: Simple Explanation
Let me explain how Amazon S3 multipart uploads work in Java with a clear, practical approach.
What is Multipart Upload?
Multipart upload allows you to break a large file into smaller parts and upload them in parallel. This provides several benefits:

Better throughput - Upload multiple parts simultaneously
Resiliency - Retry just the failed parts, not the entire file
Pause/Resume - Upload can be paused and resumed later
Large file support - Upload files larger than 5GB (single upload limit)

The 4 Main Steps of Multipart Upload
1. Initiate the Upload
First, tell S3 you want to start a multipart upload. S3 gives you a unique upload ID.

====================================

S3 Multipart Upload in Java: Simple Explanation
Let me explain how Amazon S3 multipart uploads work in Java with a clear, practical approach.
What is Multipart Upload?
Multipart upload allows you to break a large file into smaller parts and upload them in parallel. This provides several benefits:

Better throughput - Upload multiple parts simultaneously
Resiliency - Retry just the failed parts, not the entire file
Pause/Resume - Upload can be paused and resumed later
Large file support - Upload files larger than 5GB (single upload limit)

The 4 Main Steps of Multipart Upload
1. Initiate the Upload
First, tell S3 you want to start a multipart upload. S3 gives you a unique upload ID.
java// Start multipart upload
InitiateMultipartUploadRequest initRequest =
    new InitiateMultipartUploadRequest(bucketName, objectKey);
InitiateMultipartUploadResult initResponse =
    s3Client.initiateMultipartUpload(initRequest);
String uploadId = initResponse.getUploadId();

System.out.println("Upload ID: " + uploadId);
2. Calculate Parts
Decide how to split your file into parts:
java// Calculate parts
File file = new File("/path/to/large-file.zip");
long fileSize = file.length();
long partSize = 5 * 1024 * 1024; // 5MB (minimum allowed part size)
long partCount = (fileSize + partSize - 1) / partSize; // Round up

System.out.println("File size: " + fileSize + " bytes");
System.out.println("Will upload in " + partCount + " parts");
3. Upload Each Part
For each part, you:

Read the bytes for this part from the file
Upload the part to S3
Save the "ETag" that S3 returns

java// Example showing how Part 1 is uploaded
FileInputStream fileInputStream = new FileInputStream(file);

// For Part 1:
int partNumber = 1;
long partSize = 5 * 1024 * 1024; // 5MB
byte[] buffer = new byte[(int)partSize];

// Read 5MB of data from the file
fileInputStream.read(buffer, 0, (int)partSize);

// Create the request for this part
UploadPartRequest uploadRequest = new UploadPartRequest()
    .withBucketName(bucketName)
    .withKey(objectKey)
    .withUploadId(uploadId)
    .withPartNumber(partNumber)    // Parts are numbered 1, 2, 3...
    .withPartSize(partSize)
    .withInputStream(new ByteArrayInputStream(buffer));

// Upload the part and get the ETag
UploadPartResult uploadResult = s3Client.uploadPart(uploadRequest);
PartETag partETag = uploadResult.getPartETag();
4. Complete the Upload
When all parts are uploaded, tell S3 to combine them:
java// Complete the upload by providing all ETags
CompleteMultipartUploadRequest completeRequest =
    new CompleteMultipartUploadRequest(
        bucketName, objectKey, uploadId, partETags);

s3Client.completeMultipartUpload(completeRequest);
What's Actually Happening with the File Parts?
Let's use a sample 100MB image file to understand:
Original File: vacation-photos.zip (100MB)
[file header][photo1.jpg][photo2.jpg]...[photo50.jpg][file footer]
When Split into 5MB Parts:
Part 1 (0-5MB):

Contains the ZIP file header and first few photos
The AWS SDK reads exactly 5MB from the file starting at position 0
This part alone isn't a valid ZIP file since it's cut in the middle

Part 2 (5MB-10MB):

Contains just raw data from the middle of the ZIP file
Again, reading exactly 5MB starting at position 5MB (5,242,880 bytes)
May contain partial photos that cross part boundaries

Parts 3-20 (rest of file):

Similar to Part 2, just continuing through the file
The last part (Part 20) will contain the ZIP footer and may be smaller than 5MB

Here's what's happening under the hood in our Java code:
javatry (FileInputStream fileInputStream = new FileInputStream(file)) {
    // Start at the beginning of the file
    long filePosition = 0;

    // For each part...
    for (int partNumber = 1; partNumber <= partCount; partNumber++) {
        // Calculate this part's size (last part might be smaller)
        long currentPartSize = Math.min(partSize, fileSize - filePosition);

        // Create a buffer for this part's data
        byte[] buffer = new byte[(int) currentPartSize];

        // Read exactly this many bytes from the file
        fileInputStream.read(buffer, 0, (int) currentPartSize);

        // *** THIS IS THE KEY PART ***
        // A real-world example showing what happens with our vacation-photos.zip:
        // Part 1 (5MB): Contains file header + first photos
        // Position 0-5MB: [ZIP-HEADER][PHOTO1][PART-OF-PHOTO2]...

        // Part 2 (5MB): Contains middle section with partial photos
        // Position 5MB-10MB: [REST-OF-PHOTO2][PHOTO3][PART-OF-PHOTO4]...

        // Upload this part
        UploadPartRequest uploadRequest = new UploadPartRequest()
            .withBucketName(bucketName)
            .withKey(objectKey)
            .withUploadId(uploadId)
            .withPartNumber(partNumber)
            .withPartSize(currentPartSize)
            .withInputStream(new ByteArrayInputStream(buffer));

        // Store the ETag for this part
        UploadPartResult result = s3Client.uploadPart(uploadRequest);
        partETags.add(result.getPartETag());

        // Move to the next part's position in the file
        filePosition += currentPartSize;
    }
}
Practical Tips for Java S3 Multipart Uploads

Choosing the right part size:

5MB is the minimum allowed (except for the last part)
Larger files should use larger part sizes (25-100MB)
Never exceed 10,000 parts (S3 limit)


Parallelizing uploads for better performance:
java// Using ExecutorService for parallel uploads
ExecutorService executorService = Executors.newFixedThreadPool(10);
List<Future<PartETag>> futures = new ArrayList<>();

for (int partNumber = 1; partNumber <= partCount; partNumber++) {
    final int currentPartNumber = partNumber;
    Future<PartETag> future = executorService.submit(() -> {
        // Upload logic for this part
        return uploadPart(currentPartNumber);
    });
    futures.add(future);
}

// Collect results
List<PartETag> partETags = new ArrayList<>();
for (Future<PartETag> future : futures) {
    partETags.add(future.get());
}

Managing memory efficiently:

For very large files, don't load the entire part into memory
Use streaming to read and upload simultaneously:

java// Instead of loading entire part in memory
UploadPartRequest uploadRequest = new UploadPartRequest()
    .withBucketName(bucketName)
    .withKey(objectKey)
    .withUploadId(uploadId)
    .withPartNumber(partNumber)
    .withPartSize(partSize)
    .withFile(file)           // Use the file directly
    .withFileOffset(partStart); // Start position in file

Handling failures properly:

Always implement retry logic for part uploads
Save upload state (uploadId and completed parts) for long-running uploads
Use abortMultipartUpload() if you need to cancel


Uploading the last part:

The last part can be smaller than 5MB
All other parts must be at least 5MB



Common Pitfalls to Avoid

Not cleaning up incomplete uploads:
Implement a lifecycle policy to clean up incomplete multipart uploads after a few days
Calculating part numbers incorrectly:
Part numbers start at 1, not 0
Not handling ETags properly:
ETags must be provided in the correct order when completing the upload
Memory issues with large files:
Use streaming instead of loading parts entirely into memory
Not handling timeouts:
S3 uploads can time out for very large parts - implement retry logic

Simplified Example: The Most Basic Approach
If you just want a simple, working example without all the details: