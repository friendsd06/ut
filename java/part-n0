How S3 Multipart Upload Assembly Works
The multipart upload process in S3 involves a sophisticated server-side assembly mechanism that allows large files to be uploaded in parallel and then seamlessly stitched together. Let me explain this process in detail:
Multipart Upload Process Flow
Step 1: Client Calculates Optimal Part Size
When preparing to upload a large file, the client first determines the optimal part size:
javaprivate PartSizeCalculation calculateOptimalPartSize(long fileSize) {
    // Start with the minimum part size (5MB)
    long partSizeBytes = uploadConfig.minimumPartSize;

    // If file exceeds max parts capacity, increase part size
    if (fileSize > uploadConfig.minimumPartSize * uploadConfig.maxPartsAllowed) {
        partSizeBytes = (fileSize + uploadConfig.maxPartsAllowed - 1) / uploadConfig.maxPartsAllowed;

        // Round up to nearest MB for cleaner numbers
        long megabyte = 1024 * 1024;
        partSizeBytes = ((partSizeBytes + megabyte - 1) / megabyte) * megabyte;
    }

    // Calculate total number of parts (ceiling division)
    int totalParts = (int)((fileSize + partSizeBytes - 1) / partSizeBytes);

    return new PartSizeCalculation(partSizeBytes, totalParts);
}
Constraints:

Each part must be at least 5MB (except the last part)
Each part can be up to 5GB
You can have a maximum of 10,000 parts per upload
Total file size can be up to 5TB

Step 2: Initiate Multipart Upload
The client sends a request to S3 to start the multipart upload process:
javaString uploadId = s3Client.createMultipartUpload(
        request -> request.bucket(bucketName).key(objectKey)
).uploadId();
What happens on the server:

S3 creates a unique upload ID that will be used to track all parts of this upload
This ID is like a "session token" for the multipart upload
No actual file data is transferred at this stage
S3 creates temporary storage to hold the parts as they arrive

Step 3: Upload Individual Parts
The client divides the file into parts and uploads them in parallel:
javafor (int partNumber = 1; partNumber <= totalParts; partNumber++) {
    long partOffset = (partNumber - 1) * partSize;
    long partLength = Math.min(partSize, fileSize - partOffset);

    PartUploadTask task = new PartUploadTask(
            bucketName, objectKey, uploadId, filePath,
            partNumber, partOffset, partLength, maxRetries);

    partUploadFutures.add(workerThreadPool.submit(task));
}
For each part:

The client reads a specific byte range from the file
The part is uploaded with the upload ID and a sequential part number
S3 returns an ETag (Entity Tag) for each successfully uploaded part
The client stores each part number and its corresponding ETag

What happens on the server:

S3 stores each part separately in temporary storage
Parts can arrive in any order
Each part is validated and assigned an ETag (essentially a checksum)
Parts remain in a "pending" state until the upload is completed or aborted

Step 4: Complete Multipart Upload
Once all parts are uploaded, the client completes the upload by sending the list of ETags:
java// Sort parts by part number
completedParts.sort(Comparator.comparing(CompletedPart::partNumber));

// Complete the multipart upload
s3Client.completeMultipartUpload(request -> request
        .bucket(bucketName)
        .key(objectKey)
        .uploadId(uploadId)
        .multipartUpload(multipart -> multipart.parts(completedParts))
);
Server-side assembly process:

Part Verification

S3 verifies that all parts referenced in the complete request exist
It checks that all ETags match what was stored in its temporary storage
If any part is missing or has a mismatched ETag, the entire assembly fails


Part Ordering

S3 sorts the parts by part number (1, 2, 3, ...) to ensure correct sequence
This is why the part numbers must be unique and sequential


Data Concatenation

S3 concatenates all the parts in sequential order
This is a simple "byte-level" concatenation - Part 1's last byte is followed by Part 2's first byte
No special handling is needed at part boundaries since it's a raw byte stream


Object Creation

S3 creates the final object with the combined data
The temporary parts are cleaned up
A final ETag is generated for the complete object (usually a hash of the individual part ETags)


Metadata Assignment

Any metadata specified during initiation is applied to the final object
The object becomes available for access through normal S3 operations



How S3 Handles Part Boundaries
A key question is: How does S3 handle part boundaries when reassembling the file? The answer is simple but important to understand:

No Special Boundary Processing

S3 performs a direct byte-by-byte concatenation with no special boundary handling
Parts are joined without any padding, markers, or headers in between


Client Responsibility

It's the client's responsibility to calculate proper part boundaries
When splitting files like images or documents, the part boundaries might occur in the middle of encoded data
This is perfectly fine since the reassembly is an exact byte-level concatenation


Integrity Verification

S3 uses the ETags to ensure each part was uploaded correctly
The final object's ETag is derived from the individual part ETags



Error Handling During Assembly
If something goes wrong during the assembly process:

S3 will return an error response with details about what failed
The multipart upload remains in an incomplete state
All uploaded parts remain stored (and may incur storage charges)
The client can retry the completion step or abort the upload

This is why proper cleanup is essential - if the client doesn't explicitly abort failed uploads, the parts will continue to exist and accrue charges.
Code Implementation for Handling Assembly Completion
In our production code, we handle the assembly completion like this:
javatry {
    // Sort parts by part number to ensure correct assembly order
    List<CompletedPart> sortedParts = completedParts.stream()
            .sorted(Comparator.comparing(CompletedPart::partNumber))
            .collect(Collectors.toList());

    // Complete the multipart upload - triggers server-side assembly
    CompleteMultipartUploadResponse response = s3Client.completeMultipartUpload(
            request -> request
                    .bucket(bucketName)
                    .key(objectKey)
                    .uploadId(uploadId)
                    .multipartUpload(multipart -> multipart.parts(sortedParts))
    );

    logger.info("Assembly complete: {}, ETag: {}", response.location(), response.eTag());

} catch (Exception exception) {
    // In case of assembly failure, abort the upload to clean up parts
    s3Client.abortMultipartUpload(request -> request
            .bucket(bucketName)
            .key(objectKey)
            .uploadId(uploadId)
    );

    throw exception;
}
Why This Approach Is Powerful
The S3 multipart upload assembly process is elegantly designed for several key benefits:

Parallelism - Parts can be uploaded concurrently, significantly improving throughput
Resilience - Failed parts can be retried independently without restarting the entire upload
Efficiency - Only the minimum required data is transferred over the network
Scalability - Works for files from a few MB to multiple TB
Server-side assembly - The CPU-intensive task of joining the parts happens on S3's servers, not the client

This design makes it ideal for uploading large files in environments with unreliable networks or when you need to maximize throughput.
Would you like me to explain any specific aspect of the assembly process in more detail?
