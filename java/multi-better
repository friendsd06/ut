S3 Multipart Upload: Detailed Assembly Process Explained
Introduction to S3 Multipart Uploads
S3 multipart upload is Amazon's solution for reliably transferring large files to cloud storage. Unlike a standard upload that sends an entire file in a single HTTP request, multipart upload breaks a file into smaller chunks that can be uploaded independently and then reassembled on the server side.
The Complete End-to-End Process
Let's walk through each step of the process with detailed explanations:
STEP 1: Client Calculates Optimal Part Size
The first decision is how to divide the file. This calculation is critical for upload efficiency:
java// Calculate optimal part size for the file
PartSizeCalculation partCalculation = new PartSizeCalculation(fileSize);
Key Constraints:

Minimum part size: 5MB (except possibly the last part)
Maximum part size: 5GB
Maximum parts allowed: 10,000
Maximum total file size: 5TB (10,000 parts × 5GB)

Algorithm Logic:
java// Starting with minimum part size
long partSize = 5 * 1024 * 1024; // 5MB

// If file is large enough to require more than 10,000 parts
if (fileSize > partSize * 10000) {
    // Calculate minimum size needed to keep parts under 10,000
    partSize = (fileSize + 10000 - 1) / 10000; // Ceiling division

    // Round up to nearest MB for clean boundaries
    long MB = 1024 * 1024;
    partSize = ((partSize + MB - 1) / MB) * MB;
}

// Calculate total number of parts needed
int totalParts = (int)((fileSize + partSize - 1) / partSize);
For our example 200MB file, we'll use 50MB parts resulting in 4 parts total.
STEP 2: Split File Into Parts
The client identifies the precise byte ranges for each part:
Part 1: Bytes 0-52,428,799 (50MB)
Part 2: Bytes 52,428,800-104,857,599 (50MB)
Part 3: Bytes 104,857,600-157,286,399 (50MB)
Part 4: Bytes 157,286,400-209,715,199 (50MB)
Important: No actual file splitting happens at this stage - the file remains intact. The client is just calculating boundaries for reading specific byte ranges later.
STEP 3: Initiate Multipart Upload
The client tells S3 it wants to start a multipart upload:
javaCreateMultipartUploadRequest initiateRequest = CreateMultipartUploadRequest.builder()
    .bucket(bucketName)
    .key(objectKey)
    // Optional: Set content type, metadata, etc.
    .contentType("application/octet-stream")
    .build();

CreateMultipartUploadResponse initiateResponse = s3Client.createMultipartUpload(initiateRequest);
String uploadId = initiateResponse.uploadId();
Server-Side Actions:

S3 generates a unique uploadId (e.g., "EXAMPLEUPLOADID12345")
This ID serves as the "session identifier" for all subsequent operations
S3 creates temporary storage space in its internal system to receive parts
No actual file data is transferred yet

Key Point: This upload ID must be included with every part upload request to associate parts with this specific multipart upload session.
STEP 4: Upload Parts in Parallel
Now the client uploads each part to S3:
java// Create worker threads to upload parts in parallel
ExecutorService executorService = Executors.newFixedThreadPool(10);
List<CompletableFuture<CompletedPart>> uploadFutures = new ArrayList<>();

// Launch uploads for each part
for (int partNumber = 1; partNumber <= totalParts; partNumber++) {
    final long partOffset = (partNumber - 1) * partSize;
    final long partLength = Math.min(partSize, fileSize - partOffset);
    final int finalPartNumber = partNumber;

    // Create a future task for this part upload
    CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(() -> {
        // Create input stream for this part's data
        InputStream partData = createPartInputStream(filePath, partOffset, partLength);

        // Create upload request for this part
        UploadPartRequest uploadRequest = UploadPartRequest.builder()
                .bucket(bucketName)
                .key(objectKey)
                .uploadId(uploadId)    // Associate with upload session
                .partNumber(finalPartNumber)  // Label with part number
                .build();

        // Upload the part and get response
        UploadPartResponse response = s3Client.uploadPart(
                uploadRequest,
                RequestBody.fromInputStream(partData, partLength)
        );

        // Return the completed part info
        return CompletedPart.builder()
                .partNumber(finalPartNumber)
                .eTag(response.eTag())  // Store checksum (ETag)
                .build();
    }, executorService);

    uploadFutures.add(future);
}

// Wait for all uploads to complete and collect results
List<CompletedPart> completedParts = new ArrayList<>();
for (CompletableFuture<CompletedPart> future : uploadFutures) {
    completedParts.add(future.get());
}
Server-Side Actions:

S3 receives each part separately
For each part, S3:

Validates the data integrity
Stores the part in temporary storage
Generates an ETag (MD5 hash) for the part
Returns the ETag to the client


S3 tracks parts by their part numbers but doesn't assemble them yet
Parts can arrive in any order (Part 3 might arrive before Part 1)

Key Points:

Each part upload is independent and can be retried separately if it fails
Parts must have sequential numbers (1, 2, 3, ...) but can be uploaded in any order
The client must store each part's ETag for the completion step

STEP 5: Complete Multipart Upload (Assembly Process)
After all parts are successfully uploaded, the client tells S3 to assemble the final object:
java// Sort parts by part number to ensure correct order
completedParts.sort(Comparator.comparing(CompletedPart::partNumber));

// Create completion request with all part information
CompleteMultipartUploadRequest completeRequest = CompleteMultipartUploadRequest.builder()
        .bucket(bucketName)
        .key(objectKey)
        .uploadId(uploadId)
        .multipartUpload(CompletedMultipartUpload.builder()
                .parts(completedParts)
                .build())
        .build();

// Send the request to trigger assembly
CompleteMultipartUploadResponse completeResponse =
        s3Client.completeMultipartUpload(completeRequest);

// Final object is now available at this URL
String objectUrl = completeResponse.location();
Server-Side Assembly Process:

Verification Step:

S3 checks that all parts specified in the complete request exist
It verifies that ETags match what was originally stored
If any part is missing or has an ETag mismatch, assembly fails with an error


Part Ordering:

S3 sorts the parts by their part numbers
This ensures byte ranges will be concatenated in the correct sequence
The client should also sort parts before sending, but S3 double-checks


Byte-level Concatenation:

S3 performs direct binary concatenation of the parts
There is no metadata, markers, or padding added between parts
This is a raw byte-by-byte assembly: Last byte of Part 1 → First byte of Part 2 → etc.


Final Object Creation:

The assembled data is stored as a single object in S3 normal storage
All metadata specified during initiation is applied to the final object
A new ETag is calculated for the complete object
The temporary parts storage is marked for cleanup


Cleanup:

All temporary part data is eventually removed from S3's internal storage
The multipart upload "session" is closed
The uploadId becomes invalid for further operations



Diagram Explanation:
The diagram shows how a 200MB file is split into four 50MB parts. Each part has a specific byte range:

Part 1: 0 to 52,428,799
Part 2: 52,428,800 to 104,857,599
Part 3: 104,857,600 to 157,286,399
Part 4: 157,286,400 to 209,715,199

During assembly, S3 concatenates these ranges in exactly the same order to recreate the original file's byte sequence.
Understanding the Byte-Level Assembly
The most important aspect to understand is how S3 handles the binary data during assembly:

No Special Headers or Markers

S3 doesn't add any additional data between parts
The assembled file is byte-for-byte identical to the original


Exact Byte Position Matters

If a file has structured data (like an image or document), part boundaries might fall in the middle of data structures
This is completely fine since the binary concatenation preserves the exact byte sequence


The Final ETag

For single-part uploads, the ETag is an MD5 hash of the object data
For multipart uploads, the ETag is more complex: it's derived from the individual part ETags
Format: [hash of all part ETags]-[number of parts]
Example: "7e13429cc3cbeab464a45811afd45c03-4"



Code for Server-Side Assembly (Inside S3)
While we can't see Amazon's internal code, we can understand the logical process through pseudocode:
java// S3 internal assembly process (conceptual pseudocode)
void assembleMultipartUpload(String uploadId, List<CompletedPart> parts) {
    // Step 1: Verify all parts exist and ETags match
    for (CompletedPart part : parts) {
        StoredPart storedPart = getStoredPart(uploadId, part.partNumber());
        if (storedPart == null) {
            throw new PartNotFoundException("Part " + part.partNumber() + " not found");
        }
        if (!storedPart.eTag().equals(part.eTag())) {
            throw new ETagMismatchException("ETag for part " + part.partNumber() + " doesn't match");
        }
    }

    // Step 2: Sort parts by part number
    List<StoredPart> orderedParts = parts.stream()
            .map(part -> getStoredPart(uploadId, part.partNumber()))
            .sorted(Comparator.comparing(StoredPart::partNumber))
            .collect(Collectors.toList());

    // Step 3: Create output stream for final object
    ObjectOutputStream outputStream = createObjectOutputStream(bucket, key);

    // Step 4: Concatenate all parts in order
    for (StoredPart part : orderedParts) {
        InputStream partData = getPartData(uploadId, part.partNumber());
        // Copy all bytes directly with no modifications
        byte[] buffer = new byte[8192];
        int bytesRead;
        while ((bytesRead = partData.read(buffer)) != -1) {
            outputStream.write(buffer, 0, bytesRead);
        }
        partData.close();
    }

    // Step 5: Finalize the object
    outputStream.close();

    // Step 6: Calculate new ETag for complete object
    String finalETag = calculateFinalETag(orderedParts);
    setObjectMetadata(bucket, key, "ETag", finalETag);

    // Step 7: Schedule cleanup of temporary parts
    scheduleCleanup(uploadId);
}
Benefits of Server-Side Assembly
The S3 multipart upload design offers several key advantages:

Optimal Resource Usage

CPU-intensive assembly happens on S3 servers, not on the client
Network bandwidth is used efficiently with parallel uploads


Built-in Resume Capability

If the upload is interrupted, you only need to re-upload the failed parts
The uploadId remains valid for extended periods (usually 7 days by default)


Better Throughput

Parallel uploads can fully utilize available bandwidth
Multiple threads can saturate the network connection


Memory Efficiency

Client only needs to hold one part in memory at a time
Streaming design allows handling files much larger than available RAM



When Assembly Fails
If the assembly process fails, several things happen:

S3 returns an error response with details about the failure
The temporary part data remains stored (and may incur charges)
The multipart upload remains in an "incomplete" state
The client can:

Retry the completion request (after fixing any issues)
Abort the upload to clean up temporary parts
List and query parts to identify missing or corrupted parts



This is why monitoring and managing multipart uploads is critical - orphaned incomplete uploads can accumulate and lead to unexpected storage charges.
Would you like more details on any specific aspect of this process?