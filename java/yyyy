"""
Configuration manager for different environments
Author: Your Name
Date: 2025-01-01
"""

import logging
from dataclasses import dataclass, field
from enum import Enum, auto
from pathlib import Path
from typing import Dict, Optional
import yaml

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class Environment(Enum):
    LOCAL = auto()
    DEV = auto()
    TEST = auto()
    PREPROD = auto()
    PROD = auto()

    # 1. Takes an environment name like 'dev' or 'prod'
    # 2. Converts it to uppercase to match enum values
    # 3. Returns corresponding Environment enum value
    # 4. If invalid, shows list of valid environments
    @classmethod
    def from_str(cls, env_name: str) -> 'Environment':
        try:
            return cls[env_name.upper()]
        except KeyError:
            valid_environments = ', '.join(env.name.lower() for env in cls)
            raise ValueError(
                f"Invalid environment: '{env_name}'\n"
                f"Please use one of: {valid_environments}"
            )

@dataclass
class DatabricksEnv:
    hostname: str
    port: int
    httpPath: str
    ssl: str
    AuthMech: str
    transportMode: str
    ConnCatalog: str
    UseNativeQuery: str
    UseProxy: str
    ProxyHost: str
    ProxyPort: str

    # 1. Creates default settings for local development
    # 2. Uses minimal security settings for local testing
    # 3. Sets up localhost connection
    @classmethod
    def get_default_local(cls) -> 'DatabricksEnv':
        return cls(
            hostname="localhost",
            port=10000,
            transportMode="http",
            ssl="0",
            ConnCatalog="local_catalog",
            AuthMech="0",
            UseProxy="0",
            UseNativeQuery="1",
            ProxyHost="",
            ProxyPort="0",
            httpPath="/sql/1.0/warehouses/localWarehouse"
        )

    # 1. Formats connection parameters for Databricks
    # 2. Converts string values to appropriate types
    # 3. Returns dictionary ready for connection
    def get_connection_params(self) -> dict:
        return {
            "host": self.hostname,
            "port": self.port,
            "transport_mode": self.transportMode,
            "ssl": self.ssl == "1",
            "catalog": self.ConnCatalog,
            "auth_mechanism": int(self.AuthMech),
            "use_proxy": self.UseProxy == "1",
            "use_native_query": self.UseNativeQuery == "1",
            "proxy_host": self.ProxyHost,
            "proxy_port": int(self.ProxyPort),
            "http_path": self.httpPath
        }

@dataclass
class S3Env:
    base_path: str

    # 1. Creates default local storage settings
    # 2. Uses local filesystem path for testing
    @classmethod
    def get_default_local(cls) -> 'S3Env':
        return cls(base_path="file:///tmp/data/")

    # 1. Combines base path with relative path
    # 2. Handles forward slashes properly
    # 3. Returns complete storage path
    def get_full_path(self, relative_path: str) -> str:
        return str(Path(self.base_path.rstrip("/")) / relative_path.lstrip("/"))

@dataclass
class TableConfig:
    query: str
    s3_path: str
    format: str
    mode: str

    # 1. Checks if file format is supported (parquet, csv, etc.)
    # 2. Verifies write mode is valid (overwrite, append, etc.)
    # 3. Raises error with helpful message if invalid
    def validate(self) -> None:
        allowed_formats = {"parquet", "csv", "delta", "json"}
        allowed_modes = {"overwrite", "append", "ignore", "error"}

        if self.format.lower() not in allowed_formats:
            raise ValueError(
                f"Format '{self.format}' is not allowed.\n"
                f"Please use one of: {', '.join(allowed_formats)}"
            )

        if self.mode.lower() not in allowed_modes:
            raise ValueError(
                f"Mode '{self.mode}' is not allowed.\n"
                f"Please use one of: {', '.join(allowed_modes)}"
            )

@dataclass
class FullConfig:
    databricks: Dict[str, DatabricksEnv] = field(default_factory=dict)
    s3_settings: Dict[str, S3Env] = field(default_factory=dict)
    tables: Dict[str, TableConfig] = field(default_factory=dict)

    # 1. Checks if all required environments exist
    # 2. Validates all table configurations
    # 3. Ensures no missing settings
    def validate(self) -> None:
        required_envs = {"dev", "test", "preprod", "prod"}

        for section_name in ["databricks", "s3_settings"]:
            section = getattr(self, section_name)
            missing_envs = required_envs - set(section.keys())

            if missing_envs:
                raise ValueError(
                    f"Missing environments in {section_name}: "
                    f"{', '.join(missing_envs)}"
                )

        for table_name, table_config in self.tables.items():
            try:
                table_config.validate()
            except ValueError as e:
                raise ValueError(f"Invalid settings for table '{table_name}': {str(e)}")

class ConfigReader:
    # 1. Takes path to YAML config file
    # 2. Loads and parses configuration
    # 3. Validates all settings
    def __init__(self, config_file: str):
        self.config_file = config_file
        self.raw_config = self._load_yaml()
        self.config = self._build_full_config()
        self.config.validate()
        logger.info(f"Successfully loaded config from: {config_file}")

    # 1. Opens YAML file at specified path
    # 2. Parses YAML content to dictionary
    # 3. Validates basic structure
    # 4. Handles file and parsing errors
    def _load_yaml(self) -> dict:
        try:
            config_path = Path(self.config_file)

            if not config_path.exists():
                raise FileNotFoundError(f"Config file not found: {self.config_file}")

            with open(config_path, "r", encoding="utf-8") as f:
                data = yaml.safe_load(f) or {}

            if not isinstance(data, dict):
                raise ValueError(f"Config file must contain a dictionary")

            return data

        except yaml.YAMLError as e:
            raise ValueError(f"Error parsing YAML: {str(e)}")

    # 1. Checks for required config sections
    # 2. Creates environment settings with defaults
    # 3. Builds table configurations
    # 4. Returns complete configuration object
    def _build_full_config(self) -> FullConfig:
        try:
            required_sections = ["databricks", "s3_settings", "tables"]
            for section in required_sections:
                if section not in self.raw_config:
                    raise KeyError(f"Missing required section: '{section}'")

            db_envs = {"local": DatabricksEnv.get_default_local()}
            db_section = self.raw_config["databricks"]
            for env in ["dev", "test", "preprod", "prod"]:
                if env not in db_section:
                    raise KeyError(f"Missing Databricks settings for: {env}")

                env_config = db_section[env].copy()
                if isinstance(env_config.get("port"), str):
                    env_config["port"] = int(env_config["port"])

                db_envs[env] = DatabricksEnv(**env_config)

            s3_envs = {"local": S3Env.get_default_local()}
            s3_section = self.raw_config["s3_settings"]
            for env in ["dev", "test", "preprod", "prod"]:
                if env not in s3_section:
                    raise KeyError(f"Missing storage settings for: {env}")
                s3_envs[env] = S3Env(**s3_section[env])

            table_configs = {}
            table_section = self.raw_config["tables"]
            for table_name, table_data in table_section.items():
                table_configs[table_name] = TableConfig(**table_data)

            return FullConfig(
                databricks=db_envs,
                s3_settings=s3_envs,
                tables=table_configs
            )

        except Exception as e:
            raise ValueError(f"Error building configuration: {str(e)}")

    def get_full_config(self) -> FullConfig:
        return self.config

    def get_databricks_env(self, env: str) -> DatabricksEnv:
        env_obj = Environment.from_str(env)
        env_key = env_obj.name.lower()

        if env_key not in self.config.databricks:
            raise ValueError(f"No Databricks settings found for: {env}")

        return self.config.databricks[env_key]

    def get_s3_env(self, env: str) -> S3Env:
        env_obj = Environment.from_str(env)
        env_key = env_obj.name.lower()

        if env_key not in self.config.s3_settings:
            raise ValueError(f"No storage settings found for: {env}")

        return self.config.s3_settings[env_key]

    def get_table_config(self, table_name: str) -> TableConfig:
        if table_name not in self.config.tables:
            raise ValueError(f"No settings found for table: {table_name}")

        return self.config.tables[table_name]

# 1. Displays connection and storage settings for an environment
# 2. Handles errors and logs results
def show_environment_info(reader: ConfigReader, env: str) -> None:
    try:
        db_config = reader.get_databricks_env(env)
        logger.info(f"\n{env.upper()} Environment Settings:")
        logger.info("-" * 50)
        logger.info(f"Databricks Host: {db_config.hostname}")
        logger.info(f"Port: {db_config.port}")
        logger.info(f"Catalog: {db_config.ConnCatalog}")

        storage = reader.get_s3_env(env)
        logger.info(f"Storage Location: {storage.base_path}")

    except Exception as e:
        logger.error(f"Error showing {env} environment info: {str(e)}")

# 1. Shows configuration for a table in specific environment
# 2. Displays query, storage path, and settings
# 3. Handles errors and logs results
def show_table_info(reader: ConfigReader, table_name: str, env: str) -> None:
    try:
        table = reader.get_table_config(table_name)
        storage = reader.get_s3_env(env)

        logger.info(f"\nTable: {table_name}")
        logger.info("-" * 50)
        logger.info(f"Query: {table.query}")
        logger.info(f"Storage Path: {storage.get_full_path(table.s3_path)}")
        logger.info(f"Format: {table.format}")
        logger.info(f"Write Mode: {table.mode}")

    except Exception as e:
        logger.error(f"Error showing table '{table_name}' info: {str(e)}")

# 1. Loads configuration file
# 2. Shows settings for all environments
# 3. Displays table configurations
# 4. Handles any errors
def main():
    try:
        logger.info("Loading configuration...")
        reader = ConfigReader("config.yml")

        for env in ["local", "dev", "test", "preprod", "prod"]:
            show_environment_info(reader, env)

        logger.info("\nTable Configurations (DEV Environment):")
        for table_name in ["rgdfdfgdfgd_xref", "customer_dim"]:
            show_table_info(reader, table_name, "dev")

        logger.info("\nConfiguration loaded successfully!")

    except Exception as e:
        logger.error(f"Error in main function: {str(e)}")
        raise

if __name__ == "__main__":
    main()