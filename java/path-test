# Define the required JAR paths - update these paths based on your local setup
HADOOP_AWS_JAR = "/path/to/hadoop-aws-3.3.4.jar"
AWS_JAVA_SDK_JAR = "/path/to/aws-java-sdk-bundle-1.11.1026.jar"
HADOOP_COMMON_JAR = "/path/to/hadoop-common-3.3.4.jar"

# Initialize Spark Session with S3/LocalStack configuration
def create_spark_session():
    spark = SparkSession.builder \
        .appName("LocalStack S3 Example") \
        .master("local[*]") \
        .config("spark.jars", f"{HADOOP_AWS_JAR},{AWS_JAVA_SDK_JAR},{HADOOP_COMMON_JAR}") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:4566") \
        .config("spark.hadoop.fs.s3a.access.key", "test") \
        .config("spark.hadoop.fs.s3a.secret.key", "test") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider",
                "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
        .getOrCreate()

    return spark