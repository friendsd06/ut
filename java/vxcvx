"""
ReferenceDataLoader - Main class for handling ETL operations from Databricks to S3.
Manages configuration, data extraction, and loading process with proper error handling.
"""

from typing import List, Optional, Dict, Any
from pyspark.sql import SparkSession
import logging
from pathlib import Path
from datetime import datetime
from src.authentication.token_fetcher import TokenFetcherService
from src.config.config_manager import ConfigurationManager, create_configuration_manager
from src.io.databricks_warehouse_reader import WarehouseReader
from src.io.s3_writer import S3Writer

# Configure logging with timestamp and log level
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ReferenceLoadError(Exception):
    """Custom exception for handling reference data loading failures"""
    pass

class ReferenceDataLoader:
    def __init__(
        self,
        spark: SparkSession,
        warehouse_config: str,
        etl_config: str,
        environment: str
    ):
        """
        Initialize ReferenceDataLoader with required configurations and services.

        Args:
            spark: Active SparkSession
            warehouse_config: Path to warehouse configuration file
            etl_config: Path to ETL configuration file
            environment: Target environment (dev/test/prod)

        Raises:
            ReferenceLoadError: If initialization fails
        """
        try:
            logger.info(f"[INIT] Starting initialization for environment: {environment}")
            self.spark = spark

            # Load and validate configurations
            logger.info("[CONFIG] Loading configuration files")
            self.config_manager = create_configuration_manager(warehouse_config, etl_config)
            self.environment = environment

            # Get environment specific configurations
            self.warehouse_conn = self.config_manager.get_warehouse_connection(self.environment)
            self.storage_config = self.config_manager.get_storage_config(self.environment)
            logger.info("[CONFIG] Configurations loaded successfully")

            # Initialize required services
            logger.info("[SERVICES] Initializing required services")
            self.token_service = TokenFetcherService()
            self.warehouse_reader = WarehouseReader(
                config=self.warehouse_conn.get_connection_params(),
                token_service=self.token_service
            )
            self.s3_writer = S3Writer(base_path=self.storage_config.base_path)
            logger.info("[SERVICES] Services initialized successfully")

            logger.info(f"[SUCCESS] ReferenceLoader initialized for {environment}")

        except Exception as e:
            error_msg = f"[ERROR] Initialization failed: {str(e)}"
            logger.error(error_msg)
            raise ReferenceLoadError(error_msg)

    def load_table(self, table_name: str) -> None:
        """
        Load single table from Databricks to S3.

        Args:
            table_name: Name of the table to process

        Raises:
            ReferenceLoadError: If table processing fails
        """
        start_time = datetime.now()
        try:
            logger.info(f"[START] Processing table: {table_name}")

            # Get and validate table configuration
            logger.info(f"[CONFIG] Loading configuration for table: {table_name}")
            table_config = self.config_manager.get_table_config(table_name)
            logger.info(f"[CONFIG] Using format: {table_config.file_format}, mode: {table_config.write_mode}")

            # Extract data from warehouse
            logger.info("[EXTRACT] Executing warehouse query")
            df = self.warehouse_reader.execute_query(
                spark=self.spark,
                query=table_config.source_query,
                options=table_config.reader_options
            )

            row_count = df.count()
            logger.info(f"[EXTRACT] Query returned {row_count:,} rows")

            if row_count == 0:
                logger.warning(f"[WARNING] No data returned for table: {table_name}")
                return

            logger.info("[EXTRACT] Sample data preview:")
            df.show(10, truncate=False)

            # Write to S3
            dest_path = self.config_manager.get_destination_path(table_name, self.environment)
            logger.info(f"[LOAD] Writing data to: {dest_path}")

            self.s3_writer.write_dataframe(
                df=df,
                relative_path=dest_path,
                format=table_config.file_format,
                mode=table_config.write_mode,
                options=table_config.writer_options
            )

            duration = datetime.now() - start_time
            logger.info(f"[SUCCESS] Table {table_name} processed - {row_count:,} rows written in {duration}")

        except Exception as e:
            error_msg = f"[ERROR] Failed to process {table_name}. Error: {str(e)}"
            logger.error(error_msg)
            raise ReferenceLoadError(error_msg)

    def load_tables(self, table_names: List[str]) -> None:
        """
        Load multiple tables in sequence, tracking success and failures.

        Args:
            table_names: List of table names to process

        Raises:
            ReferenceLoadError: If any table fails to process
        """
        start_time = datetime.now()
        failures = []
        total_tables = len(table_names)

        logger.info(f"[START] Processing {total_tables} tables")

        for idx, table_name in enumerate(table_names, 1):
            try:
                logger.info(f"[PROGRESS] Table {idx}/{total_tables}: {table_name}")
                self.load_table(table_name=table_name)
            except Exception as e:
                logger.error(f"[ERROR] Table {table_name} failed: {str(e)}")
                failures.append(f"{table_name}: {str(e)}")

        duration = datetime.now() - start_time

        if failures:
            error_msg = "[FAILED] Tables with errors:\n" + "\n".join(failures)
            logger.error(error_msg)
            logger.error(f"[SUMMARY] Process completed with errors in {duration}")
            raise ReferenceLoadError(error_msg)
        else:
            logger.info(f"[SUCCESS] All {total_tables} tables processed successfully in {duration}")

def main():
    """Entry point for ETL process"""
    start_time = datetime.now()
    spark = None

    try:
        logger.info("[START] Initializing ETL process")

        # Initialize Spark
        logger.info("[SPARK] Creating Spark session")
        spark = (SparkSession.builder
            .appName("ReferenceDataLoader")
            .config("spark.driver.extraClassPath", "/opt/spark/jars/databricks-jdbc-2.6.29.jar")
            .config("spark.executor.extraClassPath", "/opt/spark/jars/databricks-jdbc-2.6.29.jar")
            .getOrCreate())
        logger.info("[SPARK] Session created successfully")

        # Environment setup
        env = "dev"
        logger.info(f"[ENV] Using environment: {env}")

        # Initialize loader
        loader = ReferenceDataLoader(
            spark=spark,
            warehouse_config="/path/to/warehouse_config.yaml",
            etl_config="/path/to/etl_config.yaml",
            environment=env
        )

        # Process tables
        tables = ["rdr_cis_index_xref"]
        logger.info(f"[PROCESS] Starting data load for tables: {', '.join(tables)}")
        loader.load_tables(table_names=tables)

        duration = datetime.now() - start_time
        logger.info(f"[SUCCESS] ETL process completed successfully in {duration}")

    except FileNotFoundError as e:
        logger.error(f"[ERROR] Configuration file not found: {str(e)}")
        raise SystemExit(1)
    except Exception as e:
        logger.error(f"[ERROR] ETL process failed: {str(e)}")
        raise SystemExit(1)
    finally:
        if spark:
            logger.info("[CLEANUP] Stopping Spark session")
            spark.stop()

if __name__ == "__main__":
    main()