package com.example.s3upload.service;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;
import software.amazon.awssdk.core.async.AsyncRequestBody;
import software.amazon.awssdk.services.s3.S3AsyncClient;
import software.amazon.awssdk.services.s3.model.*;

import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.file.Path;
import java.nio.file.StandardOpenOption;
import java.time.Duration;
import java.time.Instant;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;
import java.util.concurrent.*;
import java.util.stream.Collectors;

/**
 * Service for uploading files to Amazon S3 with optimized strategies.
 *
 * Features:
 * - Simple upload for small files (< threshold)
 * - Parallel multipart upload for large files
 * - Automatic retry with exponential backoff
 * - Configurable part size and thread pool
 * - Progress tracking and throughput calculation
 *
 * Upload Strategy:
 * - Files < threshold: Simple PUT operation
 * - Files >= threshold: Multipart upload with parallel parts
 *
 * S3 Constraints:
 * - Minimum part size: 5MB (except last part)
 * - Maximum parts: 10,000
 * - Part size must be consistent (except last part)
 */
@Service
public class FileUploadService {
    private static final Logger log = LoggerFactory.getLogger(FileUploadService.class);

    // S3 multipart upload constraints
    private static final long MIN_PART_SIZE_BYTES = 5 * 1024 * 1024; // 5MB minimum
    private static final int MAX_PARTS = 10000; // S3 limit
    private static final int DEFAULT_MAX_RETRIES = 3;
    private static final long MB = 1024 * 1024;

    // Operation timeouts
    private static final int INITIATE_UPLOAD_TIMEOUT_MINUTES = 2;
    private static final int COMPLETE_UPLOAD_TIMEOUT_MINUTES = 5;
    private static final int ABORT_UPLOAD_TIMEOUT_MINUTES = 2;
    private static final int SHUTDOWN_TIMEOUT_SECONDS = 30;

    // Configuration
    @Value("${app.upload.max-threads:10}")
    private int maxUploadThreads;

    @Value("${app.upload.part-size-mb:5}")
    private int partSizeMb;

    @Value("${app.upload.timeout-minutes:30}")
    private int uploadTimeoutMinutes;

    // Dependencies
    private final S3AsyncClient s3Client;
    private ExecutorService uploadThreadPool;

    public FileUploadService(S3AsyncClient s3Client) {
        this.s3Client = s3Client;
    }

    @PostConstruct
    public void initialize() {
        this.uploadThreadPool = createThreadPool();
        log.info("FileUploadService initialized - threads: {}, part size: {}MB",
                maxUploadThreads, partSizeMb);
    }

    @PreDestroy
    public void cleanup() {
        log.info("Shutting down FileUploadService");
        shutdownThreadPool();
    }

    // ==================== Public API ====================

    /**
     * Upload a small file using simple PUT operation.
     * Best for files under the multipart threshold.
     *
     * @param bucketName S3 bucket name
     * @param objectKey S3 object key (path)
     * @param filePath Local file to upload
     * @param fileSize Size in bytes
     * @throws IOException if upload fails
     */
    public void uploadSimpleFile(String bucketName, String objectKey, Path filePath, long fileSize)
            throws IOException {
        log.info("Simple upload: {} ({} MB) -> s3://{}/{}",
                filePath.getFileName(), fileSize / MB, bucketName, objectKey);

        try {
            PutObjectRequest request = PutObjectRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .build();

            s3Client.putObject(request, AsyncRequestBody.fromFile(filePath))
                    .get(uploadTimeoutMinutes, TimeUnit.MINUTES);

            log.info("Simple upload completed: {}", objectKey);
        } catch (Exception e) {
            String error = String.format("Simple upload failed for %s: %s", objectKey, e.getMessage());
            log.error(error, e);
            throw new IOException(error, e);
        }
    }

    /**
     * Upload a large file using parallel multipart upload.
     * Automatically determines optimal part size and parallelism.
     *
     * @param bucketName S3 bucket name
     * @param objectKey S3 object key (path)
     * @param filePath Local file to upload
     * @throws Exception if upload fails
     */
    public void uploadFileParallel(String bucketName, String objectKey, Path filePath)
            throws Exception {
        Instant startTime = Instant.now();
        log.info("Multipart upload starting: {} -> s3://{}/{}",
                filePath.getFileName(), bucketName, objectKey);

        // Calculate optimal upload configuration
        UploadConfig config = calculateOptimalConfig(filePath);
        logUploadPlan(config, objectKey);

        // Initiate multipart upload
        String uploadId = initiateMultipartUpload(bucketName, objectKey);
        log.info("Upload initiated with ID: {}", uploadId);

        try {
            // Upload all parts in parallel
            List<CompletedPart> completedParts = uploadPartsInParallel(
                    bucketName, objectKey, uploadId, filePath, config);

            // Combine parts to complete upload
            completeMultipartUpload(bucketName, objectKey, uploadId, completedParts);

            // Log performance metrics
            logUploadMetrics(startTime, objectKey, config);

        } catch (Exception e) {
            log.error("Multipart upload failed: {}", e.getMessage(), e);
            abortMultipartUpload(bucketName, objectKey, uploadId);
            throw new IOException("Failed to upload " + objectKey, e);
        }
    }

    // ==================== Multipart Upload Logic ====================

    /**
     * Upload all parts of a file concurrently.
     *
     * @return List of completed parts with ETags
     */
    private List<CompletedPart> uploadPartsInParallel(
            String bucketName, String objectKey, String uploadId,
            Path filePath, UploadConfig config) throws Exception {

        log.debug("Preparing {} upload tasks", config.partCount);

        // Create upload tasks for each part
        List<CompletableFuture<CompletedPart>> uploadTasks = new ArrayList<>();

        for (int partNumber = 1; partNumber <= config.partCount; partNumber++) {
            PartUploadTask task = createPartTask(partNumber, config);

            CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(
                    () -> uploadPartWithRetry(bucketName, objectKey, uploadId, filePath, task),
                    uploadThreadPool
            );

            uploadTasks.add(future);
        }

        log.info("Uploading {} parts in parallel...", config.partCount);

        // Wait for all uploads to complete
        CompletableFuture<Void> allUploads = CompletableFuture.allOf(
                uploadTasks.toArray(new CompletableFuture[0])
        );

        allUploads.get(uploadTimeoutMinutes, TimeUnit.MINUTES);
        log.info("All parts uploaded successfully");

        // Collect completed parts
        return uploadTasks.stream()
                .map(CompletableFuture::join)
                .sorted(Comparator.comparingInt(CompletedPart::partNumber))
                .collect(Collectors.toList());
    }

    /**
     * Upload a single part with automatic retry on failure.
     * Uses exponential backoff between retries.
     */
    private CompletedPart uploadPartWithRetry(
            String bucketName, String objectKey, String uploadId,
            Path filePath, PartUploadTask task) {

        String threadName = Thread.currentThread().getName();
        log.debug("[{}] Uploading part {} ({} MB)",
                threadName, task.partNumber, task.size / MB);

        Exception lastError = null;

        for (int attempt = 1; attempt <= DEFAULT_MAX_RETRIES; attempt++) {
            try {
                if (attempt > 1) {
                    log.debug("Part {} retry attempt {}/{}",
                            task.partNumber, attempt, DEFAULT_MAX_RETRIES);
                }

                CompletedPart part = uploadSinglePart(
                        bucketName, objectKey, uploadId, filePath, task);

                log.debug("[{}] Part {} completed - ETag: {}",
                        threadName, task.partNumber, part.eTag());
                return part;

            } catch (Exception e) {
                lastError = e;

                if (attempt < DEFAULT_MAX_RETRIES) {
                    long backoffMs = calculateBackoff(attempt);
                    log.warn("Part {} failed (attempt {}), retrying in {}ms: {}",
                            task.partNumber, attempt, backoffMs, e.getMessage());

                    try {
                        Thread.sleep(backoffMs);
                    } catch (InterruptedException ie) {
                        Thread.currentThread().interrupt();
                        throw new RuntimeException("Interrupted during retry backoff", ie);
                    }
                }
            }
        }

        throw new RuntimeException(
                String.format("Part %d failed after %d attempts", task.partNumber, DEFAULT_MAX_RETRIES),
                lastError);
    }

    /**
     * Upload a single part to S3.
     */
    private CompletedPart uploadSinglePart(
            String bucketName, String objectKey, String uploadId,
            Path filePath, PartUploadTask task) throws IOException {

        // Read part data from file
        ByteBuffer partData = readFilePart(filePath, task.position, task.size);

        // Build upload request
        UploadPartRequest request = UploadPartRequest.builder()
                .bucket(bucketName)
                .key(objectKey)
                .uploadId(uploadId)
                .partNumber(task.partNumber)
                .contentLength(task.size)
                .build();

        try {
            // Execute upload
            UploadPartResponse response = s3Client.uploadPart(
                    request, AsyncRequestBody.fromByteBuffer(partData))
                    .get(uploadTimeoutMinutes, TimeUnit.MINUTES);

            return CompletedPart.builder()
                    .partNumber(task.partNumber)
                    .eTag(response.eTag())
                    .build();

        } catch (Exception e) {
            throw new IOException(
                    String.format("Failed to upload part %d: %s", task.partNumber, e.getMessage()), e);
        }
    }

    // ==================== S3 Operations ====================

    /**
     * Start a new multipart upload session.
     */
    private String initiateMultipartUpload(String bucketName, String objectKey) {
        try {
            CreateMultipartUploadRequest request = CreateMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .contentType("application/octet-stream")
                    .build();

            CreateMultipartUploadResponse response = s3Client.createMultipartUpload(request)
                    .get(INITIATE_UPLOAD_TIMEOUT_MINUTES, TimeUnit.MINUTES);

            return response.uploadId();
        } catch (Exception e) {
            throw new RuntimeException("Failed to initiate multipart upload: " + e.getMessage(), e);
        }
    }

    /**
     * Complete multipart upload by combining all parts.
     */
    private void completeMultipartUpload(
            String bucketName, String objectKey, String uploadId, List<CompletedPart> parts) {
        try {
            CompletedMultipartUpload multipartUpload = CompletedMultipartUpload.builder()
                    .parts(parts)
                    .build();

            CompleteMultipartUploadRequest request = CompleteMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .uploadId(uploadId)
                    .multipartUpload(multipartUpload)
                    .build();

            CompleteMultipartUploadResponse response = s3Client.completeMultipartUpload(request)
                    .get(COMPLETE_UPLOAD_TIMEOUT_MINUTES, TimeUnit.MINUTES);

            log.info("Upload completed - Object: {}, ETag: {}",
                    response.key(), response.eTag());

        } catch (Exception e) {
            throw new RuntimeException("Failed to complete multipart upload: " + e.getMessage(), e);
        }
    }

    /**
     * Abort a failed multipart upload to clean up S3 resources.
     */
    private void abortMultipartUpload(String bucketName, String objectKey, String uploadId) {
        try {
            AbortMultipartUploadRequest request = AbortMultipartUploadRequest.builder()
                    .bucket(bucketName)
                    .key(objectKey)
                    .uploadId(uploadId)
                    .build();

            s3Client.abortMultipartUpload(request)
                    .get(ABORT_UPLOAD_TIMEOUT_MINUTES, TimeUnit.MINUTES);

            log.info("Multipart upload aborted: {}", uploadId);
        } catch (Exception e) {
            log.error("Failed to abort multipart upload: {}", e.getMessage(), e);
        }
    }

    // ==================== Helper Methods ====================

    /**
     * Calculate optimal upload configuration based on file size.
     * Ensures parts are within S3 limits and maximizes efficiency.
     */
    private UploadConfig calculateOptimalConfig(Path filePath) throws IOException {
        long fileSize = filePath.toFile().length();

        // Start with configured part size or minimum
        long partSize = Math.max(partSizeMb * MB, MIN_PART_SIZE_BYTES);

        // Adjust if file would exceed max parts
        if (fileSize / partSize > MAX_PARTS) {
            // Calculate minimum part size to stay within limit
            partSize = (fileSize + MAX_PARTS - 1) / MAX_PARTS;
            // Round up to nearest MB
            partSize = ((partSize + MB - 1) / MB) * MB;
        }

        // Calculate final part count
        long partCount = (fileSize + partSize - 1) / partSize;

        return new UploadConfig(fileSize, partSize, partCount);
    }

    /**
     * Create task description for a single part.
     */
    private PartUploadTask createPartTask(int partNumber, UploadConfig config) {
        long position = (partNumber - 1) * config.partSize;
        long size = Math.min(config.partSize, config.fileSize - position);
        return new PartUploadTask(partNumber, position, size);
    }

    /**
     * Read a specific segment from file.
     */
    private ByteBuffer readFilePart(Path filePath, long position, long size) throws IOException {
        ByteBuffer buffer = ByteBuffer.allocate((int) size);

        try (FileChannel channel = FileChannel.open(filePath, StandardOpenOption.READ)) {
            channel.position(position);

            int totalRead = 0;
            while (totalRead < size) {
                int bytesRead = channel.read(buffer);
                if (bytesRead == -1) break;
                totalRead += bytesRead;
            }

            if (totalRead < size) {
                log.warn("Read {} bytes but expected {}", totalRead, size);
            }

            buffer.flip();
            return buffer;
        }
    }

    /**
     * Calculate exponential backoff delay.
     */
    private long calculateBackoff(int attempt) {
        return (long) (Math.pow(2, attempt) * 100);
    }

    /**
     * Create configured thread pool for uploads.
     */
    private ExecutorService createThreadPool() {
        return new ThreadPoolExecutor(
                maxUploadThreads,
                maxUploadThreads,
                60L, TimeUnit.SECONDS,
                new LinkedBlockingQueue<>(),
                new ThreadFactory() {
                    private final AtomicInteger counter = new AtomicInteger(1);
                    @Override
                    public Thread newThread(Runnable r) {
                        Thread thread = new Thread(r);
                        thread.setName("S3Upload-" + counter.getAndIncrement());
                        thread.setDaemon(true);
                        return thread;
                    }
                }
        );
    }

    /**
     * Shutdown thread pool gracefully.
     */
    private void shutdownThreadPool() {
        if (uploadThreadPool != null && !uploadThreadPool.isShutdown()) {
            try {
                uploadThreadPool.shutdown();
                if (!uploadThreadPool.awaitTermination(SHUTDOWN_TIMEOUT_SECONDS, TimeUnit.SECONDS)) {
                    log.warn("Thread pool didn't terminate gracefully, forcing shutdown");
                    uploadThreadPool.shutdownNow();
                }
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                log.warn("Thread pool shutdown interrupted", e);
                uploadThreadPool.shutdownNow();
            }
        }
    }

    // ==================== Logging Helpers ====================

    private void logUploadPlan(UploadConfig config, String objectKey) {
        log.info("Upload plan for {}:", objectKey);
        log.info("  File size: {} MB", config.fileSize / MB);
        log.info("  Part size: {} MB", config.partSize / MB);
        log.info("  Part count: {}", config.partCount);
        log.info("  Parallelism: {} threads", maxUploadThreads);
    }

    private void logUploadMetrics(Instant startTime, String objectKey, UploadConfig config) {
        Duration elapsed = Duration.between(startTime, Instant.now());
        double seconds = elapsed.toMillis() / 1000.0;
        double throughputMBps = (config.fileSize / MB) / seconds;

        log.info("Upload completed for {}: ", objectKey);
        log.info("  Duration: {:.2f} seconds", seconds);
        log.info("  Throughput: {:.2f} MB/s", throughputMBps);
        log.info("  Effective parallelism: {:.1f}",
                Math.min(config.partCount, maxUploadThreads));
    }

    // ==================== Inner Classes ====================

    /**
     * Upload configuration calculated for a file.
     */
    private static class UploadConfig {
        final long fileSize;
        final long partSize;
        final long partCount;

        UploadConfig(long fileSize, long partSize, long partCount) {
            this.fileSize = fileSize;
            this.partSize = partSize;
            this.partCount = partCount;
        }
    }

    /**
     * Description of a single part to upload.
     */
    private static class PartUploadTask {
        final int partNumber;
        final long position;  // Byte position in file
        final long size;      // Bytes to read

        PartUploadTask(int partNumber, long position, long size) {
            this.partNumber = partNumber;
            this.position = position;
            this.size = size;
        }
    }
}

=================
/**
 * Service for managing file operations in a data processing pipeline.
 *
 * Key responsibilities:
 * - Create local files from structured data
 * - Upload files to S3 storage
 * - Process three file types: base (source data), adjusted (modified data), and key (identifiers)
 * - Handle synchronous and asynchronous operations
 * - Provide callback notifications for async operations
 *
 * Directory structure:
 * {baseDir}/abinitio/{env}/adjustment/{requestId}/
 *   ├── base/          (source data files)
 *   ├── adjusted/      (modified data files)
 *   │   └── key/       (identifier files)
 *   └── distinct/      (distinct ID files)
 */